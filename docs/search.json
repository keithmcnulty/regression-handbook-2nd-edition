[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handbook of Regression Modeling in People Analytics (2nd edition)",
    "section": "",
    "text": "Welcome\nWelcome to the website for the book Handbook of Regression Modeling in People Analytics (2nd Edition) by Keith McNulty.\nThis new edition is to be published by Chapman & Hall/CRC Press in 2026. The online version of this book is free to read here (thanks to Chapman & Hall/CRC Press), and licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. If you have any feedback, please feel free to file an issue on GitHub. Thank you!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#notes-on-data-used-in-this-book",
    "href": "index.html#notes-on-data-used-in-this-book",
    "title": "Handbook of Regression Modeling in People Analytics (2nd edition)",
    "section": "Notes on data used in this book",
    "text": "Notes on data used in this book\nEach of the data sets used in this book can be downloaded individually by following the code in each chapter. Alternatively, packages containing all the data sets used in this book are now available in R and Python. For R users, install and load the peopleanalyticsdata R package.\n\n# install peopleanalyticsdata package\ninstall.packages(\"peopleanalyticsdata\")\nlibrary(peopleanalyticsdata)\n\n# see a list of data sets\ndata(package = \"peopleanalyticsdata\")\n\n# find out more about a specific data set ('managers' example)\nhelp(managers)\n\nFor Python users , use pip install peopleanalyticsdata to install the package into your environment. Then, to use the package:\n\n# import peopleanalyticsdata package\nimport peopleanalyticsdata as pad\nimport pandas as pd\n\n# see a list of data sets\npad.list_sets()\n\n# load data into a dataframe\ndf = pad.managers()\n\n# find out more about a specific data set ('managers' example)\npad.managers().info()\n\nHappy modeling!\nLast update: 17 December 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "foreword.html",
    "href": "foreword.html",
    "title": "Foreword by Alexis Fink",
    "section": "",
    "text": "This is the original foreword to the 1st edition of the book. It will be revised prior to the finalization of the 2nd edition.\nOver the past decade or so, increases in compute power, emergence of friendly analytic tools and an explosion of data have created a wonderful opportunity to bring more analytical rigor to nearly every imaginable question. Not coincidentally, organizations are increasingly looking to apply all that data and capability to what is typically their greatest area of expense and their greatest strategic differentiator—their people. For too long, many of the most critical decisions in an organization—people decisions—had been guided by gut instinct or borrowed ‘best practices’ and the democratization of people analytics opened up enticing pathways to fix that. Suddenly, analysts who were originally interested in data problems began to be interested in people problems, and HR professionals who had dedicated their careers to solving people problems needed more sophisticated analysis and data storytelling to make their cases and to refine their approaches for greater efficiency, effectiveness and impact.\nDoing data work with people in organizations has complexities that some other types of data work doesn’t. Often, the employee populations are relatively smaller than data sets used in other areas, sometimes limiting the methods that can be used. Various regulatory requirements may dictate what data can be gathered and used, and what types of evidence might be required for various programs or people strategies. Human behavior and organizations are sufficiently complex that typically, multiple factors work together in influencing an outcome. Effects can be subtle or meaningful only in combination, or difficult to tease apart. While in many disciplines, prediction is the most important aim, for most people analytics projects and practitioners, understanding why something is happening is critical.\nWhile the universe of analytical approaches is wonderful and vast, the best ‘Swiss army knife’ we have in people analytics is regression. This volume is an accessible, targeted work aimed directly at supporting professionals doing people analytics work. I’ve had the privilege of knowing and respecting Keith McNulty for many years – he is the rare and marvelous individual who is deeply expert in the mechanics of data and analytics, curious about and steeped in the opportunities to improve the effectiveness and well-being of people at work, and a gifted teacher and storyteller. He is among the most prolific standard-bearers for people analytics. This new open-source volume is in keeping with many years of contributions to the practice of understanding people at work.\nAfter nearly 30 years of doing people analytics work and the privilege of leading people analytics teams at several leading global organizations, I am still excited by the problems we get to solve, the insights we get to spawn, and the tremendous impact we can have on organizations and the people that comprise them. This work is human and technical and important and exciting and deeply gratifying. I hope that you will find this Handbook of Regression Modeling in People Analytics helps you uncover new truths and create positive impacts in your own work.\nAlexis A. Fink\nDecember 2020\nAlexis A. Fink, PhD is a leading figure in people analytics and has led major people analytics teams at Microsoft and Intel before her current role as Vice President of People Analytics and Workforce Strategy at Facebook. She is a Fellow of the Society for Industrial and Organizational Psychology and is a frequent author, journal editor and research leader in her field.",
    "crumbs": [
      "Foreword by Alexis Fink"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is the original introduction to the 1st edition of the book. It will be revised prior to the finalization of the 2nd edition.\nAs a fresh-faced undergraduate in mathematics in the 1990s, I took an introductory course in statistics in my first term. I would never take another. I struggled with the subject, scored my lowest grade in it and swore I would never go anywhere near it again.\nHow wrong I was. Today I live and breathe statistics. How did that happen?\nFirstly, statistics is about solving real-world problems, and amazingly there was not a single mention of a relatable problem from real life in that course I took all those years ago, just abstract mathematics. Nowadays, I know from my work and my personal learning activities that the mathematics has no meaning without a motivating problem to apply it to, and you’ll see example problems all through this book.\nSecondly, statistics is all about data, and working with real data has encouraged me to reengage with statistics and come at it from a different angle—bottom-up you could say. Suddenly all those concepts that were put up on whiteboards using abstract formulas now had real meaning and consequence to the data I was working with. For me, real data helps statistical theory come to life, and this book is supported by numerous data sets designed for the reader to engage with.\nBut one more step solidified my newfound love of statistics, and that was when I put regression modeling into practice. Faced with data sets that I initially believed were just far too messy and random to be able to produce genuine insights, I progressively became more and more fascinated by how regression can cut through the messiness, compartmentalize the randomness and lead you straight to inferences that are often surprising both in their clarity and in their conclusions.\nHence my motivation for writing this book, which is to give others—whether working in people analytics or otherwise—a starting point for a practical learning of regression methods, with the hope that they will see immediate applications to their work and take advantage of a much-underused toolkit that provides strong support for evidence-based practice.\nI am a mathematician who is now a practitioner of analytics. For this reason you should see that this book is neither afraid of nor obsessed with the mathematics of the methodologies covered. It is my general observation that many students and practitioners make the mistake of trying to run multivariate models without even a basic understanding of the underlying mathematics of those models, and I find it very difficult to see how they can be credible in responding to a wide range of questions or critique about their work without such an understanding. That said, it is also not necessary for students and practitioners to understand the deepest levels of theory in order to be fluent in running and interpreting multivariate models. In this book I have tried to limit the mathematical exposition to a level that allows confident and fluent execution and interpretation.\nI subscribe strongly to the principles of open source sharing of knowledge. If you want to reference the material in this book or use the exercises or data sets in trainings or classes, you are free to do so and you do not need to request my permission. I only ask that you make reference to this book as the source.\nI expect this book to improve over time. If you found this book or any part of it helpful to solving a problem, I’d love to hear about it. If you have comments to improve or question any aspect of the contents of this book I encourage you to leave an issue on its Github repository. This is the most reliable way for me to see your comment. I promise to consider all comments and input, but I do have to make a personal judgment about whether they are helpful to the aims and purpose of this book. If I do make changes or additions based on your input I will make a point to acknowledge your contribution.\nI would like to thank the following individuals who have reviewed or contributed to this book at some point during its development: Liz Romero, Alex LoPilato, Kevin Jaggs, Seth Saavedra, Akshay Kotha. My sincere thanks to Alexis Fink for drawing on her years of people analytics experience to set the context for this book in her foreword. My thanks to the people analytics community for their constant encouragement and support in sharing theory, content and method, and to the R community for all the work they do in giving us amazing and constantly improving statistical tools to work with. Finally, I would like to thank my family for their patience and understanding on the evenings and weekends I dedicated to the writing of this book, and for tolerating far too much dinner conversation on the topic of statistics.\nKeith McNulty\nDecember 2020",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "importance.html",
    "href": "importance.html",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "",
    "text": "1.1 Why is regression modeling so important in people analytics?\nIn the 19th century, when Francis Galton first used the term ‘regression’ to describe a statistical phenomenon (see Chapter 4), little did he know how important that term would be today. Many of the most powerful tools of statistical inference that we now have at our disposal can be traced back to the types of early analysis that Galton and his contemporaries were engaged in. The sheer number of different regression-related methodologies and variants that are available to researchers and practitioners today is mind-boggling, and there are still rich veins of ongoing research that are focused on defining and refining new forms of regression to tackle new problems.\nNeither could Galton have imagined the advent of the age of data we now live in. Those of us (like me) who entered the world of work even as recently as 20 years ago remember a time when most problems could not be expected to be solved using a data-driven approach, because there simply was no data. Things are very different now, with data being collected and processed all around us and available to use as direct or indirect measures of the phenomena we are interested in.\nAlong with the growth in data that we have seen in recent years, we have also seen a rapid growth in the availability of statistical tools—open source and free to use—that fundamentally change how we go about analytics. Gone are the clunky, complex, repeated steps on calculators or spreadsheets. In their place are lean statistical programming languages that can implement a regression analysis in milliseconds with a single line of code, allowing us to easily run and reproduce multivariate analysis at scale.\nSo given that we have access to well-developed methodology, rich sources of data and readily accessible tools, it is somewhat surprising that many analytics practitioners have a limited knowledge and understanding of regression and its applications. The aim of this book is to encourage inexperienced analytics practitioners to ‘dip their toes’ further into the wide and varied world of regression in order to deliver more targeted and precise insights to their organizations and stakeholders on the problems they are most interested in. While the primary subject matter focus of this book is the analysis of people-related phenomena, the material is easily and naturally transferable to other disciplines. Therefore this book can be regarded as a practical introduction to a wide range of regression methods for any analytics student or practitioner.\nIt is my firm belief that all people analytics professionals should have a strong understanding of regression models and how to implement and interpret them in practice, and my aim with this book is to provide those who need it with help in getting there. In this chapter we will set the scene for the technical learning in the remainder of the book by outlining the relevance of regression models in people analytics practice. We also touch on some general inferential modeling theory to set a context for later chapters, and we provide a preview of the contents, structure and learning objectives of this book.\nPeople analytics involves the study of the behaviors and characteristics of people or groups in relation to important business, organizational or institutional outcomes. This can involve both qualitative methods and quantitative methods, but if data is available related to a particular topic of interest, then quantitative methods are almost always considered important. With such a specific focus on outcomes, any analyst working in people analytics will frequently need to model these outcomes both to understand what influences them and to potentially predict them in the future.\nModeling an outcome with the primary goal of understanding what influences it can be quite a different matter to modeling an outcome with the primary goal of predicting if it will happen in the future. If we need to understand what influences an outcome, we need to get inside a model and construct a formula or structure to infer how each variable acts on that outcome, we need to get a sense of which variables are meaningful or not, and we need to quantify the ‘explainability’ of the outcome based on our variables. If our primary aim is to predict the outcome, getting inside the model is less important because we don’t have to explain the outcome, we just need to be confident that it predicts accurately.\nA model constructed to understand an outcome is often called an inferential model. Regression models are the most well-known and well-used inferential models available, providing a wide range of measures and insights that help us explain the relationship between our input variables and our outcome of interest, as we shall see in later chapters of this book.\nThe current reality in the field of people analytics is that inferential models are more required than predictive models. There are two reasons for this. First, data sets in people analytics are rarely large enough to facilitate satisfactory prediction accuracy, and so attention is usually shifted to inference for this reason alone. Second, in the field of people analytics, decisions often have a real impact on individuals. Therefore, even in the rare situations where accurate predictive modeling is attainable, stakeholders are unlikely to trust the output and bear the consequences of predictive models without some sort of elementary understanding of how the predictions are generated. This requires the analyst to consider inference power as well as predictive accuracy in selecting their modeling approach. Again, many regression models come to the fore because they are commonly able to provide both inferential and predictive value.\nFinally, the growing importance of evidence-based practice in many clinical and professional fields has generated a need for more advanced modeling skills to satisfy rising demand for quantitative evidence from decision makers. In people-related fields such as human resources, many varieties of specialized regression-based models such as survival models or latent variable models have crossed from academic and clinical settings into business settings in recent years, and there is an increasing need for qualified individuals who understand and can implement and interpret these models in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "importance.html#sec-theory-modeling",
    "href": "importance.html#sec-theory-modeling",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "1.2 What do we mean by ‘modeling’‍?",
    "text": "1.2 What do we mean by ‘modeling’‍?\nThe term ‘modeling’ has a very wide range of meaning in everyday life and work. In this book we are focused on inferential modeling, and we define that as a specific form of statistical learning, which tries to discover and understand a mathematical relationship between a set of measurements of certain constructs and a measurement of an outcome of interest, based on a sample of data on each. Modeling is both a concept and a process.\n\n1.2.1 The theory of inferential modeling\nWe will start with a theoretical description and then provide a real example from a later chapter to illustrate.\nImagine we have a population \\(\\mathscr{P}\\) for which we believe there may be a non-random relationship between a certain construct or set of constructs \\(\\mathscr{C}\\) and a certain measurable outcome \\(\\mathscr{O}\\). Imagine that for a certain sample \\(S\\) of observations from \\(\\mathscr{P}\\), we have a collection of data which we believe measure \\(\\mathscr{C}\\) to some acceptable level of accuracy, and for which we also have a measure of the outcome \\(\\mathscr{O}\\).\nBy convention, we denote the set of data that measure \\(\\mathscr{C}\\) on our sample \\(S\\) as \\(X = x_1, x_2, \\dots, x_p\\), where each \\(x_i\\) is a vector (or column) of data measuring at least one of the constructs in \\(\\mathscr{C}\\). We denote the set of data that measure \\(\\mathscr{O}\\) on our sample set \\(S\\) as \\(y\\). An upper-case \\(X\\) is used because the expectation is that there will be several columns of data measuring our constructs, and a lower-case \\(y\\) is used because the expectation is that the outcome is a single column.\nInferential modeling is the process of learning about a relationship (or lack of relationship) between the data in \\(X\\) and \\(y\\) and using that to describe a relationship (or lack of relationship) between our constructs \\(\\mathscr{C}\\) and our outcome \\(\\mathscr{O}\\) that is valid to a high degree of statistical certainty on the population \\(\\mathscr{P}\\). This process may include:\n\nTesting a proposed mathematical relationship in the form of a function, structure or iterative method\nComparing that relationship against other proposed relationships\nDescribing the relationship statistically\nDetermining whether the relationship (or certain elements of it) can be generalized from the sample set \\(S\\) to the population \\(\\mathscr{P}\\)\n\nWhen we test a relationship between \\(X\\) and \\(y\\), we acknowledge that data and measurements are imperfect and so each observation in our sample \\(S\\) may contain random error that we cannot control. Therefore we define our relationship as:\n\\[\ny = f(X) + \\epsilon\n\\] where \\(f\\) is some transformation or function of the data in \\(X\\) and \\(\\epsilon\\) is a random, uncontrollable error.\n\\(f\\) can take the form of a predetermined function with a formula defined on \\(X\\), like a linear function for example. In this case we can call our model a parametric model. In a parametric model, the modeled value of \\(y\\) is known as soon as we know the values of \\(X\\) by simply applying the formula. In a non-parametric model, there is no predetermined formula that defines the modeled value of \\(y\\) purely in terms of \\(X\\). Non-parametric models need further information in addition to \\(X\\) in order to determine the modeled value of \\(y\\)—for example the value of \\(y\\) in other observations with similar \\(X\\) values.\nRegression models are designed to derive \\(f\\) using estimation based on statistical likelihood and expectation, founded on the theory of the distribution of random variables. Regression models can be both parametric and non-parametric, but by far the most commonly used methods (and the majority of those featured in this book) are parametric. Because of their foundation in statistical likelihood and expectation, they are particularly suited to helping answer questions of generalizability—that is, to what extent can the relationship being observed in the sample \\(S\\) be inferred for the population \\(\\mathscr{P}\\), which is usually the driving force in any form of inferential modeling.\nNote that there is a difference between establishing a statistical relationship between \\(\\mathscr{C}\\) and \\(\\mathscr{O}\\) and establishing a causal relationship between the two. This can be a common trap that inexperienced statistical analysts fall into when communicating the conclusions of their modeling. Establishing that a relationship exists between a construct and an outcome is a far cry from being able to say that one causes the other. This is the common truism that ‘correlation does not equal causation’‍.\nTo bring our theory to life, consider the walkthrough example in Chapter 4 of this book. In this example, we discuss how to establish a relationship between the academic results of students in the first three years of their education program and their results in the fourth year. In this case, our population \\(\\mathscr{P}\\) is all past, present and future students who take similar examinations, and our sample \\(S\\) is the students who completed their studies in the past three years. \\(X = x_1, x_2, x_3\\) are each of the three scores from the first three years, and \\(y\\) is the score in the fourth year. We test \\(f\\) to be a linear relationship, and we establish that such a relationship can be generalized to the entire population \\(\\mathscr{P}\\) with a substantial level of statistical confidence1.\nAlmost all our work in this book will refer to the variables \\(X\\) as input variables and the variable \\(y\\) as the outcome variable. There are many other common terms for these which you may find in other sources—for example \\(X\\) are often known as independent variables or covariates while \\(y\\) is often known as a dependent or response variable.\n\n\n1.2.2 The process of inferential modeling\nInferential modeling—regression or otherwise—is a process of numerous steps. Typically the main steps are:\n\nDefining the outcome of interest \\(\\mathscr{O}\\) and the input constructs \\(\\mathscr{C}\\) based on a broader evidence-based objective\nConfirming that \\(\\mathscr{O}\\) has reliable measurement data\nDetermining which data can be used to measure \\(\\mathscr{C}\\)\nDetermining a sample \\(S\\) and collecting, refining and cleaning data.\nPerforming exploratory data analysis (EDA) and proposing a set of models to test for \\(f\\)\nPutting the data in an appropriate format for each model\nRunning the models\nInterpreting the outputs and performing model diagnostics\nSelecting an optimal model or models\nArticulating the inferences that can be generalized to apply to \\(\\mathscr{P}\\)\n\nThis book is primarily focused on steps 7–10 of this process2. That is not to say that steps 1–6 are not important. Indeed these steps are critical and often loaded with analytic traps. Defining the problem, collecting reliable measures and cleaning and organizing data are still the source of much pain and angst for analysts, but these topics are for another day.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "importance.html#the-structure-system-and-organization-of-this-book",
    "href": "importance.html#the-structure-system-and-organization-of-this-book",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "1.3 The structure, system and organization of this book",
    "text": "1.3 The structure, system and organization of this book\nThe purpose of this book is to put inexperienced practitioners firmly on a path to the confident and appropriate use of regression techniques in their day-to-day work. This requires enough of an understanding of the underlying theory so that judgments can be made about results, but also a practical set of steps to help practitioners apply the most common regression methods to a variety of typical modeling scenarios in a reliable and reproducible way.\nIn most chapters, time is spent on the underlying mathematics. Not to the degree of an academic theorist, but enough to ensure that the reader can associate some mathematical meaning to the outputs of models. While it may be tempting to skip the math, I strongly recommend against it if you intend to be a high performer in your field. The best analysts are those who can genuinely understand what the numbers are telling them.\nThe statistical programming language R is used for most of the practical demonstration in each chapter. Because R is open source and particularly well geared to inferential statistics, it is an excellent choice for those whose work involves a lot of inferential analysis. We will also show implementations of all of the available methodologies in Python, which is also a powerful open source tool for this sort of work, and is much more popular and in the broader field of data science.\nEach chapter involves a walkthrough example to illustrate the specific method and to allow the reader to replicate the analysis for themselves. The exercises at the end of each chapter are designed so that the reader can try the same method on a different data set, or a different problem on the same data set, to test their learning and understanding. In the final chapter, a series of data sets and exercises are provided with limited instruction in order to give the reader an opportunity to test their overall knowledge in selecting and applying regression methods to a variety of people analytics data sets and problems. All in all, sixteen different data sets are used as walkthrough or exercise examples, and all of these data sets are fictitious constructions unless otherwise indicated. Despite the fiction, they are deliberately designed to present the reader with something resembling how the data might look in practice, albeit cleaner and more organized.\nThe chapters of this book are arranged as follows:\n\nChapter 2 covers the basics of the R programming language for those who want to attempt to jump straight in to the work in subsequent chapters but have very little R experience. Experienced R programmers can skip this chapter.\nChapter 3 covers the essential statistical concepts needed to understand multivariable regression models. It also serves as a tutorial in univariate and bivariate statistics illustrated with real data. If you need help developing a decent understanding of descriptive statistics, random distribution and hypothesis testing, this is an important chapter to study.\nChapter 4 covers linear regression and in the course of that introduces many other foundational concepts. The walkthrough example involves modeling academic results from prior results. The exercises involve modeling income levels based on various work and demographic factors.\nChapter 5 covers binomial logistic regression. The walkthrough example involves modeling promotion likelihood based on performance metrics. The exercises involve modeling charitable donation likelihood based on prior donation behavior and demographics.\nChapter 6 covers multinomial regression. The walkthrough example and exercise involves modeling the choice of three health insurance products by company employees based on demographic and position data.\nChapter 7 covers ordinal regression. The walkthrough example involves modeling in-game disciplinary action against soccer players based on prior discipline and other factors. The exercises involve modeling manager performance based on varied data.\nChapter 8 covers count data regression, including Poisson, Quasi-Poisson and negative binomial regression. The walkthrough example involves modeling absenteeism in a technology company. The exercises involve modeling the number of complaints received about telephone customer service representatives in a retail company.\nChapter 9 covers modeling options for data with explicit or latent hierarchy. The first part covers mixed modeling and uses a model of speed dating decisions as a walkthrough and example. The second part covers structural equation modeling and uses a survey for a political party as a walkthrough example. The exercises involve modeling latent variables in an employee engagement survey.\nChapter 10 covers survival analysis, Cox proportional hazard regression and frailty models. The chapter uses employee attrition as a walkthrough example and exercise.\nChapter 11 covers power analysis, focusing in particular on estimating the required minimum sample sizes in establishing meaningful inferences for both simple statistical tests and multivariate models. Examples related to experimental studies are used to illustrate, such as concurrent validity studies of selection instruments.\n\nChapter 16 is a set of problems and data sets which will allow the reader to practice the skills they have learned in this book and apply them to a variety of people analytics domains such as recruiting, performance, promotion, compensation and learning. Sets of discussion questions and data exercises will guide the reader through each problem, but these are designed in a way that encourages the independent selection and application of the methods covered in this book. These data sets, problems and exercises would suit as homework material for classes in statistical modeling or people analytics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "importance.html#footnotes",
    "href": "importance.html#footnotes",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "",
    "text": "We also determine that \\(x_1\\) (the first-year examination score) plays no significant role in \\(f\\) and that introducing some non-linearity into \\(f\\) further improves the statistical accuracy of the inferred relationship.↩︎\nThe book also addresses Steps 5 and 6 in some chapters.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "basic_r.html",
    "href": "basic_r.html",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "2.1 What is R?\nMost of the work in this book is implemented in the R statistical programming language which, along with Python, is one of the two languages that I use in my day-to-day statistical analysis. Sample implementations in Python are also provided at the end of the relevant chapters or sections. I have made efforts to keep the code as simple as possible, and I have tried to avoid the use of too many external packages. For the most part, readers should see (especially in the earlier chapters) that code blocks are short and simple, relying wherever possible on base R functionality. No doubt there are neater and more effective ways to code some of the material in this book using a wide array of R packages, but my priority has been to keep the code simple, consistent and easily reproducible.\nFor those who wish to follow the method and theory without the implementations in this book, there is no need to read this chapter. However, the style of this book is to use implementation to illustrate theory and practice, and so tolerance of many code blocks will be necessary as you read onward.\nFor those who wish to simply replicate the models as quickly as possible, they will be able to avail of the code block copying feature, which appears whenever you scroll over an input code block. Assuming all the required external packages have been installed, these code blocks should all be transportable and immediately usable. For those who are extra-inquisitive and want to explore how I constructed graphics used for illustration (for which code is usually not displayed), the best place to go is the Github repository for this book.\nThis chapter is for those who wish to learn the methods in this book but do not know how to use R. However, it is not intended to be a full tutorial on R. There are many more qualified individuals and existing resources that would better serve that purpose—in particular I recommend Wickham, Grolemund, and Çetinkaya-Rundel (2023). It is recommended that you consult these resources and become comfortable with the basics of R before proceeding into the later chapters of this book. However, acknowledging that many will want to dive in sooner rather than later, this chapter covers the absolute basics of R that will allow the uninitiated reader to proceed with at least some orientation.\nR is a programming language that was originally developed by and for statisticians, but in recent years its capabilities and the environments in which it is used have expanded greatly, with extensive use nowadays in academia and the public and private sectors. There are many advantages to using a programming language like R. Here are some:\nThere is often heated debate about which tools are better for doing non-trivial statistical analysis. I personally find that R provides the widest array of resources for those interested in inferential modeling, while Python has a more well-developed toolkit for predictive modeling and machine learning. Since the primary focus of this book is inferential modeling, the in-depth walkthroughs are coded in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#what-is-r",
    "href": "basic_r.html#what-is-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "It is completely free and open source.\nIt is faster and more efficient with memory than popular graphical user interface analytics tools.\nIt facilitates easier replication of analysis from person to person compared with many alternatives.\nIt has a large and growing global community of active users.\nIt has a large and rapidly growing universe of packages, which are all free and which provide the ability to do an extremely wide range of general and highly specialized tasks, statistical and otherwise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#how-to-start-using-r",
    "href": "basic_r.html#how-to-start-using-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.2 How to start using R",
    "text": "2.2 How to start using R\nJust like most programming languages, R itself is an interpreter which receives input and returns output. It is not very easy to use without an IDE. An IDE is an Integrated Development Environment, which is a convenient user interface allowing an R programmer to do all their main tasks including writing and running R code, saving files, viewing data and plots, integrating code into documents and many other things. By far the most popular IDE for R is RStudio. An example of what the RStudio IDE looks like can be seen in Figure 2.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: The RStudio IDE\n\n\n\nTo start using R, follow these steps:\n\nDownload and install the latest version of R from https://www.r-project.org/. Ensure that the version suits your operating system.\nDownload the latest version of the RStudio Desktop IDE from https://posit.co/download/rstudio-desktop/. Consult the user guide at https://docs.posit.co/ide/user/ to help orient yourself to the features of RStudio.\n\nOpen RStudio and play around.\n\nThe initial stages of using R can be challenging, mostly due to the need to become familiar with how R understands, stores and processes data. Extensive trial and error is a learning necessity. Perseverance is important in these early stages, as well as an openness to seek help from others either in person or via online forums. Try not to rely too much on Generative AI tools for help, as they can often lead to you missing important learning steps, and they can also generate error-prone code.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#data-in-r",
    "href": "basic_r.html#data-in-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.3 Data in R",
    "text": "2.3 Data in R\nAs you start to do tasks involving data in R, you will generally want to store the things you create so that you can refer to them later. Simply calculating something does not store it in R. For example, a simple calculation like this can be performed easily:\n\n3 + 3\n\n[1] 6\n\n\nHowever, as soon as the calculation is complete, it is forgotten by R because the result hasn’t been assigned anywhere. To store something in your R session, you will assign it a name using the &lt;- operator. So I can assign my previous calculation to an object called my_sum, and this allows me to access the value at any time.\n\n# store the result\nmy_sum &lt;- 3 + 3\n\n# now I can work with it\nmy_sum + 3\n\n[1] 9\n\n\nYou will see above that you can comment your code by simply adding a # to the start of a line to ensure that the line is ignored by the interpreter.\nNote that assignment to an object does not result in the value being displayed. To display the value, the name of the object must be typed, the print() command used or the command should be wrapped in parentheses.\n\n# show me the value of my_sum\nmy_sum\n\n[1] 6\n\n# assign my_sum + 3 to new_sum and show its value\n(new_sum &lt;- my_sum + 3)\n\n[1] 9\n\n\n\n2.3.1 Data types\nAll data in R has an associated type, to reflect the wide range of data that R is able to work with. The typeof() function can be used to see the type of a single scalar value. Let’s look at the most common scalar data types.\nNumeric data can be in integer form or double (decimal) form.\n\n# integers can be signified by adding an 'L' to the end\nmy_integer &lt;- 1L  \nmy_double &lt;- 6.38\n\ntypeof(my_integer)\n\n[1] \"integer\"\n\ntypeof(my_double)\n\n[1] \"double\"\n\n\nCharacter data is text data surrounded by single or double quotes.\n\nmy_character &lt;- \"THIS IS TEXT\"\ntypeof(my_character)\n\n[1] \"character\"\n\n\nLogical data takes the form TRUE or FALSE.\n\nmy_logical &lt;- TRUE\ntypeof(my_logical)\n\n[1] \"logical\"\n\n\n\n\n2.3.2 Homogeneous data structures\nVectors are one-dimensional structures containing data of the same type and are notated by using c(). The type of the vector can also be viewed using the typeof() function, but the str() function can be used to display both the contents of the vector and its type.\n\nmy_double_vector &lt;- c(2.3, 6.8, 4.5, 65, 6)\nstr(my_double_vector)\n\n num [1:5] 2.3 6.8 4.5 65 6\n\n\nCategorical data—which takes only a finite number of possible values—can be stored as a factor vector to make it easier to perform grouping and manipulation.\n\ncategories &lt;- factor(\n  c(\"A\", \"B\", \"C\", \"A\", \"C\")\n)\n\nstr(categories)\n\n Factor w/ 3 levels \"A\",\"B\",\"C\": 1 2 3 1 3\n\n\nIf needed, the factors can be given order.\n\n# character vector \nranking &lt;- c(\"Medium\", \"High\", \"Low\")\nstr(ranking)\n\n chr [1:3] \"Medium\" \"High\" \"Low\"\n\n# turn it into an ordered factor\nranking_factors &lt;- ordered(\n  ranking, levels = c(\"Low\", \"Medium\", \"High\")\n)\n\nstr(ranking_factors)\n\n Ord.factor w/ 3 levels \"Low\"&lt;\"Medium\"&lt;..: 2 3 1\n\n\nThe number of elements in a vector can be seen using the length() function.\n\nlength(categories)\n\n[1] 5\n\n\nSimple numeric sequence vectors can be created using shorthand notation.\n\n(my_sequence &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIf you try to mix data types inside a vector, it will usually result in type coercion, where one or more of the types are forced into a different type to ensure homogeneity. Often this means the vector will become a character vector.\n\n# numeric sequence vector\nvec &lt;- 1:5\nstr(vec)\n\n int [1:5] 1 2 3 4 5\n\n# create a new vector containing vec and the character \"hello\"\nnew_vec &lt;- c(vec, \"hello\")\n\n# numeric values have been coerced into their character equivalents\nstr(new_vec)\n\n chr [1:6] \"1\" \"2\" \"3\" \"4\" \"5\" \"hello\"\n\n\nBut sometimes logical or factor types will be coerced to numeric.\n\n# attempt a mixed logical and numeric\nmix &lt;- c(TRUE, 6)\n\n# logical has been converted to binary numeric (TRUE = 1)\nstr(mix)\n\n num [1:2] 1 6\n\n# try to add a numeric to our previous categories factor vector\nnew_categories &lt;- c(categories, 1)\n\n# categories have been coerced to background integer representations\nstr(new_categories)\n\n num [1:6] 1 2 3 1 3 1\n\n\nMatrices are two-dimensional data structures of the same type and are built from a vector by defining the number of rows and columns. Data is read into the matrix down the columns, starting left and moving right. Matrices are rarely used for non-numeric data types.\n\n# create a 2x2 matrix with the first four integers\n(m &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nArrays are n-dimensional data structures with the same data type and are not used extensively by most R users.\n\n\n2.3.3 Heterogeneous data structures\nLists are one-dimensional data structures that can take data of any type.\n\nmy_list &lt;- list(6, TRUE, \"hello\")\nstr(my_list)\n\nList of 3\n $ : num 6\n $ : logi TRUE\n $ : chr \"hello\"\n\n\nList elements can be any data type and any dimension. Each element can be given a name.\n\nnew_list &lt;- list(\n  scalar = 6, \n  vector = c(\"Hello\", \"Goodbye\"), \n  matrix = matrix(1:4, nrow = 2, ncol = 2)\n)\n\nstr(new_list)\n\nList of 3\n $ scalar: num 6\n $ vector: chr [1:2] \"Hello\" \"Goodbye\"\n $ matrix: int [1:2, 1:2] 1 2 3 4\n\n\nNamed list elements can be accessed by using $.\n\nnew_list$matrix\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nDataframes are the most used data structure in R; they are effectively a named list of vectors of the same length, with each vector as a column. As such, a dataframe is very similar in nature to a typical database table or spreadsheet.\n\n# two vectors of different types but same length\nnames &lt;- c(\"John\", \"Ayesha\")\nages &lt;- c(31, 24)\n\n# create a dataframe\n(df &lt;- data.frame(names, ages))\n\n   names ages\n1   John   31\n2 Ayesha   24\n\n# get types of columns\nstr(df)\n\n'data.frame':   2 obs. of  2 variables:\n $ names: chr  \"John\" \"Ayesha\"\n $ ages : num  31 24\n\n# get dimensions of df\ndim(df)\n\n[1] 2 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#working-with-dataframes",
    "href": "basic_r.html#working-with-dataframes",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.4 Working with dataframes",
    "text": "2.4 Working with dataframes\nThe dataframe is the most common data structure used by analysts in R, due to its similarity to data tables found in databases and spreadsheets. We will work almost entirely with dataframes in this book, so let’s get to know them.\n\n2.4.1 Loading and tidying data in dataframes\nTo work with data in R, you usually need to pull it in from an outside source into a dataframe1. R facilitates numerous ways of importing data from simple .csv files, from Excel files, from online sources or from databases. Let’s load a data set that we will use later—the salespeople data set, which contains some information on the sales, average customer ratings and performance ratings of salespeople. The read.csv() function can accept a URL address of the file if it is online.\n\n# url of data set \nurl &lt;- \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\n\n# load the data set and store it as a dataframe called salespeople\nsalespeople &lt;- read.csv(url)\n\nWe might not want to display this entire data set before knowing how big it is. We can view the dimensions, and if it is too big to display, we can use the head() function to display just the first few rows.\n\ndim(salespeople)\n\n[1] 351   4\n\n# hundreds of rows, so view first few\nhead(salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\nWe can view a specific column by using $, and we can use square brackets to view a specific entry. For example if we wanted to see the 6th entry of the sales column:\n\nsalespeople$sales[6]\n\n[1] 918\n\n\nAlternatively, we can use a [row, column] index to get a specific entry in the dataframe.\n\nsalespeople[34, 4]\n\n[1] 3\n\n\nWe can take a look at the data types using str().\n\nstr(salespeople)\n\n'data.frame':   351 obs. of  4 variables:\n $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n $ performance  : int  2 3 4 2 3 2 3 1 3 3 ...\n\n\nWe can also see a statistical summary of each column using summary(), which tells us various statistics depending on the type of the column.\n\nsummary(salespeople)\n\n    promoted          sales       customer_rate    performance \n Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  \n 1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  \n Median :0.0000   Median :475.0   Median :3.620   Median :3.0  \n Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  \n 3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  \n Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  \n                  NA's   :1       NA's   :1       NA's   :1    \n\n\nNote that there is missing data in this dataframe, indicated by NAs in the summary. Missing data is identified by a special NA value in R. This should not be confused with \"NA\", which is simply a character string. The function is.na() will look at all values in a vector or dataframe and return TRUE or FALSE based on whether they are NA or not. By adding these up using the sum() function, it will take TRUE as 1 and FALSE as 0, which effectively provides a count of missing data.\n\nsum(is.na(salespeople))\n\n[1] 3\n\n\nThis is a small number of NAs given the dimensions of our data set and we might want to remove the rows of data that contain NAs. The easiest way is to use the complete.cases() function, which identifies the rows that have no NAs, and then we can select those rows from the dataframe based on that condition. Note that you can overwrite objects with the same name in R.\n\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# confirm no NAs\nsum(is.na(salespeople))\n\n[1] 0\n\n\nWe can see the unique values of a vector or column using the unique() function.\n\nunique(salespeople$performance)\n\n[1] 2 3 4 1\n\n\nIf we need to change the type of a column in a dataframe, we can use the as.numeric(), as.character(), as.logical() or as.factor() functions. For example, given that there are only four unique values for the performance column, we may want to convert it to a factor.\n\nsalespeople$performance &lt;- as.factor(salespeople$performance)\nstr(salespeople)\n\n'data.frame':   350 obs. of  4 variables:\n $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n $ performance  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 3 4 2 3 2 3 1 3 3 ...\n\n\n\n\n2.4.2 Manipulating dataframes\nDataframes can be subsetted to contain only rows that satisfy specific conditions.\n\n(sales_720 &lt;- subset(salespeople, subset = sales == 720))\n\n    promoted sales customer_rate performance\n290        1   720          3.76           3\n\n\nNote the use of ==, which is used in many programming languages, to test for precise equality. Similarly we can select columns based on inequalities (&gt; for ‘greater than’‍, &lt; for ‘less than’‍, &gt;= for ‘greater than or equal to’‍, &lt;= for ‘less than or equal to’‍, or != for ‘not equal to’). For example:\n\nhigh_sales &lt;- subset(salespeople, subset = sales &gt;= 700)\nhead(high_sales)\n\n   promoted sales customer_rate performance\n6         1   918          4.54           2\n12        1   716          3.16           3\n20        1   937          5.00           2\n21        1   702          3.53           4\n25        1   819          4.45           2\n26        1   736          3.94           4\n\n\nTo select specific columns use the select argument.\n\nsalespeople_sales_perf &lt;- subset(salespeople, \n                                 select = c(\"sales\", \"performance\"))\nhead(salespeople_sales_perf)\n\n  sales performance\n1   594           2\n2   446           3\n3   674           4\n4   525           2\n5   657           3\n6   918           2\n\n\nTwo dataframes with the same column names can be combined by their rows.\n\nlow_sales &lt;- subset(salespeople, subset = sales &lt; 400)\n\n# bind the rows of low_sales and high_sales together\nlow_and_high_sales = rbind(low_sales, high_sales)\nhead(low_and_high_sales)\n\n   promoted sales customer_rate performance\n7         0   318          3.09           3\n8         0   364          4.89           1\n9         0   342          3.74           3\n10        0   387          3.00           3\n15        0   344          3.02           2\n16        0   372          3.87           3\n\n\nTwo dataframes with different column names can be combined by their columns.\n\n# two dataframes with two columns each\nsales_perf &lt;- subset(salespeople, \n                     select = c(\"sales\", \"performance\"))\nprom_custrate &lt;- subset(salespeople, \n                        select = c(\"promoted\", \"customer_rate\"))\n\n# bind the columns to create a dataframe with four columns\nfull_df &lt;- cbind(sales_perf, prom_custrate)\nhead(full_df)\n\n  sales performance promoted customer_rate\n1   594           2        0          3.94\n2   446           3        0          4.06\n3   674           4        1          3.83\n4   525           2        0          3.62\n5   657           3        1          4.40\n6   918           2        1          4.54",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#functions-packages-and-libraries",
    "href": "basic_r.html#functions-packages-and-libraries",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.5 Functions, packages and libraries",
    "text": "2.5 Functions, packages and libraries\nIn the code so far we have used a variety of functions. For example head(), subset(), rbind(). Functions are operations that take certain defined inputs and return an output. Functions exist to perform common useful operations.\n\n2.5.1 Using functions\nFunctions usually take one or more arguments. Often there are a large number of arguments that a function can take, but many are optional and not required to be specified by the user. For example, the function head(), which displays the first rows of a dataframe2, has only one required argument x: the name of the dataframe. A second argument is optional, n: the number of rows to display. If n is not entered, it is assumed to have the default value n = 6.\nWhen running a function, you can either specify the arguments by name or you can enter them in order without their names. If you enter arguments without naming them, R expects the arguments to be entered in exactly the right order.\n\n# see the head of salespeople, with the default of six rows\nhead(salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\n\n# see fewer rows - arguments need to be in the right order if not named\nhead(salespeople, 3)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n\n\n\n# or if you don't know the right order, \n# name your arguments and you can put them in any order\nhead(n = 3, x = salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n\n\n\n\n2.5.2 Help with functions\nMost functions in R have excellent help documentation. To get help on the head() function, type help(head) or ?head. This will display the results in the Help browser window in RStudio. Alternatively you can open the Help browser window directly in RStudio and do a search there. An example of the browser results for head() is in Figure 2.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Results of a search for the head() function in the RStudio Help browser\n\n\n\nThe help page normally shows the following:\n\nDescription of the purpose of the function\nUsage examples, so you can quickly see how it is used\nArguments list so you can see the names and order of arguments\nDetails or notes on further considerations on use\nExpected value of the output (for example head() is expected to return a similar object to its first input x)\nExamples to help orient you further (sometimes examples can be very abstract in nature and not so helpful to users)\n\n\n\n2.5.3 Writing your own functions\nFunctions are not limited to those that come packaged in R. Users can write their own functions to perform tasks that are helpful to their objectives. Experienced programmers in most languages subscribe to a principle called DRY (Don’t Repeat Yourself). Whenever a task needs to be done repeatedly, it is poor practice to write the same code numerous times. It makes more sense to write a function to do the task.\nIn this example, a simple function is written which generates a report on a dataframe:\n\n# create df_report function\ndf_report &lt;- function(df) {\n  paste(\"This dataframe contains\", nrow(df), \"rows and\", \n        ncol(df), \"columns. There are\", sum(is.na(df)), \"NA entries.\")\n}\n\nWe can test our function by using the built-in mtcars data set in R.\n\ndf_report(mtcars)\n\n[1] \"This dataframe contains 32 rows and 11 columns. There are 0 NA entries.\"\n\n\n\n\n2.5.4 Installing packages\nAll the common functions that we have used so far exist in the base R installation. However, the beauty of open source languages like R is that users can write their own functions or resources and release them to others via packages. A package is an additional module that can be installed easily; it makes resources available which are not in the base R installation. In this book we will be using functions from both base R and from popular and useful packages. As an example, a popular package used for statistical modeling is the MASS package, which is based on methods in a popular applied statistics book3.\nBefore an external package can be used, it must be installed into your package library using install.packages(). So to install MASS, type install.packages(\"MASS\") into the console. This will send R to the main internet repository for R packages (known as CRAN). It will find the right version of MASS for your operating system and download and install it into your package library. If MASS needs other packages in order to work, it will also install these packages.\nIf you want to install more than one package, put the names of the packages inside a character vector—for example:\n\nmy_packages &lt;- c(\"MASS\", \"DescTools\", \"dplyr\")\ninstall.packages(my_packages)\n\nOnce you have installed a package, you can see what functions are available by calling for help on it, for example using help(package = MASS). One package you may wish to install now is the peopleanalyticsdata package, which contains all the data sets used in this book. By installing and loading this package, all the data sets used in this book will be loaded into your R session and ready to work with. If you do this, you can ignore the read.csv() commands later in the book, which download the data from the internet.\n\n\n2.5.5 Using packages\nOnce you have installed a package into your package library, to use it in your R session you need to load it using the library() function. For example, to load MASS after installing it, use library(MASS). Often nothing will happen when you use this command, but rest assured the package has been loaded and you can start to use the functions inside it. Sometimes when you load the package a series of messages will display, usually to make you aware of certain things that you need to keep in mind when using the package. Note that whenever you see the library() command in this book, it is assumed that you have already installed the package in that command. If you have not, the library() command will fail.\nOnce a package is loaded from your library, you can use any of the functions inside it. For example, the stepAIC() function is not available before you load the MASS package but becomes available after it is loaded. In this sense, functions ‘belong’ to packages.\nProblems can occur when you load packages that contain functions with the same name as functions that already exist in your R session. Often the messages you see when loading a package will alert you to this. When R is faced with a situation where a function exists in multiple packages you have loaded, R always defaults to the function in the most recently loaded package. This may not always be what you intended.\nOne way to completely avoid this issue is to get in the habit of namespacing your functions. To namespace, you simply use package::function(), so to safely call stepAIC() from MASS, you use MASS::stepAIC(). Most of the time in this book when a function is being called from a package outside base R, I use namespacing to call that function. This should help avoid confusion about which packages are being used for which functions.\n\n\n2.5.6 The pipe operator\nEven in the most elementary briefing about R, it is very difficult to ignore the pipe operator. The pipe operator makes code more natural to read and write and reduces the typical computing problem of many nested operations inside parentheses.\nAs an example, imagine we wanted to do the following two operations in one command:\n\nSubset salespeople to only the sales values of those with sales less than 500\nTake the mean of those values\n\nIn base R, one way to do this is:\n\nmean(subset(salespeople$sales, subset = salespeople$sales &lt; 500))\n\n[1] 388.6684\n\n\nThis is nested and needs to be read from the inside out in order to align with the instructions. The pipe operator |&gt; takes the command that comes before it and places it inside the function that follows it (by default as the first argument). This reduces complexity and allows you to follow the logic more clearly.\n\n# use the pipe operator to lay out the steps more logically\nsubset(salespeople$sales, subset = salespeople$sales &lt; 500) |&gt; \n  mean() \n\n[1] 388.6684\n\n\nThis can be extended to perform arbitrarily many operations in one piped command.\n\nsalespeople$sales |&gt; # start with all data\n  subset(subset = salespeople$sales &lt; 500) |&gt; # get the subsetted data\n  mean() |&gt; # take the mean value\n  round() # round to the nearest integer\n\n[1] 389\n\n\nThe pipe operator is unique to R and is very widely used because it helps to make code more readable, it reduces complexity, and it helps orient around a common ‘grammar’ for the manipulation of data. The pipe operator helps you structure your code more clearly around nouns (objects), verbs (functions) and adverbs (arguments of functions). One of the most developed sets of packages in R that follows these principles is the tidyverse family of packages, which I encourage you to explore.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#errors-warnings-and-messages",
    "href": "basic_r.html#errors-warnings-and-messages",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.6 Errors, warnings and messages",
    "text": "2.6 Errors, warnings and messages\nAs I mentioned earlier in this chapter, getting familiar with R can be frustrating at the beginning if you have never programmed before. You can expect to regularly see messages, warnings or errors in response to your commands. I encourage you to regard these as your friend rather than your enemy. It is very tempting to take the latter approach when you are starting out, but over time I hope you will appreciate some wisdom from my words.\nErrors are serious problems which usually result in the halting of your code and a failure to return your requested output. They usually come with an indication of the source of the error, and these can sometimes be easy to understand and sometimes frustratingly vague and abstract. For example, an easy-to-understand error is:\n\nsubset(salespeople, subset = sales = 720)\n\nError: unexpected '=' in \"subset(salespeople, subset = sales =\"\nThis helps you see that you have used sales = 720 as a condition to subset your data, when you should have used sales == 720 for precise equality.\nA much more challenging error to understand is:\n\nhead[salespeople]\n\nError in head[salespeople] : object of type 'closure' is not subsettable\nWhen first faced with an error that you can’t understand, try not to get frustrated and proceed in the knowledge that it usually can be fixed easily and quickly. Often the problem is much more obvious than you think, and if not, there is still a 99% likelihood that others have made this error and you can read about it online. The first step is to take a look at your code to see if you can spot what you did wrong. In this case, you may see that you have used square brackets [] instead of parentheses () when calling your head() function. If you cannot see what is wrong, the next steps are to ask a colleague, do an internet search with the text of the error message you receive, ask a Generative AI coding assistant or consult online forums like https://stackoverflow.com. The more experienced you become, the easier it is to interpret error messages.\nWarnings are less serious and usually alert you to something that you might be overlooking and which could indicate a problem with the output. In many cases you can ignore warnings, but sometimes they are an important reminder to go back and edit your code. For example, you may run a model which doesn’t converge, and while this does not stop R from returning results, it is also very useful for you to know that it didn’t converge.\nMessages are pieces of information that may or may not be useful to you at a particular point in time. Sometimes you will receive messages when you load a package from your library. Sometimes messages will keep you up to date on the progress of a process that is taking a long time to execute.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#plotting-and-graphing",
    "href": "basic_r.html#plotting-and-graphing",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.7 Plotting and graphing",
    "text": "2.7 Plotting and graphing\nAs you might expect in a well-developed programming language, there are numerous ways to plot and graph information in R. If you are doing exploratory data analysis on fairly simple data and you don’t need to worry about pretty appearance or formatting, the built-in plot capabilities of base R are fine. If you need a pretty appearance, more precision, color coding or even 3D graphics or animation, there are also specialized plotting and graphing packages for these purposes. In general when working interactively in RStudio, graphical output will be rendered in the Plots pane, where you can copy it or save it as an image.\n\n2.7.1 Plotting in base R\nThe simplest plot function in base R is plot(). This performs basic X-Y plotting. As an example, this code will generate a scatter plot of customer_rate against sales in the salespeople data set, with the results displayed in Figure 2.3. Note the use of the arguments main, xlab and ylab for customizing the axis labels and title for the plot.\n\n\n\n\n# scatter plot of customer_rate against sales\nplot(x = salespeople$sales, y = salespeople$customer_rate,\n     xlab = \"Sales ($m)\", ylab = \"Average customer rating\",\n     main = \"Scatterplot of Sales vs Customer Rating\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Simple scatterplot of customer_rate against sales in the salespeople data set\n\n\n\nHistograms of data can be generated using the hist() function. This command will generate a histogram of performance as displayed in Figure 2.4. Note the use of breaks to customize how the bars appear.\n\n\n\n\n# convert performance ratings back to numeric data type for histogram\nsalespeople$performance &lt;- as.numeric(salespeople$performance)\n\n# histogram of performance ratings\nhist(salespeople$performance, breaks = 0:4,\n     xlab = \"Performance Rating\", \n     main = \"Histogram of Performance Ratings\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Simple histogram of performance in the salespeople data set\n\n\n\nBox and whisker plots are excellent ways to see the distribution of a variable, and can be grouped against another variable to see bivariate patterns. For example, this command will show a box and whisker plot of sales grouped against performance, with the output shown in Figure 2.5. Note the use of the formula and data notation here to define the variable we are interested in and how we want it grouped. We will study this formula notation in greater depth later in this book.\n\n\n\n\n# box plot of sales by performance rating\nboxplot(formula = sales ~ performance, data = salespeople,\n        xlab = \"Performance Rating\", ylab = \"Sales ($m)\",\n        main = \"Boxplot of Sales by Performance Rating\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Simple box plot of sales grouped against performance in the salespeople data set\n\n\n\nThese are among the most common plots used for data exploration purposes. They are examples of a wider range of plotting and graphing functions available in base R, such as line plots, bar plots and other varieties which you may see later in this book.\n\n\n2.7.2 Specialist plotting and graphing packages\nBy far the most commonly used specialist plotting and graphing package in R is ggplot2. ggplot2 allows the flexible construction of a very wide range of charts and graphs, but uses a very specific command grammar which can take some getting used to. However, once learned, ggplot2 can be an extremely powerful tool. Many of the illustratory figures used in this book are developed using ggplot2 and while the code for these figures is generally not included for the sake of brevity, you can always find it in the source code of this book on Github4. A great learning resource for ggplot2 is Wickham, Navarro, and Pedersen (2023).\nThe plotly package allows the use of the plotly graphing library in R. This is an excellent package for interactive graphing and is used for 3D illustrations in this book. Output can be rendered in HTML—allowing the user to play with and explore the graphs interactively—or can be saved as static 2D images.\nGGally is a package that extends ggplot2 to allow easy combination of charts and graphs. This is particularly valuable for quicker exploratory data analysis. One of its most popular functions is ggpairs(), which produces a pairplot. A pairplot is a visualization of all univariate and bivariate patterns in a data set, with univariate distributions in the diagonal and bivariate relationships or correlations displayed in the off-diagonal. Figure 2.6 is an example of a pairplot for the salespeople data set, which we will explore further in Chapter 5.\n\n\n\n\nlibrary(GGally)\n\n# convert performance and promotion to categorical\nsalespeople$promoted &lt;- as.factor(salespeople$promoted)\nsalespeople$performance &lt;- as.factor(salespeople$performance)\n\n# pairplot of salespeople\nGGally::ggpairs(salespeople)\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Pairplot of the salespeople data set",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#documenting-your-work-using-quarto",
    "href": "basic_r.html#documenting-your-work-using-quarto",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.8 Documenting your work using Quarto",
    "text": "2.8 Documenting your work using Quarto\nFor anyone performing any sort of multivariable analysis using a statistical programming language, appropriate documentation and reproducibility of the work is essential to its success and longevity. If your code is not easily obtained or run by others, it is likely to have a very limited impact and lifetime. Learning how to create integrated documents that contain both text and code is critical to providing access to your code and narration of your work.\nQuarto is a package which allows you to create integrated documents containing both formatted text and executed code. It is, in my opinion, one of the best resources available currently for this purpose. This entire book has been created using Quarto. You can start a Quarto document in RStudio by installing the by opening a new Quarto document file, which will have the suffix .qmd.\nQuarto documents always start with a particular heading type called a YAML header, which contains overall information on the document you are creating. Care must be taken with the precise formatting of the YAML header, as it is sensitive to spacing and indentation. Usually a basic YAML header is created for you in RStudio when you start a new .qmd file. Here is an example.\n---\ntitle: \"My new document\"\nauthor: \"Keith McNulty\"\ndate: 2025-11-10\nformat: html\n---\nThe format part of this header has numerous options, but the most commonly used are html, which generates your document as a web page, and pdf, which generates your document as a PDF using the open source LaTeX software package. If you wish to create PDF documents you will need to have a version of LaTeX installed on your system. Running the system command quarto install tinytex will install a minimal version of LaTeX which is fine for most purposes.\nQuarto allows you to build a formatted document using many shorthand formatting commands. Here are a few examples of how to format headings and place web links or images in your document:\n# My top heading\n\nThis section is about this general topic.\n\n## My first sub heading \n\nTo see more information on this sub-topic visit [here](https://my.web.link).\n\n## My second sub heading\n\nHere is a nice picture about this sub-topic.\n\n![](path/to/image)\nCode can be written and executed and the results displayed inline using backticks. For example, writing\n`{r} nrow(salespeople)`\ninline will display 351 in the final document. Entire code blocks can be included and executed by using triple-backticks. The following code block:\n```{r}\n# show the first few rows of salespeople\nhead(salespeople)\n```\nwill display this output:\n\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\nThe {} wrapping allows you to specify different languages for your code chunk. For example, if you wanted to run Python code instead of R code you can use {python}.\nYou can set options for your code chunks inside a code block by starting your line with #|. For example, if you want the results of your code to be displayed, but without the code itself being displayed, you can start your code block with the line #| echo: false.\nThe process of compiling your Quarto code to produce a document is known as ‘rendering’. To create a rendered document, you simply need to click on the ‘Render’ button in RStudio that appears above your Quarto code.\nIf you are not familiar with Quarto, I strongly encourage you to learn it alongside R and to challenge yourself to write up any practice exercises you take on in this book using Quarto. You can find a really helpful and thorough user guide for Quarto at https://quarto.org/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#learning-exercises",
    "href": "basic_r.html#learning-exercises",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.9 Learning exercises",
    "text": "2.9 Learning exercises\n\n2.9.1 Discussion questions\n\nDescribe the following data types: numeric, character, logical, factor.\nWhy is a vector known as a homogeneous data structure?\nGive an example of a heterogeneous data structure in R.\nWhat is the difference between NA and \"NA\"?\nWhat operator is used to return named elements of a list and named columns of a dataframe?\nDescribe some functions that are used to manipulate dataframes.\nWhat is a package and how do you install and use a new package?\nDescribe what is meant by ‘namespacing’ and why it might be useful.\nWhat is the pipe operator, and why is it popular in R?\nWhat is the difference between an error and a warning in R?\nName some simple plotting functions in base R.\nName some common specialist plotting and graphing packages in R.\nWhat is Quarto, and why is it useful to someone performing analysis using programming languages?\n\n\n\n2.9.2 Data exercises\n\nCreate a character vector called my_names that contains all your first, middle and last names as elements. Calculate the length of my_names.\nCreate a second numeric vector called which which corresponds to my_names. The entries should be the position of each name in the order of your full name. Verify that it has the same length as my_names.\nCreate a dataframe called names, which consists of the two vectors my_names and which as columns. Calculate the dimensions of names.\nCreate a new dataframe new_names with the which column converted to character type. Verify that your command worked using str().\nLoad the ugtests data set via the peopleanalyticsdata package or download it from the internet5. Calculate the dimensions of ugtests and view the first three rows only.\nView a statistical summary of all of the columns of ugtests. Determine if there are any missing values.\nView the subset of ugtests for values of Yr1 greater than 50.\nInstall and load the package dplyr. Look up the help for the filter() function in this package and try to use it to repeat the task in the previous question.\nWrite code to find the mean of the Yr1 test scores for all those who achieved Yr3 test scores greater than 100. Round this mean to the nearest integer.\nFamiliarize yourself with the two functions filter() and pull() from dplyr. Use these functions to try to do the same calculation in the previous question using a single unbroken piped command. Be sure to namespace where necessary.\nCreate a scatter plot using the ugtests data with Final scores on the \\(y\\) axis and Yr3 scores on the \\(x\\) axis.\nCreate your own 5-level grading logic and use it to create a new finalgrade column in the ugtests data set with grades 1–5 of increasing attainment based on the Final score in ugtests. Generate a histogram of this finalgrade column.\nUsing your new ugtests data with the extra column from the previous exercise, create a box plot of Yr3 scores grouped by finalgrade.\nRender all of your answers to these exercises into a Quarto document. Create one version that displays your code and answers, and another that just displays the answers.\n\n\n\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s.\n\n\nWickham, Hadley, Garrett Grolemund, and Mine Çetinkaya-Rundel. 2023. R for Data Science. https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023. ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "basic_r.html#footnotes",
    "href": "basic_r.html#footnotes",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "R also has some built-in data sets for testing and playing with. For example, check out mtcars by typing it into the terminal, or type data() to see a full list of built-in data sets.↩︎\nIt actually has a broader definition but is mostly used for showing the first rows of a dataframe.↩︎\nVenables and Ripley (2002)↩︎\nhttps://github.com/keithmcnulty/peopleanalytics-regression-book↩︎\nhttps://peopleanalytics-regression-book.org/data/ugtests.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "primer_stats.html",
    "href": "primer_stats.html",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "3.1 Elementary descriptive statistics of populations and samples\nTo properly understand multivariable models, an analyst needs to have a decent grasp of foundational statistics. Many of the assumptions and results of multivariable models require an understanding of these foundations in order to be properly interpreted. There are three topics that are particularly important for those proceeding further in this book:\nIf you have never really studied these topics, I would strongly recommend taking a course in them and spending good time getting to know them. Again, just as the last chapter was not intended to be a comprehensive tutorial on R, neither is this chapter intended to be a comprehensive tutorial on introductory statistics. However, we will introduce some key concepts here that are critical to understanding later chapters, and as always we will illustrate using real data examples.\nIn preparation for this chapter we are going to download a data set that we will work through in a later chapter, and use it for practical examples and illustration purposes. The data are a set of information on the sales, customer ratings and performance ratings on a set of 351 salespeople as well as an indication of whether or not they were promoted.\nLet’s take a brief look at the first few rows of this data to make sure we know what is inside it.\nAnd let’s understand the structure of this data.\nIt looks like:\nAny collection of numerical data on one or more variables can be described using a number of common statistical concepts. Let \\(x = x_1, x_2, \\dots, x_n\\) be a sample of \\(n\\) observations of a variable drawn from a population.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#elementary-descriptive-statistics-of-populations-and-samples",
    "href": "primer_stats.html#elementary-descriptive-statistics-of-populations-and-samples",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "3.1.1 Mean, variance and standard deviation\nThe mean is the average value of the observations and is defined by adding up all the values and dividing by the number of observations. The mean \\(\\bar{x}\\) of our sample \\(x\\) is defined as:\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^{n}x_i\n\\] While the mean of a sample \\(x\\) is denoted by \\(\\bar{x}\\), the mean of an entire population is usually denoted by \\(\\mu\\). The mean can have a different interpretation depending on the type of data being studied. Let’s look at the mean of three different columns of our salespeople data, making sure to ignore any missing data.\n\nmean(salespeople$sales, na.rm = TRUE)\n\n[1] 527.0057\n\n\nThis looks very intuitive and appears to be the average amount of sales made by the individuals in the data set.\n\nmean(salespeople$promoted, na.rm = TRUE)\n\n[1] 0.3219373\n\n\nGiven that this data can only have the value of 0 or 1, we interpret this mean as the likelihood or expectation that an individual will be labeled as 1. That is, the average probability of promotion in the data set. If this data showed a perfectly random likelihood of promotion, we would expect this to take the value of 0.5. But it is lower than 0.5, which tells us that the majority of individuals are not promoted.\n\nmean(salespeople$performance, na.rm = TRUE)\n\n[1] 2.5\n\n\nGiven that this data can only have the values 1, 2, 3 or 4, we interpret this as the expected value of the performance rating in the data set. Higher or lower means inform us about the distribution of the performance ratings. A low mean will indicate a skew towards a low rating, and a high mean will indicate a skew towards a high rating.\nOther common statistical summary measures include the median, which is the middle value when the values are ranked in order, and the mode, which is the most frequently occurring value.\nThe variance is a measure of how much the data varies around its mean. There are two different definitions of variance. The population variance assumes that that we are working with the entire population and is defined as the average squared difference from the mean:\n\\[\n\\mathrm{Var}_p(x) = \\frac{1}{n}\\sum_{i = 1}^{n}(x_i - \\bar{x})^2\n\\] The sample variance assumes that we are working with a sample and attempts to estimate the variance of a larger population by applying Bessel’s correction to account for potential sampling error. The sample variance is:\n\\[\n\\mathrm{Var}_s(x) = \\frac{1}{n-1}\\sum_{i = 1}^{n}(x_i - \\bar{x})^2\n\\]\nYou can see that\n\\[\n\\mathrm{Var}_p(x) = \\frac{n - 1}{n}\\mathrm{Var}_s(x)\n\\] So as the data set gets larger, the sample variance and the population variance become less and less distinguishable, which intuitively makes sense.\nBecause we rarely work with full populations, the sample variance is calculated by default in R and in many other statistical software packages.\n\n# sample variance \n(sample_variance_sales &lt;- var(salespeople$sales, na.rm = TRUE))\n\n[1] 34308.11\n\n\nSo where necessary, we need to apply a transformation to get the population variance.\n\n# population variance (need length of non-NA data)\nn &lt;- length(na.omit(salespeople$sales))\n(population_variance_sales &lt;- ((n-1)/n) * sample_variance_sales)\n\n[1] 34210.09\n\n\nVariance does not have intuitive scale relative to the data being studied, because we have used a ‘squared distance metric’‍, therefore we can square-root it to get a measure of ‘deviance’ on the same scale as the data. We call this the standard deviation \\(\\sigma(x)\\), where \\(\\mathrm{Var}(x) = \\sigma(x)^2\\). As with variance, standard deviation has both population and sample versions, and the sample version is calculated by default. Conversion between the two takes the form\n\\[\n\\sigma_p(x) = \\sqrt{\\frac{n-1}{n}}\\sigma_s(x)\n\\]\n\n# sample standard deviation\n(sample_sd_sales &lt;- sd(salespeople$sales, na.rm = TRUE))\n\n[1] 185.2245\n\n# verify that sample sd is sqrt(sample var)\nsample_sd_sales == sqrt(sample_variance_sales)\n\n[1] TRUE\n\n# calculate population standard deviation\n(population_sd_sales &lt;- sqrt((n-1)/n) * sample_sd_sales)\n\n[1] 184.9597\n\n\nGiven the range of sales is [151, 945] and the mean is 527, we see that the standard deviation gives a more intuitive sense of the ‘spread’ of the data relative to its inherent scale.\n\n\n3.1.2 Covariance and correlation\nThe covariance between two variables is a measure of the extent to which one changes as the other changes. If \\(y = y_1, y_2, \\dots, y_n\\) is a second variable, and \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of \\(x\\) and \\(y\\), respectively, then the sample covariance of \\(x\\) and \\(y\\) is defined as\n\\[\n\\mathrm{cov}_s(x, y) = \\frac{1}{n - 1}\\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\\] and as with variance, the population covariance is\n\\[\n\\mathrm{cov}_p(x, y) = \\frac{n-1}{n}\\mathrm{cov}_s(x, y)\n\\]\nAgain, the sample covariance is the default in R, and we need to transform to obtain the population covariance.\n\n# get sample covariance for sales and customer_rate, \n# ignoring observations with missing data\n(sample_cov &lt;- cov(salespeople$sales, salespeople$customer_rate, \n                   use = \"complete.obs\"))\n\n[1] 55.81769\n\n# convert to population covariance (need number of complete obs)\ncols &lt;- subset(salespeople, select = c(\"sales\", \"customer_rate\"))\nn &lt;- nrow(cols[complete.cases(cols), ])\n(population_cov &lt;- ((n-1)/n) * sample_cov)\n\n[1] 55.65821\n\n\nAs can be seen, the difference in covariance is very small between the sample and population versions, and both confirm a positive relationship between sales and customer rating. However, we again see this issue that there is no intuitive sense of scale for this measure.\nPearson’s correlation coefficient divides the covariance by the product of the standard deviations of the two variables:\n\\[\nr_{x, y} = \\frac{\\mathrm{cov}(x, y)}{\\sigma(x)\\sigma(y)}\n\\] This creates a scale of \\(-1\\) to \\(1\\) for \\(r_{x, y}\\), which is an intuitive way of understanding both the direction and strength of the relationship between \\(x\\) and \\(y\\), with \\(-1\\) indicating that \\(x\\) increases perfectly as \\(y\\) decreases, \\(1\\) indicating that \\(x\\) increases perfectly as \\(y\\) increases, and \\(0\\) indicating that there is no relationship between the two.\nAs before, there is a sample and population version of the correlation coefficient, and R calculates the sample version by default. Similar transformations can be used to determine a population correlation coefficient and over large samples the two measures converge.\n\n# calculate sample correlation between sales and customer_rate\ncor(salespeople$sales, salespeople$customer_rate, use = \"complete.obs\")\n\n[1] 0.337805\n\n\nThis tells us that there is a moderate positive correlation between sales and customer rating.\nYou will notice that we have so far used two variables on a continuous scale to demonstrate covariance and correlation. Pearson’s correlation can also be used between a continuous scale and a dichotomous (binary) scale variable, and this is known as a point-biserial correlation.\n\ncor(salespeople$sales, salespeople$promoted, use = \"complete.obs\")\n\n[1] 0.8511283\n\n\nCorrelating ranked variables involves an adjusted approach leading to Spearman’s rho (\\(\\rho\\)) or Kendall’s tau (\\(\\tau\\)), among others. We will not dive into the mathematics of this here, but a good source is Bhattacharya and Burman (2016). Spearman’s or Kendall’s variant should be used whenever at least one of the variables is a ranked variable, and both variants are available in R.\n\n# spearman's rho correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"spearman\", use = \"complete.obs\")\n\n[1] 0.2735446\n\n# kendall's tau correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"kendall\", use = \"complete.obs\")\n\n[1] 0.2073609\n\n\nIn this case, both indicate a low to moderate correlation. Spearman’s rho or Kendall’s tau can also be used to correlate a ranked and a dichotomous variable, and this is known as a rank-biserial correlation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#distribution-of-random-variables",
    "href": "primer_stats.html#distribution-of-random-variables",
    "title": "3  Statistics Foundations",
    "section": "3.2 Distribution of random variables",
    "text": "3.2 Distribution of random variables\nAs we outlined in Section 1.2, when we build a model we are using a set of sample data to infer a general relationship on a larger population. A major underlying assumption in our inference is that we believe the real-life variables we are dealing with are random in nature. For example, we might be trying to model the drivers of the voting choice of millions of people in a national election, but we may only have sample data on a few thousand people. When we infer nationwide voting intentions from our sample, we assume that the characteristics of the voting population are random variables.\n\n3.2.1 Sampling of random variables\nWhen we describe variables as random, we are assuming that they take a form which is independent and identically distributed. Using our salespeople data as an example, we are assuming that the sales of one person in the data set is not influenced by the sales of another person in the data set. In this case, this seems like a reasonable assumption, and we will be making it for many (though not all) of the statistical methods used in this book. However, it is good to recognize that there are scenarios where this assumption cannot be made. For example, if the salespeople worked together in serving the same customers on the same products, and each individual’s sales represented some proportion of the overall sales to the customer, we cannot say that the sales data is independent and identically distributed. In this case, we will expect to see some hierarchy in our data and will need to adjust our techniques accordingly to take this into consideration.\nUnder the central limit theorem, if we take samples from a random variable and calculate a summary statistic for each sample, that statistic is itself a random variable, and its mean converges to the true population statistic with more and more sampling. Let’s test this with a little experiment on our salespeople data. Figure 3.1 shows the results of taking 10, 100 and 1000 different random samples of 50, 100 and 150 salespeople from the salespeople data set and creating a histogram of the resulting mean sales values. We can see how greater numbers of samples (down the rows) lead to a more normal frequency distribution curve and larger sample sizes (across the columns) lead to a ‘spikier’ frequency distribution with a smaller standard deviation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Histogram and density of mean sales from the salespeople data set based on sample sizes of 50, 100 and 150 (columns) and 10, 100 and 1000 samplings (rows)\n\n\n\n\n\n3.2.2 Standard errors, the \\(t\\)-distribution and confidence intervals\nOne consequence of the observations in Figure 3.1 is that the summary statistics calculated from larger sample sizes fall into frequency distributions that are ‘narrower’ and hence represent more precise estimations of the population statistic. The standard deviation of a sampled statistic is called the standard error of that statistic. In the special case of a sampled mean, the formula for the standard error of the mean can be derived to be\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(\\sigma\\) is the (sample) standard deviation and \\(n\\) is the sample size1. This confirms that the standard error of the mean decreases with greater sample size, confirming our intuition that the estimation of the mean is more precise with larger samples.\nTo apply this logic to our salespeople data set, let’s take a random sample of 100 values of customer_rate.\n\n# set seed for reproducibility of sampling\nset.seed(123)\n\n# generate a sample of 100 observations\ncustrate &lt;- na.omit(salespeople$customer_rate)\nn &lt;- 100\nsample_custrate &lt;- sample(custrate, n)\n\nWe can calculate the mean of the sample and the standard error of the mean.\n\n# mean\n(sample_mean &lt;- mean(sample_custrate))\n\n[1] 3.6485\n\n# standard error\n(se &lt;- sd(sample_custrate)/sqrt(n))\n\n[1] 0.08494328\n\n\nBecause the normal distribution is a frequency distribution, we can interpret the standard error as a fundamental unit of ‘sensitivity’ around the sample mean. If we take ranges of values associated with greater multiples of standard errors around the sample mean, we can be confident that, over repeated samples, these ranges will contain the population mean with greater and greater frequency. For example, if we take a range of plus or minus one standard error around the sample mean, we can be confident that over repeated samples, this range will contain the true population mean approximately 68% of the time.\nTo calculate how many standard errors we would need around our sample mean to have 95% frequency of including the population mean, we need to use the \\(t\\)-distribution. The \\(t\\)-distribution is essentially an approximation of the normal distribution acknowledging that we only have a sample estimate of the true population standard deviation in how we calculate the standard error. In this case where we are dealing with a single sample mean, we use the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can use the qt() function in R to find the standard error multiple associated with the range of frequency we need. In this case, we are looking for our true population mean to be outside the top 2.5% or bottom 2.5% of the distribution2.\n\n# get se multiple for 0.975\n(t &lt;- qt(p = 0.975, df = n - 1))\n\n[1] 1.984217\n\n\nWe see that taking a range associated with approximately 1.98 standard errors on either side of our sample mean will ensure that our range will contain the population mean 95% of the time. This is called the 95% confidence interval3. Often practitioners will use a rough estimate for larger samples that the 95% confidence interval is 2 standard errors either side of the sample mean.\n\n# 95% confidence interval lower and upper bounds\nlower_bound &lt;- sample_mean - t*se\nupper_bound &lt;- sample_mean + t*se\n\ncat(paste0('[', lower_bound, ', ', upper_bound, ']')) \n\n[3.47995410046738, 3.81704589953262]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#sec-hyp-tests",
    "href": "primer_stats.html#sec-hyp-tests",
    "title": "3  Statistics Foundations",
    "section": "3.3 Hypothesis testing",
    "text": "3.3 Hypothesis testing\nObservations about the distribution of statistics on samples of random variables allow us to construct tests for hypotheses of difference or similarity. Such hypothesis testing is useful in itself for simple bivariate analysis in practice settings, but it will be particularly critical in later chapters in determining whether models are useful or not. Before we go through some technical examples of hypothesis testing, let’s overview the logic and intuition for how hypothesis testing works.\nThe purpose of hypothesis testing is to establish a high degree of statistical certainty regarding a claim of difference in a population based on the properties of a sample. Consistent with a high burden of proof, we start from the hypothesis that there is no difference, called the null hypothesis. We only reject the null hypothesis if the statistical properties of the sample data render it very unlikely to have occurred if the null hypothesis were true for the population, in which case we can consider the alternative hypothesis that a statistical difference does exist in the population.\nMost hypothesis tests can return a p-value, which is the maximum frequency of occurrence of the sample results (or results that are more extreme or unusual than the sample results) when the null hypothesis is true for the population. The analyst must decide on the level of p-value needed to reject the null hypothesis. This threshold is referred to as the significance level \\(\\alpha\\) (alpha). A common standard is to set \\(\\alpha\\) at 0.05. That is, we reject the null hypothesis if the p-value that we find for our sample results is less than 0.05. If we reject the null hypothesis at \\(\\alpha = 0.05\\), this means that the results we observe in the sample are so extreme or unusual that they would only occur in repeated random sampling at most 1 in 20 times if the null hypothesis were true. An alpha of 0.05 is not the only standard used in research and practice, and in some fields of study smaller alphas are the norm, particularly if erroneous conclusions might have very serious consequences.\nThree of the most common types of hypothesis tests are4:\n\nTesting for a difference in the means of two groups\nTesting for a non-zero correlation between two variables\nTesting for a difference in frequency distributions between different categories\n\nWe will go through an example of each of these. In each case, you will see a three-step process. First, we calculate a test statistic. Second, we determine an expected frequency distribution for that test statistic. Finally, we determine where our calculated statistic falls in that distribution in order to assess the frequency of our sample occurring if the null hypothesis is true. During these examples, we will go through all the logic and calculation steps needed to do the hypothesis testing, before we demonstrate the simple functions that perform all the steps for you in R. Readers don’t absolutely need to know all the details contained in this section, but a strong understanding of the underlying methods is encouraged.\n\n3.3.1 Testing for a difference in means (Welch’s \\(t\\)-test)\nImagine that we are asked if, in general, the sales of low-performing salespeople are different from the sales of high-performing salespeople. This question refers to all salespeople, but we only have data for the sample in our salespeople data set. Let’s take two subsets of our data for those with a performance rating of 1 and those with a performance rating of 4, and calculate the difference in mean sales.\n\n# take two performance group samples \nperf1 &lt;- subset(salespeople, subset = performance == 1)\nperf4 &lt;- subset(salespeople, subset = performance == 4)\n\n# calculate the difference in mean sales\n(diff &lt;- mean(perf4$sales) - mean(perf1$sales))\n\n[1] 154.9742\n\n\nWe can see that those with a higher performance rating in our sample did generate higher mean sales than those with a lower performance rating. But these are just samples, and we are being asked to give a conclusion about the populations they are drawn from.\nLet’s take a null hypothesis that there is no difference in true mean sales between the two performance groups that these samples are drawn from. We combine the two samples and calculate the distribution around the difference in means. To reject the null hypothesis at \\(\\alpha = 0.05\\), we would need to determine that the 95% confidence interval of this distribution does not contain zero.\nWe calculate the standard error of the combined sample using the formula5:\n\\[\n\\sqrt{\\frac{\\sigma_{\\mathrm{perf1}}^2}{n_{\\mathrm{perf1}}} + \\frac{\\sigma_{\\mathrm{perf4}}^2}{n_{\\mathrm{perf4}}}}\n\\] where \\(\\sigma_{\\mathrm{perf1}}\\) and \\(\\sigma_{\\mathrm{perf4}}\\) are the standard deviations of the two samples and \\(n_{\\mathrm{perf1}}\\) and \\(n_{\\mathrm{perf4}}\\) are the two sample sizes.\nWe use a special formula called the Welch-Satterthwaite approximation to calculate the degrees of freedom for the two samples, which in this case calculates to 100.986. This allows us to construct a 95% confidence interval for the difference between the means, and we can test whether this contains zero.\n\n# calculate standard error of the two sets\nse &lt;- sqrt(sd(perf1$sales)^2/length(perf1$sales) \n           + sd(perf4$sales)^2/length(perf4$sales))\n\n# calculate the required t-statistic\nt &lt;- qt(p = 0.975, df = 100.98)\n\n# calculate 95% confidence interval\n(lower_bound &lt;- diff - t*se)\n\n[1] 88.56763\n\n(upper_bound &lt;- diff + t*se)\n\n[1] 221.3809\n\n# test if zero is inside this interval\n(0 &lt;= upper_bound) & (0 &gt;= lower_bound)\n\n[1] FALSE\n\n\nSince this has returned FALSE, we conclude that a mean difference of zero is outside the 95% confidence interval of our sample mean difference. We therefore reject the null hypothesis that the mean sales of both performance levels are the same.\nLooking at this graphically, we are assuming a \\(t\\)-distribution of the mean difference, and we are determining where zero sits in that distribution, as in Figure 3.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: \\(t\\)-distribution of the mean sales difference between perf1 and perf4, 95% confidence intervals (red dashed lines) and a zero difference (blue dot-dash line)\n\n\n\nThe red dashed lines in this diagram represent the 95% confidence interval around the mean difference of our two samples. The ‘tails’ of the curve outside of these two lines each represent a maximum of 0.025 frequency for the true population value under repeated random sampling. So we can see that the position of the blue dot-dashed line can correspond to a maximum frequency that the population mean difference is zero. This is the p-value of the hypothesis test7.\nThe p-value can be derived by calculating the standard error multiple associated with zero in the \\(t\\)-distribution (called the \\(t\\)-statistic or \\(t\\)-value), by applying the conversion function pt() to obtain the upper tail probability and then multiplying by 2 to get the probability associated with both tails of the distribution.\n\n# get t-statistic\nt_actual &lt;- diff/se \n\n# convert t-statistic to p-value\n2*pt(t_actual, df = 100.98, lower = FALSE)\n\n[1] 1.093212e-05\n\n\nNowadays, it is never necessary to do these manual calculations ourselves because hypothesis tests are a standard part of statistical software. In R, the t.test() function performs a hypothesis test of difference in means of two samples and confirms our manually calculated p-value and 95% confidence interval.\n\nt.test(perf4$sales, perf1$sales)\n\n\n    Welch Two Sample t-test\n\ndata:  perf4$sales and perf1$sales\nt = 4.6295, df = 100.98, p-value = 1.093e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  88.5676 221.3809\nsample estimates:\nmean of x mean of y \n 619.8909  464.9167 \n\n\nBecause our p-value is less than our alpha of 0.05, we reject the null hypothesis. The standard \\(\\alpha = 0.05\\) is associated with the term statistically significant. Therefore we could say here that there is statistically significant evidence of a difference in mean sales.\nIn practice, there are numerous alphas that are of interest to analysts, each reflecting different levels of certainty. While 0.05 is the most common standard in many disciplines, more stringent alphas of 0.01 and 0.001 are often used in situations where a high degree of certainty is desirable (for example, some medical fields). Similarly, a less stringent alpha standard of 0.1 can be of interest particularly when sample sizes are small and the analyst is satisfied with ‘indications’ from the data. In many statistical software packages, including those that we will see in this book, tests that meet an \\(\\alpha = 0.1\\) standard are usually marked with period(.), those that meet \\(\\alpha = 0.05\\) with an asterisk(*), \\(\\alpha = 0.01\\) a double asterisk(**) and \\(\\alpha = 0.001\\) a triple asterisk(***).\nMany leading statisticians have argued that p-values are more a test of sample size than anything else and have cautioned against too much of a focus on p-values in making statistical conclusions from data. In particular, situations where data and methodology have been deliberately manipulated to achieve certain alpha standards—a process known as ‘p-hacking’—has been of increasing concern recently. See Chapter 11 for a better understanding of how the significance level and the sample size contribute to determining statistical power in hypothesis testing, and see Chapter 12 for an alternative statistical philosophy that avoids some of the pitfalls of p-values.\n\n\n3.3.2 Testing for a non-zero correlation between two variables (\\(t\\)-test for correlation)\nImagine that we are given a sample of data for two variables and we are asked if the variables are correlated in the overall population. We can take a null hypothesis that the variables are not correlated, determine a t-statistic associated with a zero correlation and convert this to a p-value. The t-statistic associated with a correlation \\(r\\) between two samples of length \\(n\\) is often notated \\(t^*\\) and is defined as\n\\[\nt^* = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\] \\(t^*\\) can be converted to an associated p-value using a \\(t\\)-distribution in a similar way to the previous section, this time with \\(n - 2\\) degrees of freedom in our \\(t\\)-distribution. As an example, let’s calculate \\(t^*\\) for the correlation between sales and customer rating in our sample and convert it to a p-value.\n\n# remove NAs from salespeople\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# calculate t_star\nr &lt;- cor(salespeople$sales, salespeople$customer_rate)\nn &lt;- nrow(salespeople)\nt_star &lt;- (r*sqrt(n - 2))/sqrt(1 - r^2)\n\n# convert to p-value on t-distribution with n - 2 degrees of freedom\n2*pt(t_star, df = n - 2, lower = FALSE)\n\n[1] 8.647952e-11\n\n\nAgain, there is a useful function in R to cut out the need for all our manual calculations. The cor.test() function in R performs a hypothesis test on the null hypothesis that two variables have zero correlation.\n\ncor.test(salespeople$sales, salespeople$customer_rate)\n\n\n    Pearson's product-moment correlation\n\ndata:  salespeople$sales and salespeople$customer_rate\nt = 6.6952, df = 348, p-value = 8.648e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2415282 0.4274964\nsample estimates:\n     cor \n0.337805 \n\n\nThis confirms our manual calculations, and we see the null hypothesis has been rejected and we can conclude that there is statistically significant evidence of a correlation between sales and customer rating.\n\n\n3.3.3 Testing for a difference in frequency distribution between different categories in a data set (Chi-square test)\nImagine that we are asked if the performance category of each person in the salespeople data set has a relationship with their promotion likelihood. We will test the null hypothesis that there is no difference in the distribution of promoted versus not promoted across the four performance categories.\nFirst we can produce a contingency table, which is a matrix containing counts of how many people were promoted or not promoted in each category.\n\n# create contingency table of promoted vs performance\n(contingency &lt;- table(salespeople$promoted, salespeople$performance))\n\n   \n     1  2  3  4\n  0 50 85 77 25\n  1 10 25 48 30\n\n\nWe can see by summing each row that for the total sample we can expect 113 people to be promoted and 237 to miss out on promotion. We can use this ratio to compute an expected proportion in each performance category under the assumption that the distribution was exactly the same across all four categories.\n\n# calculate expected promoted and not promoted\n(expected_promoted &lt;- (sum(contingency[2, ])/sum(contingency)) * \n   colSums(contingency))\n\n       1        2        3        4 \n19.37143 35.51429 40.35714 17.75714 \n\n(expected_notpromoted &lt;- (sum(contingency[1, ])/sum(contingency)) * \n    colSums(contingency))\n\n       1        2        3        4 \n40.62857 74.48571 84.64286 37.24286 \n\n\nNow we can compare our observed versus expected values using the difference metric:\n\\[\n\\frac{(\\mathrm{observed} - \\mathrm{expected})^2}{\\mathrm{expected}}\n\\] and add these all up to get a total, known as the \\(\\chi^2\\) statistic.\n\n# calculate the difference metrics for promoted and not promoted\npromoted &lt;- sum((expected_promoted - contingency[2, ])^2/\n                  expected_promoted)\n\nnotpromoted &lt;- sum((expected_notpromoted - contingency[1, ])^2/\n                     expected_notpromoted)\n\n# calculate chi-squared statistic\n(chi_sq_stat &lt;- notpromoted + promoted)\n\n[1] 25.89541\n\n\nThe \\(\\chi^2\\) statistic has an expected frequency distribution that can be used to determine the p-value associated with this statistic. As with the \\(t\\)-distribution, the \\(\\chi^2\\)-distribution depends on the degrees of freedom. This is calculated by subtracting one from the number of rows and from the number of columns in the contingency table and multiplying them together. In this case we have 2 rows and 4 columns, which calculates to 3 degrees of freedom. Armed with our \\(\\chi^2\\) statistic and our degrees of freedom, we can now calculate the p-value for the hypothesis test using the pchisq() function.\n\n# calculate p-value from chi_squared stat\npchisq(chi_sq_stat, df = 3, lower.tail=FALSE)\n\n[1] 1.003063e-05\n\n\nThe chisq.test() function in R performs all the steps involved in a chi-square test of independence on a contingency table and returns the \\(\\chi^2\\) statistic and associated p-value for the null hypothesis, in this case confirming our manual calculations.\n\nchisq.test(contingency)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency\nX-squared = 25.895, df = 3, p-value = 1.003e-05\n\n\nAgain, we can reject the null hypothesis and say that there is statistically significant evidence of a difference in the distribution of promoted/not promoted individuals between the four performance categories.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#foundational-statistics-in-python",
    "href": "primer_stats.html#foundational-statistics-in-python",
    "title": "3  Statistics Foundations",
    "section": "3.4 Foundational statistics in Python",
    "text": "3.4 Foundational statistics in Python\nElementary descriptive statistics can be performed in Python using various packages. Descriptive statistics of numpy arrays are usually available as methods.\n\nimport pandas as pd\nimport numpy as np\n\n# get data\nurl = \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# mean sales\nmean_sales = salespeople.sales.mean()\nprint(mean_sales)\n\n527.0057142857142\n\n\n\n# sample variance\nvar_sales = salespeople.sales.var()\nprint(var_sales)\n\n34308.11458043389\n\n\n\n# sample standard deviation\nsd_sales = salespeople.sales.std()\nprint(sd_sales)\n\n185.2244977869663\n\n\nPopulation statistics can be obtained by setting the ddof parameter to zero.\n\n# population standard deviation\npopsd_sales = salespeople.sales.std(ddof = 0)\nprint(popsd_sales)\n\n184.9597020864771\n\n\nThe numpy covariance function produces a covariance matrix.\n\n# generate a sample covariance matrix between two variables\nsales_rate = salespeople[['sales', 'customer_rate']]\nsales_rate = sales_rate[~np.isnan(sales_rate)]\ncov = sales_rate.cov()\nprint(cov)\n\n                      sales  customer_rate\nsales          34308.114580      55.817691\ncustomer_rate     55.817691       0.795820\n\n\nSpecific covariances between variable pairs can be pulled out of the matrix.\n\n# pull out specific covariances\nprint(cov['sales']['customer_rate'])\n\n55.81769119934507\n\n\nSimilarly for Pearson correlation:\n\n# sample pearson correlation matrix\ncor = sales_rate.corr()\nprint(cor)\n\n                  sales  customer_rate\nsales          1.000000       0.337805\ncustomer_rate  0.337805       1.000000\n\n\nSpecific types of correlation coefficients can be accessed via the stats module of the scipy package.\n\nfrom scipy import stats\n\n# spearman's correlation\nstats.spearmanr(salespeople.sales, salespeople.performance, \nnan_policy='omit')\n\nSignificanceResult(statistic=np.float64(0.27354459847452534), pvalue=np.float64(2.0065434379079837e-07))\n\n\n\n# kendall's tau\nstats.kendalltau(salespeople.sales, salespeople.performance, \nnan_policy='omit')\n\nSignificanceResult(statistic=np.float64(0.20736088105812), pvalue=np.float64(2.7353258226376403e-07))\n\n\nCommon hypothesis testing tools are available in scipy.stats. Here is an example of how to perform Welch’s \\(t\\)-test on a difference in means of samples of unequal variance.\n\n# get sales for top and bottom performers\nperf1 = salespeople[salespeople.performance == 1].sales\nperf4 = salespeople[salespeople.performance == 4].sales\n\n# welch's t-test with unequal variance\nttest = stats.ttest_ind(perf4, perf1, equal_var=False)\nprint(ttest)\n\nTtestResult(statistic=np.float64(4.629477606844271), pvalue=np.float64(1.0932443461577037e-05), df=np.float64(100.9768911762055))\n\n\nAs seen above, hypothesis tests for non-zero correlation coefficients are performed automatically as part of scipy.stats correlation calculations.\n\n# calculate correlation and p-value \nsales = salespeople.sales[~np.isnan(salespeople.sales)]\n\ncust_rate = salespeople.customer_rate[\n  ~np.isnan(salespeople.customer_rate)\n]\n\ncor = stats.pearsonr(sales, cust_rate)\nprint(cor)\n\nPearsonRResult(statistic=np.float64(0.337805044858678), pvalue=np.float64(8.647952212092207e-11))\n\n\nFinally, a chi-square test of difference in frequency distribution can be performed on a contingency table as follows. The first value of the output is the \\(\\chi^2\\) statistic, and the second value is the p-value.\n\n# create contingency table for promoted versus performance\ncontingency = pd.crosstab(salespeople.promoted, salespeople.performance)\n\n# perform chi-square test\nchi2_test = stats.chi2_contingency(contingency)\nprint(chi2_test)\n\nChi2ContingencyResult(statistic=np.float64(25.895405268094862), pvalue=np.float64(1.0030629464566802e-05), dof=3, expected_freq=array([[40.62857143, 74.48571429, 84.64285714, 37.24285714],\n       [19.37142857, 35.51428571, 40.35714286, 17.75714286]]))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#learning-exercises",
    "href": "primer_stats.html#learning-exercises",
    "title": "3  Statistics Foundations",
    "section": "3.5 Learning exercises",
    "text": "3.5 Learning exercises\n\n3.5.1 Discussion questions\nWhere relevant in these discussion exercises, let \\(x = x_1, x_2, \\dots, x_n\\) and \\(y = y_1, y_2, \\dots, y_m\\) be samples of two random variables of length \\(n\\) and \\(m\\) respectively.\n\nIf the values of \\(x\\) can only take the form 0 or 1, and if their mean is 0.25, how many of the values equal 0?\nIf \\(m = n\\) and \\(x + y\\) is formed from the element-wise sum of \\(x\\) and \\(y\\), show that the mean of \\(x + y\\) is equal to the sum of the mean of \\(x\\) and the mean of \\(y\\).\nFor a scalar multiplier \\(a\\), show that \\(\\mathrm{Var}(ax) = a^2\\mathrm{Var}(x)\\).\nExplain why the standard deviation of \\(x\\) is a more intuitive measure of the deviation in \\(x\\) than the variance.\nDescribe which two types of correlation you could use if \\(x\\) is an ordered ranking.\nDescribe the role of sample size and sampling frequency in the distribution of sampling means for a random variable.\nDescribe what a standard error of a statistic is and how it can be used to determine a confidence interval for the true population statistic.\nIf we conduct a t-test on the null hypothesis that \\(x\\) and \\(y\\) are drawn from populations with the same mean, describe what a p-value of 0.01 means.\nExtension: The sum of variance law states that, for independent random variables \\(x\\) and \\(y\\), \\(\\mathrm{Var}(x \\pm y) = \\mathrm{Var}(x) + \\mathrm{Var}(y)\\). Use this together with the identity from Exercise 3 to derive the formula for the standard error of the mean of \\(x = x_1, x_2, \\dots, x_n\\):\n\n\\[\nSE = \\frac{\\sigma(x)}{\\sqrt{n}}\n\\]\n\nExtension: In a similar way to Exercise 9, show that the standard error for the difference between the means of \\(x\\) and \\(y\\) is\n\n\\[\n\\sqrt{\\frac{\\sigma(x)^2}{n} + \\frac{\\sigma(y)^2}{m}}\n\\]\n\n\n3.5.2 Data exercises\nFor these exercises, load the charity_donation data set via the peopleanalyticsdata package, or download it from the internet8. This data set contains information on a sample of individuals who made donations to a nature charity.\n\nCalculate the mean total_donations from the data set.\nCalculate the sample variance for total_donation and convert this to a population variance.\nCalculate the sample standard deviation for total_donations and verify that it is the same as the square root of the sample variance.\nCalculate the sample correlation between total_donations and time_donating. By using an appropriate hypothesis test, determine if these two variables are independent in the overall population.\nCalculate the mean and the standard error of the mean for the first 20 entries of total_donations.\nCalculate the mean and the standard error of the mean for the first 50 entries of total_donations. Verify that the standard error is less than in Exercise 5.\nBy using an appropriate hypothesis test, determine if the mean age of those who made a recent donation is different from those who did not.\nBy using an appropriate hypothesis test, determine if there is a difference in whether or not a recent donation was made according to where people reside.\nExtension: By using an appropriate hypothesis test, determine if the age of those who have recently donated is at least 10 years older than those who have not recently donated in the population.\nExtension: By using an appropriate hypothesis test, determine if the average donation amount is at least 10 dollars higher for those who recently donated versus those who did not. Retest for 20 dollars higher.\n\n\n\n\n\nBhattacharya, P. K., and Prabir Burman. 2016. Theory and Methods of Statistics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "primer_stats.html#footnotes",
    "href": "primer_stats.html#footnotes",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "Note that this formula assumes that the sample standard deviation is a close approximation of the population standard deviation, which is generally fine for samples that are not very small.↩︎\nAs sample sizes increase and sample statistics get very close to population statistics, whether we use a \\(t\\)-distribution or a \\(z\\)-distribution (normal distribution) for determining confidence intervals or p-values becomes less important as they become almost identical on large samples. The output of some later models will refer to \\(t\\)-statistics and others to \\(z\\)-statistics, but the difference is only likely to matter in small samples of less than 50 or so observations. In this chapter we will use the \\(t\\)-distribution as it is a better choice for all sample sizes.↩︎\nNote that our ‘confidence’ is based on the frequency with which our process, when done with repeated random samplng, will result in what we want. It is not based on a direct measurement of the likelihood that our interval will contain the population mean. This is why this statistical philosophy is often labelled as ‘frequentist’. Later, in Chapter 12, we will introduce an alternative philosopy which does allow us to make precise statements about the likelihood of our interval containing the population mean.↩︎\nWe go through these three examples both because they are relatively common and to illustrate the details of the logic behind hypothesis testing. By understanding how hypothesis tests work, this will allow the reader to grasp the meaning of other such tests like the F-test or the Wald test, which we will refer to in later chapters of this book↩︎\nIf you are inquisitive about this formula, see the exercises at the end of this chapter.↩︎\nI’ve kept the gory details of how this is derived out of view, but you can see them if you view the source code for this book.↩︎\nWe call this type of hypothesis test a two-tailed test, because the tested population mean can be either higher or lower than the sample mean, thus it can appear in any of the two tails for the null hypothesis to be rejected. One-tailed tests are used when you are testing for an alternative hypothesis that the difference is specifically ‘less than zero’ or ‘greater than zero’‍. In the t.test() function in R, you can specify this in the arguments.↩︎\nhttps://peopleanalytics-regression-book.org/data/charity_donation.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "linear_regression.html",
    "href": "linear_regression.html",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "",
    "text": "4.1 When to use it\nIn this chapter, we will introduce and explore linear regression, one of the first learning methods to be developed by statisticians and one of the easiest to interpret. Despite its simplicity—indeed because of its simplicity—it can be a very powerful tool in many situations. Linear regression will often be the first methodology to be trialed on a given problem, and will give an immediate benchmark with which to judge the efficacy of other, more complex, modeling techniques. Given the ease of interpretation, many analysts will select a linear regression model over more complex approaches even if those approaches produce a slightly better fit. This chapter will also introduce many critical concepts that will apply to other modeling approaches as we proceed through this book. Therefore for inexperienced modelers this should be considered a foundational chapter which should not be skipped.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#sec-when-ols",
    "href": "linear_regression.html#sec-when-ols",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "",
    "text": "4.1.1 Origins and intuition of linear regression\nLinear regression, also known as Ordinary Least Squares linear regression or OLS regression for short, was developed independently by the mathematicians Gauss and Legendre at or around the first decade of the 19th century, and there remains today some controversy about who should take credit for its discovery. However, at the time of its discovery it was not actually known as ‘regression’‍. This term became more popular following the work of Francis Galton—a British intellectual jack-of-all-trades and a cousin of Charles Darwin. In the late 1800s, Galton had researched the relationship between the heights of a population of almost 1000 children and the average height of their parents (mid-parent height). He was surprised to discover that there was not a perfect relationship between the height of a child and the average height of its parents, and that in general children’s heights were more likely to be in a range that was closer to the mean for the total population. He described this statistical phenomenon as a ‘regression towards mediocrity’ (‘regression’ comes from a Latin term approximately meaning ‘go back’).\nFigure 4.1 is a scatter plot of Galton’s data with the black solid line showing what a perfect relationship would look like, the black dot-dashed line indicating the mean child height and the red dashed line showing the actual relationship determined by Galton1. You can regard the red dashed line as ‘going back’ from the perfect relationship (symbolized by the black line). This might give you an intuition that will help you understand later sections of this chapter. In an arbitrary data set, the red dashed line can lie anywhere between the black dot-dashed line (no relationship) and the black solid line (a perfect relationship). Linear regression is about finding the red dashed line in your data and using it to explain the degree to which your input data (the \\(x\\) axis) explains your outcome data (the \\(y\\) axis).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Galton’s study of the height of children introduced the term ‘regression’\n\n\n\n\n\n4.1.2 Use cases for linear regression\nLinear regression is particularly suited to a problem where the outcome of interest is on some sort of continuous scale (for example, quantity, money, height, weight). For outcomes of this type, it can be a first port of call before trying more complex modeling approaches. It is simple and easy to explain, and analysts will often accept a somewhat poorer fit using linear regression in order to avoid having to interpret a more complex model.\nHere are some illustratory examples of questions that could be tackled with a linear regression approach:\n\nGiven a data set of demographic data, job data and current salary data, to what extent can current salary be explained by the rest of the data?\nGiven annual test scores for a set of students over a four-year period, what is the relationship between the final test score and earlier test scores?\nGiven a set of GPA data, SAT data and data on the percentile score on an aptitude test for a set of job applicants, to what extent can the GPA and SAT data explain the aptitude test score?\n\n\n\n4.1.3 Walkthrough example\nYou are working as an analyst for the biology department of a large academic institution which offers a four-year undergraduate degree program. The academic leaders of the department are interested in understanding how student performance in the final-year examination of the degree program relates to performance in the prior three years.\nTo help with this, you have been provided with data for 975 individuals graduating in the past three years, and you have been asked to create a model to explain each individual’s final examination score based on their examination scores for the first three years of their program. The Year 1 examination scores are awarded on a scale of 0–100, Years 2 and 3 on a scale of 0–200, and the Final year is awarded on a scale of 0–300.\nWe will load the ugtests data set into our session and take a brief look at it.\n\n# if needed, download ugtests data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests &lt;- read.csv(url)\n\n\n# look at the first few rows of data\nhead(ugtests)\n\n  Yr1 Yr2 Yr3 Final\n1  27  50  52    93\n2  70 104 126   207\n3  27  36 148   175\n4  26  75 115   125\n5  46  77  75   114\n6  86 122 119   159\n\n\nThe data looks as expected, with test scores for four years all read in as numeric data types, but of course this is only a few rows. We need a quick statistical and structural overview of the data.\n\n# view structure\nstr(ugtests)\n\n'data.frame':   975 obs. of  4 variables:\n $ Yr1  : int  27 70 27 26 46 86 40 60 49 80 ...\n $ Yr2  : int  50 104 36 75 77 122 100 92 98 127 ...\n $ Yr3  : int  52 126 148 115 75 119 125 78 119 67 ...\n $ Final: int  93 207 175 125 114 159 153 84 147 80 ...\n\n# view statistical summary\nsummary(ugtests)\n\n      Yr1             Yr2             Yr3            Final    \n Min.   : 3.00   Min.   :  6.0   Min.   :  8.0   Min.   :  8  \n 1st Qu.:42.00   1st Qu.: 73.0   1st Qu.: 81.0   1st Qu.:118  \n Median :53.00   Median : 94.0   Median :105.0   Median :147  \n Mean   :52.15   Mean   : 92.4   Mean   :105.1   Mean   :149  \n 3rd Qu.:62.00   3rd Qu.:112.0   3rd Qu.:130.0   3rd Qu.:175  \n Max.   :99.00   Max.   :188.0   Max.   :198.0   Max.   :295  \n\n\nWe can see that the results do seem to have different scales in the different years as we have been informed, and judging by the means, students seem to have found Year 2 exams more challenging. We can also be assured that there is no missing data, as these would have been displayed as NA counts in our summary if they existed.\nWe can also plot our four years of test scores pairwise to see any initial relationships of interest, as displayed in Figure 4.2.\n\n\n\n\nlibrary(GGally)\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Pairplot of the ugtests data set\n\n\n\nIn the diagonal, we can see the distributions of the data in each column. We observe relatively normal-looking distributions in each year. We can see scatter plots and pairwise correlation statistics off the diagonal. For example, we see a particularly strong correlation between Yr3 and Final test scores, a moderate correlation between Yr2 and Final and relative independence elsewhere.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#sec-simple-ols",
    "href": "linear_regression.html#sec-simple-ols",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.2 Simple linear regression",
    "text": "4.2 Simple linear regression\nIn order to visualize our approach and improve our intuition, we will start with simple linear regression, which is the case where there is only a single input variable and outcome variable.\n\n4.2.1 Linear relationship between a single input and an outcome\nLet our input variable be \\(x\\) and our outcome variable be \\(y\\). In Chapter 1, we introduced the concept of parametric modeling as an assumption that our outcome variable \\(y\\) is expected to be related to the input variable \\(x\\) by means of some mathematically definable function. In other words, we assume:\n\\[\ny_i = f(x_i) + \\epsilon_i\n\\]\nwhere \\(y_i\\) and \\(x_i\\) are individual observations of our outcome and input variables, \\(f\\) is some mathematical transformation and \\(\\epsilon_i\\) is some random, uncontrollable error.\nRecalling the equation of a straight line, because we assume that the expected relationship is linear, it will take the form:\n\\[f(x_i) = mx_i + c\\] where \\(m\\) represents the slope or gradient of the line, and \\(c\\) represents the point at which the line intercepts the \\(y\\) axis. We call \\(c\\) and \\(m\\) the coefficients of the model.\nAnother way of expressing all this is:\n\\[\ny_i = \\mu_i + \\epsilon_i\n\\] where, over many repeated observations, \\(\\mu_i = mx_i + c\\) is the mean value of \\(y_i\\) for a given value of \\(x_i\\), and \\(\\epsilon_i\\) is the random error term. Finally, we make the assumption that all our error terms \\(\\epsilon_i\\) are independent and identically distributed (i.i.d.) according to a normal distribution with mean 0 and some constant variance \\(\\sigma^2\\). In other words, we assume that:\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\] Now let’s assume that we have a sample of 10 observations with which to estimate our linear relationship. Let’s take the first 10 values of Yr3 and Final in our ugtests data set:\n\n(d &lt;- head(ugtests[ , c(\"Yr3\", \"Final\")], 10))\n\n   Yr3 Final\n1   52    93\n2  126   207\n3  148   175\n4  115   125\n5   75   114\n6  119   159\n7  125   153\n8   78    84\n9  119   147\n10  67    80\n\n\nWe can do a simple plot of these observations as in Figure 4.3. Intuitively, we can imagine a line passing through these points that ‘fits’ the general pattern. For example, taking \\(m = 1.2\\) and \\(c = 5\\), the resulting line \\(y = 1.2x + 5\\) could fit between the points we are given, as displayed in Figure 4.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Basic scatter plot of 10 observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Fitting \\(y=1.2x + 5\\) to our 10 observations\n\n\n\nThis looks like an approximation of the relationship, but how do we know that it is the best approximation?\n\n\n4.2.2 Minimising the error\nRemember that our assumed relationship \\(y_i = mx_i+ c\\) represents the expected mean value of \\(y_i\\) over many repeated observations. For a specific single observation of \\(x_i\\), we do not expect \\(y_i\\) to be precisely on our expected line, because each individual observation will have an error term \\(\\epsilon_i\\) either above or below the expected line. We can determine this error term \\(\\epsilon_i\\) in the fitted model by calculating the difference between the real value of \\(y_i\\) and the one predicted by our model. For example, at \\(x_i = 52\\), our modeled value of \\(y_i\\) is 67.4, but the real value is 93, producing an error \\(\\epsilon_i\\) of 25.6. These errors \\(\\epsilon_i\\) are known as the residuals of our model. The residuals for the 10 points in our data set are illustrated by the solid red line segments in Figure 4.5. It looks like at least one of our residuals is pretty large.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Residuals of \\(y=1.2x + 5\\) for our 10 observations\n\n\n\nThe error of our model—which we want to minimize—could be defined in a number of ways:\n\nThe average of our residuals \\(\\epsilon_i\\)\nThe average of the absolute values of our residuals \\(\\epsilon_i\\) (so that negative values are converted to positive values)\nThe average of the squares of our residuals \\(\\epsilon_i\\) (note that all squares are positive)\n\nFor a number of reasons (not least the fact that at the time this method was developed it was one of the easiest to derive), the most common approach is number 3, which is why we call our regression model Ordinary Least Squares regression. Some algebra and calculus can help us determine the equation of the line that generates the least-squared residual error. For more of the theory behind this, consult Montgomery, Peck, and Vining (2012), but let’s look at how this works in practice.\n\n\n4.2.3 Determining the best fit\nWe can run a fairly simple function in R to calculate the best fit linear model for our data. Once we have run that function, the model and all the details will be saved in our session for further investigation or use.\nFirst we need to express the model we are looking to calculate as a formula. In this simple case, we want to regress the outcome \\(y =\\) Final against the input \\(x =\\) Yr3, and therefore we would use the simple formula notation Final ~ Yr3. Now we can use the lm() function to calculate the linear model based on our data set and our formula.\n\n# calculate model\nmodel &lt;- lm(formula = Final ~ Yr3, data = d)\n\nThe model object that we have created is a list of a number of different pieces of information, which we can see by looking at the names of the objects in the list.\n\n# view the names of the objects in the model\nnames(model) \n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nSo we can already see some terms we are familiar with. For example, we can look at the coefficients.\n\nmodel$coefficients\n\n(Intercept)         Yr3 \n  16.630452    1.143257 \n\n\nThis tells us that that our best fit model—the one that minimizes the average squares of the residuals—is \\(y = 1.14x + 16.63\\). In other words, our Final test score can be expected to take a value of 16.63 even with zero score in the Yr3 input, and every additional point scored in Yr3 will increase the Final score by 1.14.\n\n\n4.2.4 Measuring the fit of the model\nWe have calculated a model which minimizes the average squared residual error for the sample of data that we have, but we don’t really have a sense of how ‘good’ the model is. How do we tell how well our model uses the input data to explain the outcome? This is an important question to answer because you would not want to propose a model that does not do a good job of explaining your outcome, and you also may need to compare your model to other alternatives, which will require some sort of benchmark metric.\nOne natural way to benchmark how good a job your model does of explaining the outcome is to compare it to a situation where you have no input and no model at all. In this situation, all you have is your outcome values, which can be considered a random variable with a mean and a variance. In the case of our 10 observations, we have 10 values of Final with a mean of 133.7. We can consider the horizontal line representing the mean of \\(y\\) as our ‘random model’‍, and we can calculate the residuals around the mean. This can be seen in Figure 4.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Residuals of our 10 observations around their mean value\n\n\n\nRecall from Section 3.1.1 the definition of the population variance of \\(y\\), notated as \\(\\mathrm{Var}(y)\\). Note that it is defined as the average of the squares of the residuals around the mean of \\(y\\). Therefore \\(\\mathrm{Var}(y)\\) represents the average squared residual error of a null or random model. This calculates in this case to 1574.21. Let’s overlay our fitted model onto this random model in Figure 4.7.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: Comparison of residuals of fitted model (red) against random variable (blue)\n\n\n\nSo for most of our observations (though not all) we seem to have reduced the ‘distance’ from the random model by fitting our new model. If we average the square of our residuals for the fitted model, we obtain the average squared residual error of our fitted model, which calculates to 398.35.\nTherefore, before we fit our model, we have an error of 1574.21, and after we fit it, we have an error of 398.35. So we have reduced the error of our model by 1175.86 or, expressed as a proportion, by 0.75. In other words, we can say that our model explains 0.75 (or 75%) of the variance of our outcome.\nThis metric is known as the \\(R^2\\) of our model and is the primary metric used in measuring the fit of a linear regression model2.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#sec-multiple-linear-regression",
    "href": "linear_regression.html#sec-multiple-linear-regression",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.3 Multiple linear regression",
    "text": "4.3 Multiple linear regression\nIn reality, regression problems rarely involve one single input variable, but rather multiple variables. The methodology for multiple linear regression is similar in nature to simple linear regression, but obviously more difficult to visualize because of its increased dimensionality.\nIn this case, our inputs are a set of \\(p\\) variables \\(x_1, x_2, \\dots, x_p\\). Extending the simple linear regression equation, we can express the expected relationship between an outcome observation \\(y_i\\) and its corresponding input observations \\(x_{i1}, x_{i2}, \\cdots, x_{ip}\\) as:\n\\[y_i = \\mu_i + \\epsilon_i\\]\nwhere\n\\[\n\\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\nand our \\(\\epsilon_i\\)’s are normally distributed with a mean of zero and constant variance. Here, \\(\\beta_0\\) is the intercept of the model, and \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) are the coefficients of the input variables, also known as slope coefficients. The goal of multiple linear regression is to determine the values of these coefficients so that our average squared residual error is minimized.\n\n4.3.1 Running a multiple linear regression model and interpreting its coefficients\nA multiple linear regression model is run in a similar way to a simple linear regression model, with your formula notation determining what outcome and input variables you wish to have in your model. Let’s now perform a multiple linear regression on our entire ugtests data set and regress our Final test score against all prior test scores using the formula Final ~ Yr3 + Yr2 + Yr1 and determine our coefficients as before.\n\nmodel &lt;- lm(data = ugtests, formula = Final ~ Yr3 + Yr2 + Yr1)\nmodel$coefficients\n\n(Intercept)         Yr3         Yr2         Yr1 \n14.14598945  0.86568123  0.43128539  0.07602621 \n\n\nReferring to our formula in Section 4.3, let’s understand what each coefficient \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) means. \\(\\beta_0\\), the intercept of the model, represents the expected value of \\(y\\) assuming that all the inputs were zero. You can imagine that your output can be expected to have a base value even without any inputs—a student who completely flunked the first three years can still redeem themselves to some extent in the Final year.\nNow looking at the other coefficients, let’s consider what happens if our first input \\(x_1\\) increased by a single unit, assuming nothing else changed. We would then expect our value of \\(y\\) to increase by \\(\\beta_1\\). Similarly for any input \\(x_k\\), a unit increase would result in an increase in \\(y\\) of \\(\\beta_k\\), assuming no other changes in the inputs.\nIn the case of our ugtests data set, we can say the following:\n\nThe intercept of the model is 14.146. This is the value that a student could be expected to score on their final exam even if they had scored zero in all previous exams.\nThe Yr3 coefficient is 0.866. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 3 score.\nThe Yr2 coefficient is 0.431. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 2 score.\nThe Yr1 coefficient is 0.076. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 1 score.\n\n\n\n4.3.2 Coefficient confidence\nIntuitively, these coefficients appear too precise for comfort. After all, we are attempting to estimate a relationship based on a limited set of data. In particular, looking at the Yr1 coefficient, it seems to be very close to zero, implying that there is a possibility that the Year 1 examination score has no impact on the final examination score. Like in any statistical estimation, the coefficients calculated for our model have a margin of error. Typically, in any such situation, we seek to know a 95% confidence interval around the values we are interpreting.\nThe summary() function is a useful way to gather critical information in your model, including important statistics on your coefficients:\n\nmodel_summary &lt;- summary(model)\nmodel_summary$coefficients\n\n               Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 14.14598945 5.48005618  2.581358  9.986880e-03\nYr3          0.86568123 0.02913754 29.710169 1.703293e-138\nYr2          0.43128539 0.03250783 13.267124  4.860109e-37\nYr1          0.07602621 0.06538163  1.162807  2.451936e-01\n\n\nThe 95% confidence interval corresponds to approximately two standard errors above or below the estimated value. For a given coefficient, if this confidence interval includes zero, you cannot reject the hypothesis that the variable has no relationship with the outcome. Another indicator of this is the Pr(&gt;|t|) column of the coefficient summary, which represents the p-value of the null hypothesis that the input variable has no relationship with the outcome. If this value is less than a certain threshold (usually 0.05), you can conclude that this variable has a statistically significant relationship with the outcome. To see the precise confidence intervals for your model coefficients, you can use the confint() function.\n\nconfint(model)\n\n                  2.5 %     97.5 %\n(Intercept)  3.39187185 24.9001071\nYr3          0.80850142  0.9228610\nYr2          0.36749170  0.4950791\nYr1         -0.05227936  0.2043318\n\n\nIn this case, we can conclude that the examinations in Years 2 and 3 have a significant relationship with the Final examination score, but we cannot conclude this for Year 1. Effectively, this means that we can drop Yr1 from our model with no substantial loss of fit. In general, simpler models are easier to manage and interpret, so let’s remove the non-significant variable now.\n\nnewmodel &lt;- lm(data = ugtests, formula = Final ~ Yr3 + Yr2)\n\nGiven that our new model only has three dimensions, we have the luxury of visualizing it. Figure 4.9 shows the data and the fitted plane of our model.\n\n\n\n#| fig.align: \"center\"\n#| echo: false\n\nlibrary(plotly)\nlibrary(magrittr)\n\nplot_ly(data = ugtests) |&gt;\n  add_trace(x = ~Yr3, y = ~Yr2, z = ~Final, mode = \"markers\", type = \"scatter3d\",\n            marker = list(size = 5, color = \"blue\", symbol = 104), name = \"Observations\") |&gt; \n  add_trace(z = newmodel$fitted.values, x = ~Yr3, y = ~Yr2, type = \"mesh3d\", \n            name = \"Fitted values\") |&gt;\n  layout(scene = list(xaxis = list(title = 'Yr3'), yaxis = list(title = 'Yr2'),\n                      camera = list(eye = list(x = -1.5, y = 1.75, z = 0)),\n                      zaxis = list(title = 'Final'), aspectmode='cube')) \n\n\n\n\n\n\n\nFigure 4.8: 3D visualization of the fitted newmodel against the ugtests data\n\n\n\n\n4.3.3 Model ‘goodness-of-fit’\nAt this point we can further explore the overall summary of our model. As you saw in the previous section, our model summary contains numerous objects of interest, including statistics on the coefficients of our model. We can see what is inside our summary by looking at the names of its contents, and we can then dive in and explore specific objects of interest.\n\n# get summary of model\nnewmodel_summary &lt;- summary(newmodel)\n\n# see summary contents\nnames(newmodel_summary)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\n\n# view r-squared\nnewmodel_summary$r.squared\n\n[1] 0.5296734\n\n\nWe can see that our model explains more than half of the variance in the Final examination score. Alternatively, we can view the entire summary to receive a formatted report on our model.\n\n# see full model summary\nnewmodel_summary\n\n\nCall:\nlm(formula = Final ~ Yr3 + Yr2, data = ugtests)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-91.12 -20.36  -0.22  18.94  98.29 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.08709    4.30701   4.199 2.92e-05 ***\nYr3          0.86496    0.02914  29.687  &lt; 2e-16 ***\nYr2          0.43236    0.03250  13.303  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.44 on 972 degrees of freedom\nMultiple R-squared:  0.5297,    Adjusted R-squared:  0.5287 \nF-statistic: 547.3 on 2 and 972 DF,  p-value: &lt; 2.2e-16\n\n\nThis provides us with some of the most important metrics from our model. In particular, the last line gives us a report on our overall model confidence or ‘goodness-of-fit’—this is a hypothesis test on the null hypothesis that our model does not fit the data any better than a random model. A high F-statistic indicates a strong likelihood that the model fits the data better than a random model. More intuitively, perhaps, we also have the p-value for the F-statistic. In this case it is extremely small, so we can reject the null hypothesis and conclude that our model has significant explanatory power over and above a random model.\nBe careful not to confuse model goodness-of-fit with \\(R^2\\). Depending on your sample, it is entirely possible for a model with a low \\(R^2\\) to have high certainty for goodness-of-fit and vice versa.\n\n\n4.3.4 Making predictions from your model\nWhile this book focuses on inferential rather than predictive analytics, we briefly touch here on the mechanics of generating predictions from models. As you might imagine, once the model has been fitted, prediction is a relatively straightforward process. We feed the Yr2 and Yr3 examination scores into our fitted model, and it applies the coefficients to calculate the predicted outcome. Let’s look at three fictitious students and create a dataframe with their scores to input into the model.\n\n(new_students &lt;- data.frame(\n  Yr2 = c(67, 23, 88), \n  Yr3 = c(144, 100, 166)\n))\n\n  Yr2 Yr3\n1  67 144\n2  23 100\n3  88 166\n\n\nNow we can feed these values into our model to get predictions of the Final examination result for our three new students.\n\n# use newmodel to predict for new_students\npredict(newmodel, new_students)\n\n       1        2        3 \n171.6093 114.5273 199.7179 \n\n\nBecause we are modeling the expected value of the Final examination result, this prediction represents the expected mean over many students with the same scores in previous years examinations. We know from our earlier work in this chapter that there is a confidence interval around the coefficients of our model, which means that there is a range of values for our expected Final score according to those confidence intervals. This can be determined by specifying that you require a confidence interval for your predictions.\n\n# get a confidence interval \npredict(newmodel, new_students, interval = \"confidence\")\n\n       fit      lwr      upr\n1 171.6093 168.2125 175.0061\n2 114.5273 109.7081 119.3464\n3 199.7179 195.7255 203.7104\n\n\nAs an example, another way of interpreting the confidence interval is to say that, if you have many students who scored precisely 67 and 144 in the Year 2 and Year 3 examinations respectively, we expect the mean Final score of these students to be somewhere between 168 and 175.\nYou may also recall from Section 4.2.1 that any prediction of an individual observation is subject to an error term \\(\\epsilon\\). Therefore, to generate a more reliable prediction range for an individual observation, you should calculate a ‘prediction interval’‍.\n\n# get a prediction interval \npredict(newmodel, new_students, interval = \"prediction\")\n\n       fit       lwr      upr\n1 171.6093 111.77795 231.4406\n2 114.5273  54.59835 174.4562\n3 199.7179 139.84982 259.5860\n\n\nAs discussed in Chapter 1, the process of developing a model to predict an outcome can be quite different from developing a model to explain an outcome. For a start, it is unlikely that you would use your entire sample to fit a predictive model, as you would want to reserve a portion of your data to test for its fit on new data. Since the focus of this book is inferential modeling, much of this topic will be out of our scope.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#managing-inputs-in-linear-regression",
    "href": "linear_regression.html#managing-inputs-in-linear-regression",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.4 Managing inputs in linear regression",
    "text": "4.4 Managing inputs in linear regression\nOur walkthrough example for this chapter, while useful for illustrating the key concepts, is a very straightforward data set to run a model on. There is no missing data, and all the data inputs have the same numeric data type (in the exercises at the end of this chapter we will present a more varied data set for analysis). Commonly, an analyst will have a list of possible input variables that they can consider in their model, and rarely will they run a model using all of these variables. In this section we will cover some common elements of decision making and design of input variables in regression models.\n\n4.4.1 Relevance of input variables\nThe first step in managing your input variables is to make a judgment about their relevance to the outcome being modeled. Analysts should not blindly run a model on a set of variables before considering their relevance. There are two common reasons for rejecting the inclusion of an input variable:\n\nThere is no reasonable possibility of a direct or indirect causal relationship between the input and the outcome. For example, if you were provided with the height of each individual taking the Final examination in our walkthrough example, it would be difficult to see how that could reasonably relate to the outcome that you are modeling.\nIf there is a possibility that the model will be used to predict based on new data in the future, there may be variables that you explicitly do not wish to be used in any prediction. For example, if our walkthrough model contained student gender data, we would not want to include that in a model that predicted future student scores because we would not want gender to be taken into consideration when determining student performance.\n\n\n\n4.4.2 Sparseness (‘missingness’) of data\nMissing data is a very common problem in modeling. If an observation has missing data in a variable that is being included in the model, that observation will be ignored, or an error will be thrown. This forces a model trained on a smaller set of data, which can compromise its powers of inference. Running summary functions on your data (such as summary() in R) will reveal variables that contain missing data if they exist.\nThere are three main options for how missing data is handled:\n\nIf the data for a given variable is relatively complete and only a small number of observations are missing, it’s usually best and simplest to remove the observations that are missing from the data set. Note that many modeling functions (though not all) will take care of this automatically.\nAs data becomes more sparse, removing observations becomes less of an option. If the sparseness is massive (for example, more than half of the data is missing), then there is no choice but to remove that variable from the model. While this may be unsatisfactory for a given variable (because it is thought to have an important explanatory role), the fact remains that data that is mostly missing is not a good measure of a construct in the first place.\nModerate sparse data could be considered for imputation. Imputation methods involve using the overall statistical properties of the entire data set or of specific other variables to ‘suggest’ what the missing value might be, ranging from simple mean and median values to more complex imputation methods. Imputation methods are more commonly used in predictive settings, and we will not cover imputation methods in depth here.\n\n\n\n4.4.3 Transforming categorical inputs to dummy variables\nMany models will have categorical inputs rather than numerical inputs. Categorical inputs usually take forms such as:\n\nBinary values—for example, Yes/No, True/False\nUnordered categories—for example Car, Train, Bicycle\nOrdered categories—for example Low, Medium, High\n\nCategorical variables do not behave like numerical variables. There is no sense of quantity in a categorical variable. We do not know how a Car relates to a Train quantitatively, we only know that they are different. Even for an ordered category, although we know that ‘Medium’ is higher than ‘Low’‍, we do not know how much higher or indeed whether the difference is the same as that between ‘High’ and ‘Medium’‍.\nIn general, all model input variables should take a numeric form. The most reliable way to do this is to convert categorical values to dummy variables. While some packages and functions have a built-in ability to convert categorical data to dummy variables, not all do, so it is important to know how to do this yourself. Consider the following data set:\n\n(vehicle_data &lt;- data.frame(\n  make = factor(c(\"Ford\", \"Toyota\", \"Audi\")), \n  manufacturing_cost = c(15000, 19000, 28000)\n))\n\n    make manufacturing_cost\n1   Ford              15000\n2 Toyota              19000\n3   Audi              28000\n\n\nThe make data is categorical, so it will be converted to several columns for each possible value of make, and binary labeling will be used to identify whether that value is present in that specific observation. Many packages and functions are available to conveniently do this, for example:\n\nlibrary(makedummies)\n(dummy_vehicle &lt;- makedummies::makedummies(vehicle_data, \n                                           basal_level = TRUE))\n\n  make_Audi make_Ford make_Toyota manufacturing_cost\n1         0         1           0              15000\n2         0         0           1              19000\n3         1         0           0              28000\n\n\nIt is worth a moment to consider how to interpret coefficients of dummy variables in a linear regression model. Note that all observations will have one of the dummy variable values (all cars must have a make). Therefore the model will assume a ‘reference value’ for the categorical variable—often this is the first value in alphabetical or numerical order. In this case, Audi would be the reference dummy variable. The model then calculates the effect on the outcome variable of a ‘switch’ from Audi to one of the other dummies3. If we were to try to use the data in our vehicle_data_dummies data set to explain the retail price of a vehicle, we would interpret coefficients like this:\n\nComparing two cars of the same make, we would expect each extra dollar spent on manufacturing to change the retail price by …\nComparing a Ford with an Audi of the same manufacturing cost, we would expect a difference in retail price of …\nComparing a Toyota with an Audi of the same manufacturing cost, we would expect a difference in retail price of …\n\nThis highlights the importance of appropriate interpretation of coefficients, and in particular the proper understanding of units. It will be common to see much larger coefficients for dummy variables in regression models because they represent a binary ‘all’ or ‘nothing’ variable in the model. The coefficient for manufacturing cost would be much smaller because a unit in this case is a dollar of manufacturing spend, on a scale of many thousands of potential dollars in spend. Care should be taken not to ‘rank’ coefficients by their value. Higher coefficients in and of themselves do not imply greater importance4.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#testing-your-model-assumptions",
    "href": "linear_regression.html#testing-your-model-assumptions",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.5 Testing your model assumptions",
    "text": "4.5 Testing your model assumptions\nAll modeling techniques have underlying assumptions about the data that they model and can generate inaccurate results when those assumptions do not hold true. Conscientious analysts will verify that these assumptions are satisfied before finalizing their modeling efforts. In this section we will outline some common checks of model assumptions when running linear regression models.\n\n4.5.1 Assumption of linearity and additivity\nLinear regression assumes that the relationship we are trying to model is linear and additive in nature. Therefore you can expect problems if you are using this approach to model a pattern that is not linear or additive.\nYou can check whether your linearity assumption was reasonable in a couple of ways. You can plot the true versus the predicted (fitted) values to see if they look correlated. You can see such a plot on our student examination model in Figure 4.10.\n\n\n\n\npredicted_values &lt;- newmodel$fitted.values\ntrue_values &lt;- ugtests$Final\n\n# plot true values against predicted values\nplot(predicted_values, true_values)\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Plot of true versus fitted/predicted student scores\n\n\n\nAlternatively, you can plot the residuals of your model against the predicted values and look for the pattern of a random distribution (that is, no major discernible pattern) such as in Figure 4.11.\n\n\n\n\nresiduals &lt;- newmodel$residuals\n\n# plot residuals against predicted values\nplot(predicted_values, residuals)\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Plot of residuals against fitted/predicted scores\n\n\n\nYou can also plot the residuals against each input variable as an extra check of independent randomness, looking for a reasonably random distribution in all cases. If you find that your residuals are following a clear pattern and are not random in nature, this is an indication that a linear model is not a good choice for your data.\n\n\n4.5.2 Assumption of constant error variance\nIt is assumed in a linear model that the errors or residuals are homoscedastic—this means that their variance is constant across the values of the input variables. If the errors of your model are heteroscedastic—that is, if they increase or decrease according to the value of the model inputs—this can lead to poor estimations and inaccurate inferences.\nWhile a simple plot of residuals against predicted values (such as in Figure 4.11) can give a quick indication on homoscedacity, to be thorough the residuals should be plotted against each input variable, and it should be verified that the range of the residuals remains broadly stable. In our student examination model, we can first plot the residuals against the values of Yr2 in Figure 4.12.\n\n\n\n\nYr2 &lt;- ugtests$Yr2\n\n# plot residuals against Yr2 values\nplot(Yr2, residuals)\n\n\n\n\n\n\n\n\n\n\nFigure 4.12: Plot of residuals against Yr2 values\n\n\n\nWe see a pretty consistent range of values for the residuals in Figure 4.12. Similarly we can plot the residuals against the values of Yr3, as in Figure 4.13.\n\n\n\n\nYr3 &lt;- ugtests$Yr3\n\n# plot residuals against Yr3 values\nplot(Yr3, residuals)\n\n\n\n\n\n\n\n\n\n\nFigure 4.13: Plot of residuals against Yr3 values\n\n\n\nFigure 4.13 also shows a consistent range of values for the residuals, which reassures us that we have homoscedacity.\n\n\n4.5.3 Assumption of normally distributed errors\nRecall from Section 4.2.1 that one of the assumptions when we run a linear regression model is that the residuals of the model are normally distributed around a mean of zero. Therefore it is useful to check your data to see how well it fits these assumptions. If your data shows that the residuals have a substantially non-normal distribution, this may tell you something about your model and how well you can rely on it for accurate inferences or predictions.\nWhen residuals violate this assumption, it means that the ingoing mathematical premise of your model does not fully describe your data. Whether or not this is a big problem depends on the existing properties of your sample and on other model diagnostics. In models built on smaller samples, where hypothesis test criteria are only narrowly met, residual non-normality could present a significant challenge to the reliability of inferences and predictions. On models where hypothesis test criteria are very comfortably met, residual non-normality is less likely to be a problem (Lumley et al. (2002)). In any case, it is good practice to examine the distribution of your residuals so that you can refine or improve your model.\nThe quickest way to determine if residuals in your sample are consistent with a normal distribution is to run a quantile-quantile plot (or Q-Q plot) on the residuals. This will plot the observed quantiles of your sample against the theoretical quantiles of a normal distribution. The closer this plot looks like a perfect correlation, the more certain you can be that this normality assumption holds. An example for our student examination model is in Figure 4.14.\n\n\n\n\n# normal distribution qqplot of residuals\nqqnorm(newmodel$residuals)\n\n\n\n\n\n\n\n\n\n\nFigure 4.14: Quantile-quantile plot of residuals\n\n\n\n\n\n4.5.4 Avoiding high collinearity and multicollinearity between input variables\nIn multiple linear regression, the various input variables used can be considered ‘dimensions’ of the problem or model. In theory, we ideally expect dimensions to be independent and uncorrelated. Practically speaking, however, it’s very challenging in large data sets to ensure that every input variable is completely uncorrelated from another. For example, even in our limited ugtests data set we saw in Figure 4.2 that Yr2 and Yr3 examination scores are correlated to some degree.\nWhile some correlation between input variables can be expected and tolerated in linear regression models, high levels of correlation can result in significant inflation of coefficients and inaccurate estimates of p-values of coefficients.\nCollinearity means that two input variables are highly correlated. The definition of ‘high correlation’ is a matter of judgment, but as a rule of thumb correlations greater than 0.5 might be considered high and greater than 0.7 might be considered extreme. Creating a simple correlation matrix or a pairplot (such as Figure 4.2) can immediately surface high or extreme collinearity.\nMulticollinearity means that there is a linear relationship between more than two of the input variables. This may not always present itself in the form of high correlations between pairs of input variables, but may be seen by identifying ‘clusters’ of moderately correlated variables, or by calculating a Variance Inflation Factor (VIF) for each input variable—where VIFs greater than 5 indicate high multicollinearity. Easy-to-use tests also exist in statistical software for identifying multicollinearity (for example the mctest package in R). Here is how we would test for multicollinearity in our student examination model.\n\nlibrary(mctest)\n\n# diagnose possible overall presence of multicollinearity\nmctest::omcdiag(newmodel)\n\n\nCall:\nmctest::omcdiag(mod = newmodel)\n\n\nOverall Multicollinearity Diagnostics\n\n                       MC Results detection\nDeterminant |X'X|:         0.9981         0\nFarrar Chi-Square:         1.8365         0\nRed Indicator:             0.0434         0\nSum of Lambda Inverse:     2.0038         0\nTheil's Method:           -0.5259         0\nCondition Number:          9.1952         0\n\n1 --&gt; COLLINEARITY is detected by the test \n0 --&gt; COLLINEARITY is not detected by the test\n\n\n\n# if necessary, diagnose specific multicollinear variables using VIF \nmctest::imcdiag(newmodel, method = \"VIF\")\n\n\nCall:\nmctest::imcdiag(mod = newmodel, method = \"VIF\")\n\n\n VIF Multicollinearity Diagnostics\n\n       VIF detection\nYr3 1.0019         0\nYr2 1.0019         0\n\nNOTE:  VIF Method Failed to detect multicollinearity\n\n\n0 --&gt; COLLINEARITY is not detected by the test\n\n===================================\n\n\nNote that collinearity and multicollinearity only affect the coefficients of the variables impacted, and do not affect other variables or the overall statistics and fit of a model. Therefore, if a model is being developed primarily to make predictions and there is little interest in using the model to explain a phenomenon, there may not be any need to address this issue at all. However, in inferential modeling the accuracy of the coefficients is very important, and so testing of multicollinearity is essential. In general, the best way to deal with collinear variables is to remove one of them from the model (usually the one that has the least significance in explaining the outcome).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#extending-multiple-linear-regression",
    "href": "linear_regression.html#extending-multiple-linear-regression",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.6 Extending multiple linear regression",
    "text": "4.6 Extending multiple linear regression\nBy introducing some extensions of linear regression to relax the linear or additive assumptions, we can try to improve the overall fit of a model. It is rare for practitioners to extend linear regression models too greatly due to the negative impact this can have on interpretation, but simple extensions such as experimenting with interaction terms or quadratics are not uncommon. If you have an appetite to explore this topic more fully, I recommend Rao et al. (2008).\n\n4.6.1 Interactions between input variables\nRecall that our model of student examination scores took each year’s score as an independent input variable, and therefore we are making the assumption that the score obtained in each year acts independently and additively in predicting the Final score. However, it is very possible that several input variables act together in relation to the outcome. One way of modeling this is to include interaction terms in your model, which are new input variables formed as products of the original input variables.\nIn our student examination data in ugtests, we could consider extending our model to not only include the individual year examinations, but also to include the impact of combined changes across multiple years. For example, we could combine the impact of Yr2 and Yr3 examinations by multiplying them together in our model.\n\ninteraction_model &lt;- lm(data = ugtests, \n                        formula = Final ~ Yr2 + Yr3 + Yr2*Yr3)\nsummary(interaction_model)\n\n\nCall:\nlm(formula = Final ~ Yr2 + Yr3 + Yr2 * Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-78.084 -18.284  -0.546  18.395  79.824 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.320e+02  1.021e+01  12.928  &lt; 2e-16 ***\nYr2         -7.947e-01  1.056e-01  -7.528 1.18e-13 ***\nYr3         -2.267e-01  9.397e-02  -2.412   0.0161 *  \nYr2:Yr3      1.171e-02  9.651e-04  12.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.38 on 971 degrees of freedom\nMultiple R-squared:  0.5916,    Adjusted R-squared:  0.5903 \nF-statistic: 468.9 on 3 and 971 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that introducing this interaction term has improved the fit of our model from 0.53 to 0.59, and that the interaction term is significant, so we conclude that in addition to a significant effect of the Yr2 and Yr3 scores, there is an additional significant effect from their interaction Yr2*Yr3. Let’s take a moment to understand how to interpret this, since we note that some of the coefficients are now negative.\nOur model now includes two input variables and their interaction, so it can be written as\n\\[\n\\begin{aligned}\n\\mathrm{Final} &= \\beta_0 + \\beta_1\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} + \\beta_3\\mathrm{Yr3}\\mathrm{Yr2} \\\\\n&= \\beta_0 + (\\beta_1 + \\beta_3\\mathrm{Yr2})\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} \\\\\n&= \\beta_0 + \\gamma\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2}\n\\end{aligned}\n\\]\nwhere \\(\\gamma = \\beta_1 + \\beta_3\\mathrm{Yr2}\\). Therefore our model has coefficients which are not constant but change with the values of the input variables. We can conclude that the effect of an extra point in the examination in Year 3 will be different depending on how the student performed in Year 2. Visualizing this, we can see in Figure 4.16 that this non-constant term introduces a curvature to our fitted surface that aligns it a little more closely with the observations in our data set.\n\n\n\n\n\n\n\n\n\n\nFigure 4.15: 3D visualization of the fitted interaction_model against the ugtests data\n\n\nBy examining the shape of this curved plane, we can observe that the model considers trajectories in the Year 2 and Year 3 examination scores. Those individuals who have improved from one year to the next will perform better in this model than those who declined. To demonstrate, let’s look at the predicted scores from our interaction_model for someone who declined and for someone who improved from Year 2 to Year 3.\n\n# data frame with a declining and an improving observation\nobs &lt;- data.frame(\n  Yr2 = c(150, 75),\n  Yr3 = c(75, 150)\n)\n\npredict(interaction_model, obs)\n\n       1        2 \n127.5010 170.1047 \n\n\nThrough including the interaction effect, the model interprets declining examination scores more negatively than improving examination scores. These kinds of additional inferential insights may be of great interest. However, consider the impact on interpretability of modeling too many combinations of interactions. As always, there is a trade-off between intepretability and accuracy5.\nWhen running models with interaction terms, you can expect to see a hierarchy in the coefficients according to the level of the interaction. For example, single terms will usually generate higher coefficients than interactions of two terms, which will generate higher coefficients than interactions of three terms, and so on. Given this, whenever an interaction of terms is considered significant in a model, then the single terms contained in that interaction should automatically be regarded as significant.\n\n\n4.6.2 Quadratic and higher-order polynomial terms\nIn many situations the real underlying relationship between the outcome and the inputs may be non-linear. For example, if the underlying relationship was thought to be quadratic on a given input variable \\(x\\), then the formula would take the form \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2\\). We can easily trial polynomial terms using our linear model technology.\nFor example, recall that we removed Yr1 data from our model because it was not significant when modeled linearly. We could test if a quadratic model on Yr1 helps improve our fit6:\n\n# add a quadratic term in Yr1\nquadratic_yr1_model &lt;- lm(data = ugtests, \n                          formula = Final ~ Yr3 + Yr2 + Yr1 + I(Yr1^2))\n\n# test R-squared\nsummary(quadratic_yr1_model)$r.squared\n\n[1] 0.5304198\n\n\nIn this case we find that modeling Yr1 as a quadratic makes no difference to the fit of the model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#ordinary-least-squares-ols-linear-regression-using-python",
    "href": "linear_regression.html#ordinary-least-squares-ols-linear-regression-using-python",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.7 Ordinary Least Squares (OLS) linear regression using Python",
    "text": "4.7 Ordinary Least Squares (OLS) linear regression using Python\nThe OLS linear regression model reviewed in this chapter can be generated in Python using the statsmodels package, which can report a reasonably thorough set of model statistics. By using the statsmodels formula API, model formulas similar to those used in R can be used.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# get data\nurl = \"https://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\n\n# define model\nmodel = smf.ols(formula = \"Final ~ Yr3 + Yr2 + Yr1\", data = ugtests)\n\n# fit model\nugtests_model = model.fit()\n\n# see results summary\nprint(ugtests_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Final   R-squared:                       0.530\nModel:                            OLS   Adj. R-squared:                  0.529\nMethod:                 Least Squares   F-statistic:                     365.5\nDate:                Mon, 22 Dec 2025   Prob (F-statistic):          8.22e-159\nTime:                        13:30:54   Log-Likelihood:                -4711.6\nNo. Observations:                 975   AIC:                             9431.\nDf Residuals:                     971   BIC:                             9451.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     14.1460      5.480      2.581      0.010       3.392      24.900\nYr3            0.8657      0.029     29.710      0.000       0.809       0.923\nYr2            0.4313      0.033     13.267      0.000       0.367       0.495\nYr1            0.0760      0.065      1.163      0.245      -0.052       0.204\n==============================================================================\nOmnibus:                        0.762   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.683   Jarque-Bera (JB):                0.795\nSkew:                           0.067   Prob(JB):                        0.672\nKurtosis:                       2.961   Cond. No.                         858.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#sec-lin-reg-ols-exercises",
    "href": "linear_regression.html#sec-lin-reg-ols-exercises",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "4.8 Learning exercises",
    "text": "4.8 Learning exercises\n\n4.8.1 Discussion questions\n\nWhat is the approximate meaning of the term ‘regression’‍? Why is the term particularly suited to the methodology described in this chapter?\nWhat basic condition must the outcome variable satisfy for linear regression to be a potential modeling approach? Describe some ideas for problems that might be modeled using linear regression.\nWhat is the difference between simple linear regression and multiple linear regression?\nWhat is a residual, and how does it relate to the term ‘Ordinary Least Squares’‍?\nHow are the coefficients of a linear regression model interpreted? Explain why higher coefficients do not necessarily imply greater importance.\nHow is the \\(R^2\\) of a linear regression model interpreted? What are the minimum and maximum possible values for \\(R^2\\), and what does each mean?\nWhat are the key considerations when preparing input data for a linear regression model?\nDescribe your understanding of the term ‘dummy variable’‍. Why are dummy variable coefficients often larger than other coefficients in linear regression models?\nDescribe the term ‘collinearity’ and why it is an important consideration in regression models.\nDescribe some ways that linear regression models can be extended into non-linear models.\n\n\n\n4.8.2 Data exercises\nLoad the sociological_data data set via the peopleanalyticsdata package or download it from the internet7. This data represents a sample of information obtained from individuals who participated in a global research study and contains the following fields:\n\nannual_income_ppp: The annual income of the individual in PPP adjusted US dollars\naverage_wk_hrs: The average number of hours per week worked by the individual\neducation_months: The total number of months spent by the individual in formal primary, secondary and tertiary education\nregion: The region of the world where the individual lives\njob_type: Whether the individual works in a skilled or unskilled profession\ngender: The gender of the individual\nfamily_size: The size of the individual’s family of dependents\nwork_distance: The distance between the individual’s residence and workplace in kilometers\nlanguages: The number of languages spoken fluently by the individual\n\nConduct some exploratory data analysis on this data set. Including:\n\nIdentify the extent to which missing data is an issue.\nDetermine if the data types are appropriate for analysis.\nUsing a correlation matrix, pairplot or alternative method, identify whether collinearity is present in the data.\nIdentify and discuss anything else interesting that you see in the data.\n\nPrepare to build a linear regression model to explain the variation in annual_income_ppp using the other data in the data set.\n\nAre there any fields which you believe should not be included in the model? If so, why?\nWould you consider imputing missing data for some or all fields where it is an issue? If so, what might be some simple ways to impute the missing data?\nWhich variables are categorical? Convert these variables to dummy variables using a convenient function or using your own approach.\n\nRun and interpret the model. For convenience, and to avoid long formula strings, you can use the formula notation annual_income_ppp ~ . which means ‘regress annual_income against everything else’‍. You can also remove fields this way, for example annual_income_ppp ~ . - family_size.\n\nDetermine what variables are significant predictors of annual income and what is the effect of each on the outcome.\nDetermine the overall fit of the model.\nDo some simple analysis on the residuals of the model to determine if the model is safe to interpret.\nExperiment with improving the model fit through possible interaction terms or non-linear extensions.\nComment on your results. Did anything in the results surprise you? If so, what might be possible explanations for this.\nExplain why you would or would not be comfortable using a model like this in a predictive setting—for example to help employers determine the right pay for employees.\n\n\n\n\n\nLumley, T., P. Diehr, S. Emerson, and L. Chen. 2002. “The Importance of the Normality Assumption in Large Public Health Data Sets.” Annu Rev Public Health.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2012. Introduction to Linear Regression Analysis.\n\n\nRao, C. Radhakrishna, Shalabh, Helge Toutenburg, and Christian Heumann. 2008. The Multiple Linear Regression Model and Its Extensions.\n\n\nSenn, Stephen. 2011. “Francis Galton and Regression to the Mean.” Significance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#footnotes",
    "href": "linear_regression.html#footnotes",
    "title": "4  Linear Regression for Continuous Outcomes",
    "section": "",
    "text": "This chart includes a random ‘jitter’ to better illustrate observations that are identical and was first used to illustrate Galton’s data in Senn (2011).↩︎\nAs a side note, in a simple regression model like this, where there is only one input variable, we have the simple identity \\(R^2 = r^2\\), where \\(r\\) is the correlation between the input and outcome (for our small set of 10 observations here, the correlation is 0.864).↩︎\nFor more on how to control which categorical value is used as a reference, see Section 6.3.1.↩︎\nRescaling numerical input variables onto common scales can help with understanding the ranked importance of these variables. In some techniques, for example structural modeling which we will review in Section 9.2, scaled regression coefficients help determine the ranked importance of constructs to the outcome.↩︎\nIn a predictive context, there is also the issue of ‘overfitting’ the model, where the model is too ‘tightly’ aligned to the past data that was used in fitting it that it may be very inaccurate for new data. For example, in our interaction model, someone who scores very low in both Year 2 and Year 3 will be awarded an unreasonably high score (see the intercept coefficient in the interactive model summary). This reinforces the need to test and validate model fits in a predictive context.↩︎\nNote the use of I() in the formula notation here. This is because the symbol ^ has a different meaning inside a formula, and we use I() to isolate what is inside the parentheses to ensure that it is interpreted literally as ‘the square of Yr1’‍.↩︎\nhttps://peopleanalytics-regression-book.org/data/sociological_data.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Regression for Continuous Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html",
    "href": "binomial_logistic_regression.html",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "5.1 When to use it\nIn the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Binary (two class) problems are very common. Hiring, promotion and attrition are often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Multi-class outcomes like performance ratings on an ordinal scale, or survey responses on a Likert scale are often converted to binary outcomes by dividing the ratings into two groups, for example ‘High’ and ‘Not High’.\nIn any situation where our outcome is binary, we are effectively working with likelihoods. These are not generally linear in nature, and so we no longer have the comfort of our inputs being directly linearly related to our outcome. Therefore direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. Instead, linear relationships can be inferred on transformations of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of generalized linear models or GLMs. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved but worth the time investment in order to build a competent understanding of how to interpret these types of models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#when-to-use-it",
    "href": "binomial_logistic_regression.html#when-to-use-it",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "5.1.1 Origins and intuition of binomial logistic regression\nImagine that we have a set of observations of a random variable that is binary or dichotomous in nature. For convenience, let’s call the two values that any observation can take ‘success’ and ‘failure’. Let’s say that the probability of an observation being a success is \\(p\\), and therefore the probability of an observation being a failure is \\(1 - p\\). Such a random variable is known as a Bernoulli random variable.\nLet’s also imagine we take a sample of \\(n\\) observations of a Bernoulli random variable. Then the number of successes in the sample, which we will call \\(Y\\), will follow a binomial distribution with parameters \\(n\\) and \\(p\\). The probability of observing exactly \\(k\\) successes in our sample is given by the formula:\n\\[\nP(Y = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nwhere \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient, which gives the number of ways of choosing a subset of size \\(k\\) from a total of \\(n\\) observations. Here is the distribution of \\(Y\\) for \\(n = 100\\) and \\(p = 0.3\\), shown in Figure 5.1:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Binomial distribution showing the probability of \\(k\\) successes in 100 observations with a success probability of 0.3\n\n\n\nNow, although the variable \\(Y\\) here is discrete (can only take non-negative integer values), we can see that the shape of \\(Y\\) resembles a normal distribution. In fact, as long as \\(n\\) is not very small and \\(p\\) is not very biased1, the binomial distribution can be well approximated by a normal distribution with mean \\(np\\) and variance \\(np(1 - p)\\) (standard deviation \\(\\sqrt{np(1-p)}\\)) as shown in Figure 5.2 for \\(n = 100\\) and \\(p = 0.3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Binomial distribution with \\(n = 100\\), \\(p = 0.3\\) (blue bars) and normal approximation with \\(\\mu = 30\\), \\(\\sigma = \\sqrt{21}\\) (red line)\n\n\n\nNow if we consider tha cumulative probability of \\(Y\\), that is, the probability of observing up to and including \\(k\\) successes, we can see that this cumulative probability takes on an S-shape, which is also well approximated by the cumulative normal distribution, as shown in Figure 5.3 for \\(n = 100\\) and \\(p = 0.3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: Cumulative binomial distribution with \\(n = 100\\), \\(p = 0.3\\) (blue bars) and normal approximation with \\(\\mu = 30\\), \\(\\sigma = \\sqrt{21}\\) (red line)\n\n\n\nWe will be interested in modeling this cumulative probability of success, in order to understand how various input variables might influence it. Because we know that it is well approximated by the normal distribution, we can utilize a function that has very similar characteristics to the normal distribution, but is easier to work with mathematically.\nThe logistic function was first introduced by the Belgian mathematician Pierre François Verhulst in the early 1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population. The formula for Verhulst’s function was:\n\\[\ny = \\frac{L}{1 + e^{-k(x - x_0)}}\n\\] where \\(e\\) is the exponential constant, \\(x_0\\) is the value of \\(x\\) at the midpoint, \\(L\\) is the maximum value of \\(y\\) (known as the ‘carrying capacity’) and \\(k\\) is the maximum gradient of the curve.\nThe logistic function, as shown in Figure 5.4, was felt to accurately capture the theorized stages of population growth, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.4: Verhulst’s logistic function modeled both the exponential nature and the natural limit of population growth\n\n\n\nIn the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was observed that the logistic function has a similar S-shape (or sigmoid) to a cumulative normal distribution of probability, as depicted in Figure 5.52, where the \\(x\\) scale for the normal distribution represents standard deviations around the mean. As we will learn, the logistic function gives rise to a mathematical model where the coefficients are easily interpreted in terms of likelihood of the outcome. Unsurprisingly, therefore, the logistic model soon became a common approach to modeling probabilistic phenomena.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret\n\n\n\n\n\n5.1.2 Use cases for binomial logistic regression\nBinomial logistic regression can be used when the outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is an underlying assumption that the cumulative probability of the outcome takes a shape similar to a cumulative normal distribution.\nHere are some example questions that could be approached using binomial logistic regression:\n\nGiven a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what influence do these factors have on the likelihood of the individual receiving a high performance rating?\nGiven a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election?\nGiven a set of statistics about the in-game activity of soccer players, what relationship does each statistic have with the likelihood of a player scoring a goal?\n\n\n\n5.1.3 Walkthrough example\nYou are an analyst for a large company consisting of regional sales teams across the country. Twice every year, this company promotes some of its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment.\nYou are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a data set containing data for the last three years of salespeople considered for promotion. The salespeople data set contains the following fields:\n\npromoted: A binary value indicating 1 if the individual was promoted and 0 if not\nsales: the sales (in thousands of dollars) attributed to the individual in the period of the promotion\ncustomer_rate: the average satisfaction rating from a survey of the individual’s customers during the promotion period\nperformance: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest)\n\nLet’s take a quick look at the data.\n\n# if needed, download salespeople data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople &lt;- read.csv(url)\n\n\n# look at the first few rows of data\nhead(salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\nThe data looks as expected. Let’s get a summary of the data.\n\nsummary(salespeople)\n\n    promoted          sales       customer_rate    performance \n Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  \n 1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  \n Median :0.0000   Median :475.0   Median :3.620   Median :3.0  \n Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  \n 3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  \n Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  \n                  NA's   :1       NA's   :1       NA's   :1    \n\n\nFirst we see a small number of missing values, and we should remove those observations. We see that about a third of individuals were promoted, that sales ranged from $151k to $945k, that as expected the average satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and promoted is numeric when it should be categorical. Let’s convert these, and then let’s do a pairplot to get a quick view on some possible underlying relationships, as in Figure 5.6.\n\n\n\n\nlibrary(GGally)\n\n# remove NAs\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# convert performance to ordered factor and promoted to categorical\nsalespeople$performance &lt;- ordered(salespeople$performance, \n                                   levels = 1:4)\nsalespeople$promoted &lt;- as.factor(salespeople$promoted)\n\n# generate pairplot\nGGally::ggpairs(salespeople)\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Pairplot for the salespeople data set\n\n\n\nWe can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t think much of you, sales wouldn’t likely be very high).\nSo we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#sec-mod-prob",
    "href": "binomial_logistic_regression.html#sec-mod-prob",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "5.2 Modeling probabilistic outcomes using a logistic function",
    "text": "5.2 Modeling probabilistic outcomes using a logistic function\nImagine that you have an outcome observation \\(y_i\\) which takes a value of either 1 (true) or 0 (false). The probability of \\(y_i\\) being true, or \\(P(y_i = 1)\\), obviously takes a value between 0 and 1. Now imagine that some input observation \\(x_i\\) has a positive effect on \\(P(y_i = 1)\\). Then you would naturally expect \\(P(y_i = 1)\\) to increase as \\(x_i\\) increases.\nIn our salespeople data set, let’s plot our promotion outcome against the sales input. This can be seen in Figure 5.7.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Plot of promotion against sales in the salespeople data set\n\n\n\nIt’s clear that promotion is more likely with higher sales levels. As we move along the \\(x\\) axis from left to right and gradually include more and more individuals with higher sales, we know that the probability of promotion is gradually increasing overall.\nReferring back to Section 5.1.1, each \\(y_i\\) observation is a Bernoulli random variable with a specific success probability \\(P(y_i = 1)\\). Therefore, over many observations of that variable, the total number of successes (promotions) will follow a binomial distribution. As we saw in Figure 5.3, the cumulative probability of success in a binomial distribution takes an S-shape that can be well approximated by a logistic function.\nTherefore, we could try to model this cumulative probability using our logistic function, which we learned about in Section 5.1.1, such that the probability of our observation being a promotion \\(P(y_i = 1)\\) has some relationship with our input observation of sales \\(x_i\\). For example, let’s plot the logistic function\n\\[\nP(y_i = 1) = \\frac{1}{1 + e^{-k(x_i - x_{0i})}}\n\\]\non this data, where we set \\(x_{0i}\\) to the mean of sales and \\(k\\) to be some maximum gradient value. In Figure 5.8 we can see these logistic functions for different values of \\(k\\). All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best-fitting logistic function?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: Overlaying logistic functions with various gradients onto previous plot\n\n\n\n\n5.2.1 Deriving the concept of log odds\nLet’s look more carefully at the index of the exponential constant \\(e\\) in the denominator of our logistic function. Note that, because \\(x_{0i}\\) is a constant midpoint, we have:\n\\[\n-k(x_i - x_{0i}) = -(-kx_{0i} + kx_i) = -(\\beta_{0} + \\beta_1x_i)\n\\] where \\(\\beta_0 = -kx_{0i}\\) and \\(\\beta_{1} = k\\). Therefore,\n\\[\nP(y_i = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_i)}}\n\\]\nThis equation makes intuitive sense. As the value of \\(x_i\\) increases, the value \\(e^{-(\\beta_0 + \\beta_1x_i)}\\) gets smaller and smaller towards zero, and thus \\(P(y_i = 1)\\) approaches its theoretical maximum value of 1. As the value of \\(x_i\\) decreases towards zero, we see that the value of \\(P(y_i = 1)\\) approaches a minimum value of \\(\\frac{1}{1 + e^{-\\beta_0}}\\). Referring back to our salespeople example, we can thus see that \\(\\beta_0\\) helps determine the baseline probability of promotion assuming no sales at all. If \\(\\beta_0\\) has an extremely negative value, this baseline probability will approach its theoretical minimum of zero.\nLet’s formalize the role of \\(\\beta_0\\) and \\(\\beta_1\\) in the likelihood of a positive outcome. We know that for any binary outcome observation \\(y_i\\), \\(P(y_i = 0)\\) is equal to \\(1 - P(y_i = 1)\\), so\n\\[\n\\begin{aligned}\nP(y_i = 0) &= 1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_i)}} \\\\\n&= \\frac{1 + e^{-(\\beta_0 + \\beta_1x_i)} - 1}{1 + e^{-(\\beta_0 + \\beta_1x_i)}} \\\\\n&= \\frac{e^{-(\\beta_0 + \\beta_1x_i)}}{1 + e^{-(\\beta_0 + \\beta_1x_i)}}\n\\end{aligned}\n\\]\nPutting these together, we find that\n\\[\n\\begin{aligned}\n\\frac{P(y_i = 1)}{P(y_i = 0)} &= \\frac{\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_i)}}}{\\frac{e^{-(\\beta_0 + \\beta_1x_i)}}{1 + e^{-(\\beta_0 + \\beta_1x_i)}}} \\\\\n&= \\frac{1}{e^{-(\\beta_0 + \\beta_1x_i)}} \\\\\n&= e^{\\beta_0 + \\beta_1x_i}\n\\end{aligned}\n\\]\nor alternatively, if we apply the natural logarithm to both sides\n\\[\n\\ln\\left(\\frac{P(y_i = 1)}{P(y_i = 0)}\\right) = \\beta_0 + \\beta_1x_i\n\\]\nThe right-hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left-hand side?\n\\(P(y_i = 1)\\) is the probability that our outcome observation is true (the individual was promoted), while \\(P(y_i = 0)\\) is the probability that our outcome observation is false (the individual was not promoted). You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the odds of success. For example, if a given horse has odds of 1:4 in a race, this means that there is a 20% probability they will win and an 80% probability they will not3.\nTherefore we can conclude that the natural logarithm of the odds of \\(y_i\\)—usually termed the log odds of \\(y_i\\)—is linear in \\(x_i\\), and therefore we can model the log odds of \\(y_i\\) using similar linear regression methods to those studied in Chapter 44.\n\n\n5.2.2 Modeling the log odds and interpreting the coefficients\nLet’s take our simple case of regressing the promoted outcome against sales. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter.\n\n# run a binomial model \nsales_model &lt;- glm(formula = promoted ~ sales, \n                   data = salespeople, family = \"binomial\")\n\n# view the coefficients\nsales_model$coefficients\n\n (Intercept)        sales \n-21.77642020   0.03675848 \n\n\nWe can interpret the coefficients as follows:\n\nThe (Intercept) coefficient is the value of the log odds with zero input value of \\(x\\)—it is the log odds of promotion if you made no sales.\nThe sales coefficient represents the change in the log odds of promotion associated with each unit increase in sales.\n\nWe can convert these coefficients from log odds to odds by applying the exponent function, to return to the identity we had previously\n\\[\n\\frac{P(y_i = 1)}{P(y_i = 0)} = e^{\\beta_0 + \\beta_1x_i} = e^{\\beta_0}(e^{\\beta_1})^{x_i}\n\\]\nFrom this, we can interpret that \\(e^{\\beta_0}\\) represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by \\(e^{\\beta_1}\\). Given this multiplicative effect that \\(e^{\\beta_1}\\) has on the odds, it is known as an odds ratio.\n\n# convert log odds to base odds and odds ratio\nexp(sales_model$coefficients)\n\n (Intercept)        sales \n3.488357e-10 1.037442e+00 \n\n\nSo we can see that the base odds of promotion with zero sales is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (that is, nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04—in other words, it increases the odds of promotion by 4%.\n\n\n5.2.3 Odds versus probability\nIt is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model.\nIf a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a probability of 1, the odds become exponentially large, as illustrated in Figure 5.9):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.9: Odds plotted against probability\n\n\n\nThe consequence of this is that a given increase in odds can have a different effect on probability depending on what the original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116, which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. Figure 5.10 shows the impact of a 4% increase in odds according to the original probability of the event.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.10: Effect of 4% increase in odds plotted against original probability\n\n\n\nWe can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for converting odds to probability and vice versa. If \\(O\\) represents odds and \\(P\\) represents probability then we have:\n\\[\n\\begin{aligned}\nO &= \\frac{P}{1 - P} \\\\\nP &= \\frac{O}{1 + O}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#running-a-multiple-binomial-logistic-regression-model",
    "href": "binomial_logistic_regression.html#running-a-multiple-binomial-logistic-regression-model",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "5.3 Running a multiple binomial logistic regression model",
    "text": "5.3 Running a multiple binomial logistic regression model\nThe derivations in the previous section extend to multivariable data. Let \\(y_i\\) be a dichotomous outcome observation, and let \\(x_{i1}, x_{i2}, \\cdots, x_{ip}\\) be our corresponding input observations. Then\n\\[\n\\ln\\left(\\frac{P(y_i = 1)}{P(y_i = 0)}\\right) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_px_{ip}\n\\] for coefficients \\(\\beta_0, \\beta_1,\\dots, \\beta_p\\). As before:\n\n\\(\\beta_0\\) represents the log odds of our outcome when all inputs are zero\nFor \\(r = 1, 2, \\cdots, p\\), each \\(\\beta_r\\) represents the change in the log odds of our outcome associated with a unit change in the input variable \\(x_r\\), assuming no change in other inputs.\n\nApplying an exponent as before, we have\n\\[\n\\begin{aligned}\n\\frac{P(y_i = 1)}{P(y_i = 0)} &= e^{\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_px_{ip}} \\\\\n&= e^{\\beta_0}(e^{\\beta_1})^{x_{i1}}(e^{\\beta_2})^{x_{i2}}\\cdots(e^{\\beta_p})^{x_{ip}}\n\\end{aligned}\n\\]\nTherefore we can conclude that:\n\n\\(e^{\\beta_0}\\) represents the odds of the outcome when all inputs are zero.\nFor \\(r = 1, 2, \\cdots, p\\), each \\(e^{\\beta_r}\\) represents the odds ratio associated with a unit increase in the input variable \\(x_r\\) assuming no change in the other inputs (that is, a unit increase in \\(x_r\\) multiplies the odds of our outcome by \\(e^{\\beta_r}\\)).\n\nLet’s put this into practice.\n\n5.3.1 Running and interpreting a multiple binomial logistic regression model\nLet’s use a binomial logistic regression model to understand how each of the three inputs in our salespeople data set influence the likelihood of promotion.\nFirst, as we learned previously, it is good practice to convert the categorical performance variable to a dummy variable5.\n\nlibrary(makedummies)\n\n# convert performance to dummy\nsalespeople_dummies &lt;- makedummies::makedummies(salespeople)\n\n# check it worked\nhead(salespeople_dummies)\n\n  promoted sales customer_rate performance_2 performance_3 performance_4\n1        0   594          3.94             1             0             0\n2        0   446          4.06             0             1             0\n3        1   674          3.83             0             0             1\n4        0   525          3.62             1             0             0\n5        1   657          4.40             0             1             0\n6        1   918          4.54             1             0             0\n\n\nNow we can run our model (using the formula promoted ~ . to mean regressing promoted against everything else) and view our coefficients.\n\n# run binomial glm\nfull_model &lt;- glm(formula = \"promoted ~ .\",\n                  family = \"binomial\",\n                  data = salespeople_dummies)\n\n# get coefficient summary \n(coefs &lt;- summary(full_model)$coefficients)\n\n                  Estimate  Std. Error    z value     Pr(&gt;|z|)\n(Intercept)   -19.85893195 3.444078811 -5.7661085 8.112287e-09\nsales           0.04012425 0.006576429  6.1012212 1.052611e-09\ncustomer_rate  -1.11213130 0.482681585 -2.3040682 2.121881e-02\nperformance_2   0.26299953 1.021980179  0.2573431 7.969139e-01\nperformance_3   0.68495453 0.982166998  0.6973911 4.855581e-01\nperformance_4   0.73449340 1.071963758  0.6851849 4.932272e-01\n\n\nNote how only three of the performance dummies have displayed. This is because everyone is in one of the four performance categories, so the model is using performance_1 as the reference case. We can interpret each performance coefficient as the effect of a move to that performance category from performance_1.\nWe can already see from the last column of our coefficient summary—the coefficient p-values—that only sales and customer_rate meet the significance threshold of less than 0.05. Interestingly, it appears from the Estimate column that customer_rate has a negative effect on the log odds of promotion. For convenience, we can add an extra column to our coefficient summary to create the exponents of our estimated coefficients so that we can see the odds ratios. We can also remove columns that are less useful to us if we wish.\n\n# create coefficient table with estimates, p-values and odds ratios\n(full_coefs &lt;- cbind(coefs[ ,c(\"Estimate\", \"Pr(&gt;|z|)\")], \n                     odds_ratio = exp(full_model$coefficients))) \n\n                  Estimate     Pr(&gt;|z|)   odds_ratio\n(Intercept)   -19.85893195 8.112287e-09 2.373425e-09\nsales           0.04012425 1.052611e-09 1.040940e+00\ncustomer_rate  -1.11213130 2.121881e-02 3.288573e-01\nperformance_2   0.26299953 7.969139e-01 1.300826e+00\nperformance_3   0.68495453 4.855581e-01 1.983682e+00\nperformance_4   0.73449340 4.932272e-01 2.084426e+00\n\n\nNow we can interpret our model as follows:\n\nAll else being equal, sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%\nAll else being equal, customer ratings have a significant negative effect on the likelihood of promotion, with one full rating higher associated with 67% lower odds of promotion\nAll else being equal, performance ratings have no significant effect on the likelihood of promotion\n\nThe second conclusion may appear counter-intuitive, but remember from our pairplot in Section 5.1.3 that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act assuming all other variables are the same. Therefore, if two individuals have the same sales and performance ratings, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion.\nMany analysts will feel uncomfortable with stating these conclusions with too much precision, and therefore exponent confidence intervals can be calculated to provide a range for the odds ratios.\n\nexp(confint(full_model))\n\n                     2.5 %       97.5 %\n(Intercept)   7.879943e-13 7.385387e-07\nsales         1.029762e+00 1.057214e+00\ncustomer_rate 1.141645e-01 7.793018e-01\nperformance_2 1.800447e-01 1.061602e+01\nperformance_3 3.060299e-01 1.547188e+01\nperformance_4 2.614852e-01 1.870827e+01\n\n\nTherefore we can say that—all else being equal—every additional unit of sales increases the odds of promotion by between 3.0% and 5.7%, and every additional point in customer rating decreases the odds of promotion by between 22% and 89%.\nSimilar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of up to 89% in odds seems a lot more important than an increase of up to 5.7% in odds. However, the increase of up to 5.7% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of up to 89% is on a full customer rating point on a scale of only 4 full points.\n\n\n5.3.2 Understanding the fit and goodness-of-fit of a binomial logistic regression model\nUnderstanding the fit of a binomial logistic regression model is not straightforward and is sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome.\n\n# simplify model\nsimpler_model &lt;- glm(formula = promoted ~ sales + customer_rate,\n                     family = \"binomial\",\n                     data = salespeople)\n\nAs in the previous chapter, again we have the luxury of a three-dimensional model, so we can visualize it in Figure 5.12, revealing a 3D sigmoid curve which ‘twists’ to reflect the relative influence of sales and customer_rate on the outcome.\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: 3D visualization of the fitted simpler_model against the salespeople data\n\n\nNow let’s look at the summary of our simpler_model.\n\nsummary(simpler_model)\n\n\nCall:\nglm(formula = promoted ~ sales + customer_rate, family = \"binomial\", \n    data = salespeople)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -19.517689   3.346762  -5.832 5.48e-09 ***\nsales           0.040389   0.006525   6.190 6.03e-10 ***\ncustomer_rate  -1.122064   0.466958  -2.403   0.0163 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 440.303  on 349  degrees of freedom\nResidual deviance:  65.131  on 347  degrees of freedom\nAIC: 71.131\n\nNumber of Fisher Scoring iterations: 8\n\n\nNote that, unlike what we saw for linear regression in Section 4.3.3, our summary does not provide a statistic on overall model fit or goodness-of-fit. The main reason for this is that there is no clear unified point of view in the statistics community on a single appropriate measure for model fit in the case of logistic regression. Nevertheless, a number of options are available to analysts for estimating fit and goodness-of-fit for these models.\nPseudo-\\(R^2\\) measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the \\(R^2\\) in linear regression. There are numerous variants of pseudo-\\(R^2\\) with some of the most common listed here:\n\nMcFadden’s \\(R^2\\) works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome.\nCox and Snell’s \\(R^2\\) works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating \\(R^2\\) in linear regression. However, this usually means that the maximum value is less than 1 and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an \\(R^2\\).\nNagelkerke’s \\(R^2\\) resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s \\(R^2\\) by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation.\nTjur’s \\(R^2\\) is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations.\n\nStandard modeling functions generally do not offer the calculation of pseudo-\\(R^2\\) as standard, but numerous methods are available for their calculation. For example:\n\nlibrary(DescTools)\nDescTools::PseudoR2(\n  simpler_model, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"Tjur\")\n)\n\n  McFadden   CoxSnell Nagelkerke       Tjur \n 0.8520759  0.6576490  0.9187858  0.8784834 \n\n\nWe see that the Cox and Snell variant is notably lower than the other estimates, which is consistent with the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit.\nGoodness-of-fit tests for logistic regression models compare the predictions to the observed outcome and test the null hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually 10) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there is a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness-of-fit tests, and not rely entirely on any one specific approach.\nIn R, the generalhoslem package can perform the popular Hosmer-Lemeshow test of goodness of fit for logistic regression models, and is recommended for exploration. Here is an example using the logitgof() function for assessing goodness-of-fit, which uses 10 groups as default.\n\nlibrary(generalhoslem)\n\n# run Hosmer-Lemeshow GOF test on observed versus fitted values\nsimpler_model_diagnostics &lt;- generalhoslem::logitgof(\n  salespeople$promoted, \n  fitted(simpler_model) \n)\n\n# view results\nsimpler_model_diagnostics\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  salespeople$promoted, fitted(simpler_model)\nX-squared = 3.4458, df = 8, p-value = 0.9034\n\n\nThe non-significant result of the Hosmer-Lemeshow test suggests a good fit for our model.\nVarious measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness-of-fit tests for logistic regression models is Hosmer, Lemeshow, and Sturdivant (2013).\n\n\n5.3.3 Model parsimony\nWe saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of Occam’s Razor states that—all else being equal—the simplest explanation is the best. In this sense, a model that contains information that does not contribute to its primary inference objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return.\nParsimony describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or very close to the same) fit with a smaller number of inputs. The Akaike Information Criterion or AIC is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC.\n\n# sales only model\nAIC(sales_model)\n\n[1] 76.49508\n\n# sales and customer rating model\nAIC(simpler_model)\n\n[1] 71.13145\n\n# model with all inputs\nAIC(full_model)\n\n[1] 76.37433\n\n\nWe can see that the model which is limited to our two significant inputs—sales and customer rating—is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence—it is possible that the lowest AIC might still be a very poor fit.\nModel parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpret clearly, and the greater the risk of measurement problems, such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex, and in extreme cases there are more inputs than there are observations. The primary way to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#other-considerations-in-binomial-logistic-regression",
    "href": "binomial_logistic_regression.html#other-considerations-in-binomial-logistic-regression",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "5.4 Other considerations in binomial logistic regression",
    "text": "5.4 Other considerations in binomial logistic regression\nTo predict from new data, just use the predict() function as in the previous chapter. This function recognizes the type of model being used—in this case a generalized linear model—and adjusts its prediction approach accordingly. In particular, if you want to return the probability of the new observations being promoted, you need to use type = \"response\" as an argument.\n\n# define new observations\n(new_data &lt;- data.frame(sales = c(420, 510, 710), \n                        customer_rate = c(3.4, 2.3, 4.2)))\n\n  sales customer_rate\n1   420           3.4\n2   510           2.3\n3   710           4.2\n\n# predict probability of promotion\npredict(simpler_model, new_data, type = \"response\")\n\n         1          2          3 \n0.00171007 0.18238565 0.98840506 \n\n\nMany of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental mathematical transformations which take place in a logistic regression model when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on \\(y\\), in logistic regression they have a direct additive impact on the log odds of \\(y\\), or alternatively their exponents have a direct multiplicative impact on the odds of \\(y\\). Therefore coefficient overestimation such as that which can occur when collinearity is not managed can result in inferences that could substantially overstate the importance or effect of input variables.\nBecause of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined, but they can be useful in identifying outliers or particularly influential observations and in assessing goodness-of-fit. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the Pearson residual is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large-enough samples. We can see in Figure 5.13 that this is the case for our simpler_model, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is Menard (2010).\n\n\n\n\nd &lt;- density(residuals(simpler_model, \"pearson\"))\nplot(d, main= \"\")\n\n\n\n\n\n\n\n\n\n\nFigure 5.13: Distribution of Pearson residuals in simpler_model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#binomial-logistic-regression-using-python",
    "href": "binomial_logistic_regression.html#binomial-logistic-regression-using-python",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "5.5 Binomial logistic regression using Python",
    "text": "5.5 Binomial logistic regression using Python\nIn Python, binomial logistic regression models can be generated in a similar way to OLS linear regression models using the statsmodels formula API, calling the binomial family from the general statsmodels API.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# obtain salespeople data\nurl = \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# define model\nmodel = smf.glm(formula = \"promoted ~ sales + customer_rate\", \n                data = salespeople, \n                family = sm.families.Binomial())\n\n\n# fit model\npromotion_model = model.fit()\n\n\n# see results summary\nprint(promotion_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               promoted   No. Observations:                  350\nModel:                            GLM   Df Residuals:                      347\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -32.566\nDate:                Mon, 22 Dec 2025   Deviance:                       65.131\nTime:                        13:31:18   Pearson chi2:                     198.\nNo. Iterations:                     9   Pseudo R-squ. (CS):             0.6576\nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept       -19.5177      3.347     -5.831      0.000     -26.078     -12.958\nsales             0.0404      0.007      6.189      0.000       0.028       0.053\ncustomer_rate    -1.1221      0.467     -2.403      0.016      -2.037      -0.207\n=================================================================================",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#learning-exercises",
    "href": "binomial_logistic_regression.html#learning-exercises",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "5.6 Learning exercises",
    "text": "5.6 Learning exercises\n\n5.6.1 Discussion questions\n\nDraw the shape of a logistic function. Describe the three population growth phases it was originally intended to model.\nExplain why the logistic function is useful to statisticians in modeling.\nIn the formula for the logistic function in Section 5.1.1, what might be a common value for \\(L\\) in probabilistic applications? Why?\nWhat types of problems are suitable for logistic regression modeling?\nCan you think of some modeling scenarios in your work or studies that could use a logistic regression approach?\nExplain the concept of odds. How do odds differ from probability? How do odds change as probability increases?\nComplete the following:\n\n\nIf an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.\nIf an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.\n\n\nDescribe how the coefficients of a logistic regression model affect the fitted outcome. If \\(\\beta\\) is a coefficient estimate, how is the odds ratio associated with \\(\\beta\\) calculated and what does it mean?\nWhat are some of the options for determining the fit of a binomial logistic regression model?\nDescribe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model?\n\n\n\n5.6.2 Data exercises\nA nature preservation charity has asked you to analyze some data to help them understand the features of those members of the public who donated in a given month. Load the charity_donation data set via the peopleanalyticsdata package or download it from the internet6. It contains the following data:\n\nn_donations: The total number of times the individual donated previous to the month being studied.\ntotal_donations: The total amount of money donated by the individual previous to the month being studied\ntime_donating: The number of months between the first donation and the month being studied\nrecent_donation: Whether or not the individual donated in the month being studied\nlast_donation: The number of months between the most recent previous donation and the month being studied\ngender: The gender of the individual\nreside: Whether the person resides in an Urban or Rural Domestic location or Overseas\nage: The age of the individual\n\n\nView the data and obtain statistical summaries. Ensure data types are appropriate and there is no missing data. Determine the outcome and input variables.\nUsing a pairplot or by plotting or correlating selected fields, try to hypothesize which variables may be significant in explaining who recently donated.\nRun a binomial logistic regression model using all input fields. Determine which input variables have a significant effect on the outcome and the direction of that effect.\nCalculate the odds ratios for the significant variables and explain their impact on the outcome.\nCheck for collinearity or multicollinearity in your model using methods from previous chapters.\nExperiment with model parsimony by reducing input variables that do not have a significant impact on the outcome. Decide on the most parsimonious model.\nCalculate a variety of Pseudo-\\(R^2\\) variants for your model. How would you explain these to someone with no statistics expertise?\nReport the conclusions of your modeling exercise to the charity by writing a simple explanation that assumes no knowledge of statistics.\nExtension: Using a variety of methods of your choice, test the hypothesis that your model fits the data. How conclusive are your tests?\n\n\n\n\n\nHosmer, David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression.\n\n\nMenard, Scott. 2010. Logistic Regression: From Introductory to Advanced Concepts and Applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "binomial_logistic_regression.html#footnotes",
    "href": "binomial_logistic_regression.html#footnotes",
    "title": "5  Binomial Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "I usually find that \\(n &gt; 50\\) and \\(0.2 &lt; p &lt; 0.8\\) are decent rules of thumb for my needs.↩︎\nThe logistic function plotted in Figure 5.5 takes the simple form \\(y = \\frac{1}{1 + e^{-\\frac{x}{0.61}}}\\).↩︎\nOften in sports the odds are expressed in the reverse order, but the concept is the same.↩︎\nIn this case, a more general form of the Ordinary Least Squares procedure is used to fit the model, known as maximum likelihood estimation.↩︎\nNote that most standard modeling functions have a built-in capability to deal with categorical variables, meaning that it’s often not necessary to explicitly construct dummies. However, it is shown here for completion sake. You may wish to try running the subsequent code without explicitly constructing dummies, but note that constructing your own dummies gives you greater control over how they are labeled in any modeling output.↩︎\nhttps://peopleanalytics-regression-book.org/data/charity_donation.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Binomial Logistic Regression for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html",
    "href": "multinomial_regression.html",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "",
    "text": "6.1 When to use it\nIn the previous chapter we looked at how to model a binary or dichotomous outcome using a logistic function. In this chapter we look at how to extend this to the case when the outcome has a number of categories that do not have any order to them. When an outcome has this nominal categorical form, it does not have a sense of direction. There is no ‘better’ or ‘worse’‍, no ‘higher’ or ‘lower’‍, there is only ‘different’‍.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#when-to-use-it",
    "href": "multinomial_regression.html#when-to-use-it",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "",
    "text": "6.1.1 Intuition for multinomial logistic regression\nA binary or dichotomous outcome like we studied in the previous chapter is already in fact a nominal outcome with two categories, so in principle we already have the basic technology with which to study this problem. That said, the way we approach the problem can differ according to the types of inferences we wish to make.\nIf we only wish to make inferences about the choice of each specific category—what drives whether an observation is in Category A versus the others, or Category B versus the others—then we have the option of running separate binomial logistic regression models on a ‘one versus the rest’ basis. In this case we can refine our model differently for each category, eliminating variables that are not significant in determining membership of that category. This could potentially lead to models being defined differently for different target outcome categories. Notably, there will be no common comparison category between these models. This is sometimes called a stratified approach.\nHowever, in many studies there is a need for a ‘reference’ category to better understand the relative odds of category membership. For example, in clinical settings the relative risk factors for different clinical outcomes can only be understood relative to a reference (usually that of the ‘most healthy’ or ‘most recovered’ patients)1. In organizational settings, one can imagine that the odds of different types of mid-tenure career path changes could only be well understood relative to a reference career path (probably the most common one). While this approach would still be founded on binomial models, the reference points of these models are different; we would need to make decisions on refining the model differently, and we interpret the coefficients in a different way.\nIn this chapter we will briefly look at the stratified approach (which is effectively a repetition of work done in the previous chapter) before focusing more intently on how we construct models and make inferences using a multinomial approach.\n\n\n6.1.2 Use cases for multinomial logistic regression\nMultinomial logistic regression is appropriate for any situation where a limited number of outcome categories (more than two) are being modeled and where those outcome categories have no order. An underlying assumption is the independence of irrelevant alternatives (IIA). Otherwise stated, this assumption means that there is no other alternative for the outcome that, if included, would disproportionately influence the membership of one of the other categories2. In cases where this assumption is violated, one could choose to take a stratified approach, or attempt hierarchical or nested multinomial model alternatives, which are beyond the scope of this book.\nExamples of typical situations that might be modeled by multinomial logistic regression include:\n\nModeling voting choice in elections with multiple candidates\nModeling choice of career options by students\nModeling choice of benefit options by employees\n\n\n\n6.1.3 Walkthrough example\nYou are an analyst at a large technology company. The company recently introduced a new health insurance provider for its employees. At the beginning of the year the employees had to choose one of three different health plan products from this provider to best suit their needs. You have been asked to determine which factors influenced the choice in product.\nThe health_insurance data set consists of the following fields:\n\nproduct: The choice of product of the individual—A, B or C\nage: The age of the individual when they made the choice\ngender: The gender of the individual as stated when they made the choice\nhousehold: The number of people living with the individual in the same household at the time of the choice\nposition_level: Position level in the company at the time they made the choice, where 1 is is the lowest and 5 is the highest\nabsent: The number of days the individual was absent from work in the year prior to the choice\n\nFirst we load the data and take a look at it briefly.\n\n# if needed, download health_insurance data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/health_insurance.csv\"\nhealth_insurance &lt;- read.csv(url)\n\n\n# view first few rows\nhead(health_insurance)\n\n  product age household position_level gender absent\n1       C  57         2              2   Male     10\n2       A  21         7              2   Male      7\n3       C  66         7              2   Male      1\n4       A  36         4              2 Female      6\n5       A  23         0              2   Male     11\n6       A  31         5              1   Male     14\n\n\n\n# view structure\nstr(health_insurance)\n\n'data.frame':   1453 obs. of  6 variables:\n $ product       : chr  \"C\" \"A\" \"C\" \"A\" ...\n $ age           : int  57 21 66 36 23 31 37 37 55 66 ...\n $ household     : int  2 7 7 4 0 5 3 0 3 2 ...\n $ position_level: int  2 2 2 2 2 1 3 3 3 4 ...\n $ gender        : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n $ absent        : int  10 7 1 6 11 14 12 25 3 18 ...\n\n\nIt looks like two of these columns should be converted to factor—product and gender—so let’s do that and then run a pairplot for a quick overview of any patterns, which can be seen in Figure 6.1.\n\n\n\n\nlibrary(GGally)\n\n# convert product and gender to factors\nhealth_insurance$product &lt;- as.factor(health_insurance$product)\nhealth_insurance$gender &lt;- as.factor(health_insurance$gender)\n\nGGally::ggpairs(health_insurance)\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Pairplot of the health_insurance data set\n\n\n\nThe data appears somewhat chaotic here. However, there are a few things to note. Firstly, we notice that there is a relatively even spread in choice between the products. We also notice that age seems to be playing a role in product choice. There are also some mild-to-moderate correlations in the data—in particular between age and position_level, and between absent and position_level. However, this problem is clearly more complex than we can determine from a bivariate perspective.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#sec-stratified",
    "href": "multinomial_regression.html#sec-stratified",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "6.2 Running stratified binomial models",
    "text": "6.2 Running stratified binomial models\nOne approach to this problem is to look at each product choice and treat it as an independent binomial logistic regression model, modeling that choice against an alternative of all other choices. Each such model may help us describe the dynamics of the choice of a specific product, but we have to be careful in making conclusions about the overall choice between the three products. Running stratified models would not be very efficient if we had a wider range of choices for our outcome, but since we only have three possible choices here, it is reasonable to take this route.\n\n6.2.1 Modeling the choice of Product A versus other products\nLet’s first create and refine a binomial model for the choice of Product A.\n\nlibrary(makedummies)\n\n# create dummies for product choice outcome\ndummy_product &lt;- makedummies::makedummies(health_insurance, \n                                          col = \"product\",\n                                          basal_level = TRUE)\n\n# combine to original set\nhealth_insurance &lt;- cbind(health_insurance, dummy_product)\n\n# run a binomial model for the Product A dummy against \n# all input variables (let glm() handle dummy input variables)\nA_model &lt;- glm(\n  formula = product_A ~ age + gender + household + \n    position_level + absent, \n  data = health_insurance, \n  family = \"binomial\"\n)\n\n\n# summary\nsummary(A_model)\n\n\nCall:\nglm(formula = product_A ~ age + gender + household + position_level + \n    absent, family = \"binomial\", data = health_insurance)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       5.873634   0.453041  12.965  &lt; 2e-16 ***\nage              -0.239814   0.013945 -17.197  &lt; 2e-16 ***\ngenderMale        0.845978   0.168237   5.028 4.94e-07 ***\ngenderNon-binary  0.222521   1.246591   0.179    0.858    \nhousehold         0.240205   0.037358   6.430 1.28e-10 ***\nposition_level    0.321497   0.071770   4.480 7.48e-06 ***\nabsent           -0.003751   0.010753  -0.349    0.727    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1864.15  on 1452  degrees of freedom\nResidual deviance:  940.92  on 1446  degrees of freedom\nAIC: 954.92\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe see that all variables except absent seem to play a significant role in the choice of Product A. All else being equal, being older makes the choice of Product A less likely. Males are more likely to choose Product A, and larger households and higher position levels also make the choice of Product A more likely. Based on this, we can consider simplifying our model to remove absent. We can also calculate odds ratios and perform some model diagnostics if we wish, similar to how we approached the problem in the previous chapter.\nThese results need to be interpreted carefully. For example, the odds ratios for the Product A choice based on a simplified model are as follows:\n\n# simpler model\nA_simple &lt;- glm(\n  formula = product_A ~ age + household + gender + position_level, \n  data = health_insurance,\n  family = \"binomial\"\n)\n\n# view odds ratio as a data frame\nas.data.frame(exp(A_simple$coefficients))\n\n                 exp(A_simple$coefficients)\n(Intercept)                     343.4406669\nage                               0.7868098\nhousehold                         1.2711317\ngenderMale                        2.3282637\ngenderNon-binary                  1.2794288\nposition_level                    1.3692971\n\n\nAs an example, and as a reminder from our previous chapter, we interpret the odds ratio for age as follows: all else being equal, every additional year of age is associated with an approximately 21% decrease in the odds of choosing Product A over the other products.\n\n\n6.2.2 Modeling other choices\nIn a similar way we can produce two other models, representing the choice of Products B and C. These models produce similar significant variables, except that position_level does not appear to be significant in the choice of Product C. If we simplify all our three models we will have a slightly differently defined model for the choice of Product C versus our models for the other two product choices. However, we can conclude in general that the only input variable that seems to be non-significant across all choices of product is absent.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#running-a-multinomial-regression-model",
    "href": "multinomial_regression.html#running-a-multinomial-regression-model",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "6.3 Running a multinomial regression model",
    "text": "6.3 Running a multinomial regression model\nAn alternative to running separate binary stratified models is to run a multinomial logistic regression model. A multinomial logistic model will base itself from a defined reference category, and run a generalized linear model on the log-odds of membership of each of the other categories versus the reference category. Due to its extensive use in epidemiology and medicine, this is often known as the relative risk of one category compared to the reference category. Mathematically speaking, if \\(X_i\\) is the vector of input variables for an observation, and the outcome observation \\(y_i\\) takes the value \\(A\\), \\(B\\) or \\(C\\), with \\(A\\) as the reference, a multinomial logistic regression model will calculate:\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i = B)}{P(y_i=A)}\\right) = \\alpha{X_i}\n\\] and\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i = C)}{P(y_i = A)}\\right) = \\beta{X_i}\n\\] for different vectors of coefficients \\(\\alpha\\) and \\(\\beta\\).\n\n6.3.1 Defining a reference level and running the model\nThe nnet package in R contains a multinom() function for running a multinomial logistic regression model using neural network technology3. Before we can run the model we need to make sure our reference level is defined.\n\n# define reference by ensuring it is the first level of the factor\nhealth_insurance$product &lt;- relevel(health_insurance$product, ref = \"A\")\n\n# check that A is now our reference\nlevels(health_insurance$product)\n\n[1] \"A\" \"B\" \"C\"\n\n\nOnce the reference outcome is defined, the multinom() function from the nnet package will run a series of binomial models comparing the reference to each of the other categories.\nFirst we will calculate our multinomial logistic regression model.\n\nlibrary(nnet)\n\nmulti_model &lt;- multinom(\n  formula = product ~ age + gender + household + \n    position_level + absent, \n  data = health_insurance\n)\n\nNow we will look at a summary of the results.\n\nsummary(multi_model)\n\nCall:\nmultinom(formula = product ~ age + gender + household + position_level + \n    absent, data = health_insurance)\n\nCoefficients:\n  (Intercept)       age  genderMale genderNon-binary  household position_level\nB    -4.60100 0.2436645 -2.38259765        0.2523409 -0.9677237     -0.4153040\nC   -10.22617 0.2698141  0.09670752       -1.2715643  0.2043568     -0.2135843\n       absent\nB 0.011676034\nC 0.003263631\n\nStd. Errors:\n  (Intercept)        age genderMale genderNon-binary  household position_level\nB   0.5105532 0.01543139  0.2324262         1.226141 0.06943089     0.08916739\nC   0.6197408 0.01567034  0.1954353         2.036273 0.04960655     0.08226087\n      absent\nB 0.01298141\nC 0.01241814\n\nResidual Deviance: 1489.365 \nAIC: 1517.365 \n\n\nNotice that the output of summary(multi_model) is much less detailed than for our standard binomial models, and it effectively just delivers the coefficients and standard errors of the two models against the reference. To determine whether specific input variables are significant we will need to calculate the p-values of the coefficients manually by calculating the z-statistics and converting (we covered this hypothesis testing methodology in Section 3.3.1).\n\n# calculate z-statistics of coefficients\nz_stats &lt;- summary(multi_model)$coefficients/\n  summary(multi_model)$standard.errors\n\n# convert to p-values\np_values &lt;- (1 - pnorm(abs(z_stats)))*2\n\n\n# display p-values in transposed data frame\ndata.frame(t(p_values))\n\n                            B            C\n(Intercept)      0.000000e+00 0.000000e+00\nage              0.000000e+00 0.000000e+00\ngenderMale       0.000000e+00 6.207192e-01\ngenderNon-binary 8.369465e-01 5.323278e-01\nhousehold        0.000000e+00 3.796088e-05\nposition_level   3.199529e-06 9.419906e-03\nabsent           3.684170e-01 7.926958e-01\n\n\n\n\n6.3.2 Interpreting the model\nThis confirms that all variables except absent play a role in the choice between all products relative to a reference of Product A. We can also calculate odds ratios as before.\n\n# display odds ratios in transposed data frame\nodds_ratios &lt;- exp(summary(multi_model)$coefficients)\ndata.frame(t(odds_ratios))\n\n                          B            C\n(Intercept)      0.01004179 3.621021e-05\nage              1.27591615 1.309721e+00\ngenderMale       0.09231048 1.101538e+00\ngenderNon-binary 1.28703467 2.803927e-01\nhousehold        0.37994694 1.226736e+00\nposition_level   0.66013957 8.076841e-01\nabsent           1.01174446 1.003269e+00\n\n\nHere are some examples of how these odds ratios can be interpreted in the multinomial context (used in combination with the p-values above):\n\nAll else being equal, every additional year of age increases the relative odds of selecting Product B versus Product A by approximately 28%, and increases the relative odds of selecting Product C versus Product A by approximately 31%\nAll else being equal, being Male reduces the relative odds of selecting Product B relative to Product A by 91%.\nAll else being equal, each additional household member deceases the odds of selecting Product B relative to Product A by 62%, and increases the odds of selecting Product C relative to Product A by 23%.\n\n\n\n6.3.3 Changing the reference\nIt may be the case that someone would like to hear the odds ratios stated against the reference of an individual choosing Product B. For example, what are the odds ratios of Product C relative to a reference of Product B? One way to do this would be to change the reference and run the model again. Another option is to note that:\n\\[\n\\frac{P(y_i = C)}{P(y_i = B)} = \\frac{\\frac{P(y_i = C)}{P(y_i = A)}}{\\frac{P(y_i = B)}{P(y_i = A)}}\n= \\frac{e^{\\beta{X_i}}}{e^{\\alpha{X_i}}}\n= e^{(\\beta - \\alpha)X_i}\n\\] Therefore\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i = C)}{P(y_i = B)}\\right) = (\\beta - \\alpha)X_i\n\\] This means we can obtain the coefficients of C against the reference of B by simply calculating the difference between the coefficients of C and B against the common reference of A. Let’s do this.\n\n# calculate difference between coefficients and view as column\ncoefs_c_to_b &lt;- summary(multi_model)$coefficients[2, ] - \n   summary(multi_model)$coefficients[1, ]\n\ndata.frame(coefs_c_to_b)\n\n                 coefs_c_to_b\n(Intercept)      -5.625169520\nage               0.026149597\ngenderMale        2.479305168\ngenderNon-binary -1.523905192\nhousehold         1.172080452\nposition_level    0.201719688\nabsent           -0.008412403\n\n\nIf the number of categories in the outcome variable is limited, this can be an efficient way to obtain the model coefficients against various reference points without having to rerun models. However, to determine standard errors and p-values for these coefficients the model will need to be recalculated against the new reference.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#model-simplification-fit-and-goodness-of-fit-for-multinomial-logistic-regression-models",
    "href": "multinomial_regression.html#model-simplification-fit-and-goodness-of-fit-for-multinomial-logistic-regression-models",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "6.4 Model simplification, fit and goodness-of-fit for multinomial logistic regression models",
    "text": "6.4 Model simplification, fit and goodness-of-fit for multinomial logistic regression models\nSimplifying a multinomial regression model needs to be done with care. In a binomial model, there is one set of coefficients and their p-values can be a strong guide to which variables can be removed safely. However, in multinomial models there are several sets of coefficients to consider.\n\n6.4.1 Gradual safe elimination of variables\nIn Hosmer, Lemeshow, and Sturdivant (2013), a gradual process of elimination of variables is recommended to ensure that significant variables that confound each other in the different logistic models are not accidentally dropped from the final model. The recommended approach is as follows:\n\nStart with the variable with the least significant p-values in all sets of coefficients—in our case absent would be the obvious first candidate.\nRun the multinomial model without this variable.\nTest that none of the previous coefficients change by more than 20–25%.\nIf there was no such change, safely remove the variable and proceed to the next non-significant variable.\nIf there is such a change, retain the variable and proceed to the next non-significant variable.\nStop when all non-significant variables have been tested.\n\nIn our case, we can compare the coefficients of the model with and without absent included and verify that the changes in the coefficients are not substantial.\n\n# remove absent\nsimpler_multi_model &lt;- multinom(\n  formula = product ~ age + gender + household + position_level,\n  data = health_insurance, \n  model = TRUE\n)\n\n\n# view coefficients with absent\ndata.frame(t(summary(multi_model)$coefficients))\n\n                           B             C\n(Intercept)      -4.60099991 -10.226169428\nage               0.24366447   0.269814063\ngenderMale       -2.38259765   0.096707521\ngenderNon-binary  0.25234087  -1.271564323\nhousehold        -0.96772368   0.204356774\nposition_level   -0.41530400  -0.213584308\nabsent            0.01167603   0.003263631\n\n\n\n# view coefficients without absent\ndata.frame(t(summary(simpler_multi_model)$coefficients))\n\n                          B            C\n(Intercept)      -4.5008999 -10.19269011\nage               0.2433855   0.26976294\ngenderMale       -2.3771342   0.09801281\ngenderNon-binary  0.1712091  -1.29636779\nhousehold        -0.9641956   0.20510806\nposition_level   -0.3912014  -0.20908835\n\n\nWe can see that only genderNon-binary changed substantially, but we note that this is on an extremely small sample size and so will not have any effect on our model4. It therefore appears safe to remove absent. Furthermore, the Akaike Information Criterion is equally valid in multinomial models for evaluating model parsimony. Here we can calculate that the AIC of our model with and without absent is 1517.36 and 1514.25, respectively, confirming that the model without absent is marginally more parsimonious.\n\n\n6.4.2 Model fit and goodness-of-fit\nAs with the binomial case, a variety of Pseudo-\\(R^2\\) methods are available to assess the fit of a multinomial logistic regression model, although some of our previous variants (particularly Tjur) are not defined on models with more than two outcome categories.\n\nDescTools::PseudoR2(simpler_multi_model, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n\n  McFadden   CoxSnell Nagelkerke \n 0.5329175  0.6896945  0.7760413 \n\n\nDue to the fact that multinomial models have more than one set of coefficients, assessing goodness-of-fit is more challenging, and is still an area of intense research. The most approachable method to assess model confidence is the Hosmer-Lemeshow test mentioned in the previous chapter, which was extended in Fagerland, Hosmer, and Bofin (2008) for multinomial models. An implementation is available in the generalhoslem package in R. However, this version of the Hosmer-Lemeshow test is problematic for models with a small number of input variables (fewer than ten), and therefore we will not experiment with it here. For further exploration of this topic, Chapter 8 of Hosmer, Lemeshow, and Sturdivant (2013) is recommended, and for a more thorough treatment of the entire topic of categorical analytics, Agresti (2007) is an excellent companion.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#multinomial-logistic-regression-using-python",
    "href": "multinomial_regression.html#multinomial-logistic-regression-using-python",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "6.5 Multinomial logistic regression using Python",
    "text": "6.5 Multinomial logistic regression using Python\nIn Python, multinomial logistic regression is similarly available using the statsmodels formula API. As usual, care must be taken to ensure that the reference category is appropriately defined, dummy input variables need to be explicitly constructed, and a constant term must be added to ensure an intercept is calculated.\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# load health insurance data\nurl = \"https://peopleanalytics-regression-book.org/data/health_insurance.csv\"\nhealth_insurance = pd.read_csv(url)\n\n# convert product to categorical as an outcome variable\ny = pd.Categorical(health_insurance['product'])\n\n# create dummies for gender\nX1 = pd.get_dummies(health_insurance['gender'], drop_first = True)\n\n# replace back into input variables \nX2 = health_insurance.drop(['product', 'gender'], axis = 1)\nX = pd.concat([X1, X2], axis = 1)\n\n# add a constant term to ensure intercept is calculated\nXc = sm.add_constant(X)\n\n# define model\nmodel = sm.MNLogit(y, Xc.astype(float))\n\n# fit model\ninsurance_model = model.fit()\n# see results summary\nprint(insurance_model.summary())\n\n\n\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1453\nModel:                        MNLogit   Df Residuals:                     1439\nMethod:                           MLE   Df Model:                           12\nDate:                Mon, 22 Dec 2025   Pseudo R-squ.:                  0.5332\nTime:                        13:31:43   Log-Likelihood:                -744.68\nconverged:                       True   LL-Null:                       -1595.3\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==================================================================================\n           y=B       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst             -4.6010      0.511     -9.012      0.000      -5.602      -3.600\nMale              -2.3826      0.232    -10.251      0.000      -2.838      -1.927\nNon-binary         0.2528      1.226      0.206      0.837      -2.151       2.656\nage                0.2437      0.015     15.790      0.000       0.213       0.274\nhousehold         -0.9677      0.069    -13.938      0.000      -1.104      -0.832\nposition_level    -0.4153      0.089     -4.658      0.000      -0.590      -0.241\nabsent             0.0117      0.013      0.900      0.368      -0.014       0.037\n----------------------------------------------------------------------------------\n           y=C       coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst            -10.2261      0.620    -16.501      0.000     -11.441      -9.011\nMale               0.0967      0.195      0.495      0.621      -0.286       0.480\nNon-binary        -1.2698      2.036     -0.624      0.533      -5.261       2.721\nage                0.2698      0.016     17.218      0.000       0.239       0.301\nhousehold          0.2043      0.050      4.119      0.000       0.107       0.302\nposition_level    -0.2136      0.082     -2.597      0.009      -0.375      -0.052\nabsent             0.0033      0.012      0.263      0.793      -0.021       0.028\n==================================================================================",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#learning-exercises",
    "href": "multinomial_regression.html#learning-exercises",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "6.6 Learning exercises",
    "text": "6.6 Learning exercises\n\n6.6.1 Discussion questions\n\nDescribe the difference between a stratified versus a multinomial approach to modeling an outcome with more than two nominal categories.\nDescribe how you would interpret the odds ratio of an input variable for a given category in a stratified modeling approach.\nDescribe what is meant by the ‘reference’ of a multinomial logistic regression model with at least three nominal outcome categories.\nDescribe how you would interpret the odds ratio of an input variable for a given category in a multinomial modeling approach.\nGiven a multinomial logistic regression model with outcome categories A, B, C and D and reference category A, describe two ways to determine the coefficients of a multinomial logistic regression model with reference category C.\nDescribe a process for safely simplifying a multinomial logistic regression model by removing input variables.\n\n\n\n6.6.2 Data exercises\nUse the same health_insurance data set from this chapter to answer these questions.\n\nComplete the full stratified approach to modeling the three product choices that was started in Section 6.2. Calculate the coefficients, odds ratios and p-values in each case.\nCarefully write down your interpretation of the odds ratios from the previous question.\nRun a multinomial logistic regression model on the product outcome using Product B as reference. Calculate the coefficients, ratios and p-values in each case.\nVerify that the coefficients for Product C against reference Product B matches those calculated in Section 6.3.3.\nCarefully write down your interpretation of the odds ratios calculated in the previous question.\nUse the process described in Section 6.4.1 to simplify the multinomial model in Question 3.\n\n\n\n\n\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis.\n\n\nFagerland, Morten W., David W. Hosmer, and Anna M. Bofin. 2008. “Multinomial Goodness‐of‐fit Tests for Logistic Regression Models.” Statistics in Medicine.\n\n\nHosmer, David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied Logistic Regression.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "multinomial_regression.html#footnotes",
    "href": "multinomial_regression.html#footnotes",
    "title": "6  Multinomial Logistic Regression for Nominal Category Outcomes",
    "section": "",
    "text": "In Hosmer, Lemeshow, and Sturdivant (2013), a good example is provided where the outcome is the placement of psychiatric patients in various forms of aftercare, with Outpatient Care as the reference.↩︎\nPut differently, it assumes that adding or removing any other available alternative would affect the odds of the other alternatives in equal proportion. It has been shown that there have been many studies that proceeded with a multinomial approach despite the violation of this assumption.↩︎\nNeural networks are computational structures which consist of a network of nodes, each of which take an input and perform a mathematical function to return an output onward in the network. Most commonly they are used in deep learning, but a simple neural network here can model these different categories using a logistic function.↩︎\nRemoving insignificant dummy variables, or combining them to make simpler dummy variables can also be done. In the case of these observations of genderNon-binary, given the relatively small number of these observations in the data set, it does not harm the model to leave this variable included, safe in the knowledge that it has a minuscule effect↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multinomial Logistic Regression for Nominal Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html",
    "href": "ordinal_regression.html",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "",
    "text": "7.1 When to use it\nOften our outcomes will be categorical in nature, but they will also have an order to them. These are sometimes known as ordinal outcomes. Some very common examples of this include ratings of some form, such as job performance ratings or survey responses on Likert scales. The appropriate modeling approach for these outcome types is ordinal logistic regression. Surprisingly, this approach is frequently not understood or adopted by analysts. Often they treat the outcome as a continuous variable and perform simple linear regression, which can lead to wildly inaccurate inferences. Given the prevalence of ordinal outcomes in people analytics, it would serve analysts well to know how to run ordinal logistic regression models, how to interpret them and how to confirm their validity.\nIn fact, there are numerous known ways to approach the inferential modeling of ordinal outcomes, all of which build on the theory of linear, binomial and multinomial regression which we covered in previous chapters. In this chapter, we will focus on the most commonly adopted approach: proportional odds logistic regression. Proportional odds models (sometimes known as constrained cumulative logistic models) are more attractive than other approaches because of their ease of interpretation but cannot be used blindly without important checking of underlying assumptions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#when-to-use-it",
    "href": "ordinal_regression.html#when-to-use-it",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "",
    "text": "7.1.1 Intuition for proportional odds logistic regression\nOrdinal outcomes can be considered to be suitable for an approach somewhere ‘between’ linear regression and multinomial regression. In common with linear regression, we can consider our outcome to increase or decrease dependent on our inputs. However, unlike linear regression the increase and decrease is ‘stepwise’ rather than continuous, and we do not know that the difference between the steps is the same across the scale. In medical settings, the difference between moving from healthy to an early-stage disease may not be equivalent to moving from an early-stage disease to an intermediate- or advanced-stage. Equally, it may be a much bigger psychological step for a performance evaluator to give someone a low performance rating than to give them a high performance rating. In this sense, we are analyzing categorical outcomes similar to a multinomial approach.\nTo formalize this intuition, we can imagine a latent version of our outcome variable that takes a continuous form, and where the categories are formed at specific cutoff points on that continuous variable. For example, if our outcome variable \\(y\\) represents job performance on an ordinal scale of 1 (low) to 5 (high), we can imagine we are actually dealing with a continuous variable \\(y'\\) along with four increasing ‘cutoff points’ for \\(y'\\) at \\(\\tau_1\\), \\(\\tau_2\\), \\(\\tau_3\\) and \\(\\tau_4\\). We then define each ordinal category as follows: \\(y = 1\\) corresponds to \\(y' \\le \\tau_1\\), \\(y \\le 2\\) to \\(y' \\le \\tau_2\\), \\(y \\le 3\\) to \\(y' \\le \\tau_3\\) and \\(y \\le 4\\) to \\(y' \\le \\tau_4\\). Further, at each such cutoff \\(\\tau_k\\), we assume that the probability \\(P(y' &gt; \\tau_k)\\) takes the form of a logistic function. Therefore, in the proportional odds model, we ‘divide’ the probability space at each level of the outcome variable and consider each as a binomial logistic regression model. For example, at rating 3, we generate a binomial logistic regression model of \\(P(y' &gt; \\tau_3)\\), as illustrated in Figure 7.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Proportional odds model illustration for a 5-point job performance scale outcome greater than 3 on a single input variable. Each cutoff point in the latent continuous outcome variable gives rise to a binomial logistic function.\n\n\n\nThis approach leads to a highly interpretable model that provides a single set of coefficients that are agnostic to the outcome category. For example, we can say that each unit increase in input variable \\(x\\) increases the odds of \\(y\\) being in a higher category by a certain ratio.\n\n\n7.1.2 Use cases for proportional odds logistic regression\nProportional odds logistic regression can be used when there are more than two outcome categories that have an order. An important underlying assumption is that no input variable has a disproportionate effect on a specific level of the outcome variable. This is known as the proportional odds assumption. Referring to Figure 7.1, this assumption means that the ‘slope’ of the logistic function is the same for all category cutoffs1. If this assumption is violated, we cannot reduce the coefficients of the model to a single set across all outcome categories, and this modeling approach fails. Therefore, testing the proportional odds assumption is an important validation step for anyone running this type of model.\nExamples of problems that can utilize a proportional odds logistic regression approach include:\n\nUnderstanding the factors associated with higher ratings in an employee survey on a Likert scale\nUnderstanding the factors associated with higher job performance ratings on an ordinal performance scale\nUnderstanding the factors associated with voting preference in a ranked preference voting system (for example, proportional representation systems)\n\n\n\n7.1.3 Walkthrough example\nYou are an analyst for a sports broadcaster who is doing a feature on player discipline in professional soccer games. To prepare for the feature, you have been asked to verify whether certain metrics are significant in influencing the extent to which a player will be disciplined by the referee for unfair or dangerous play in a game. You have been provided with data on over 2000 different players in different games, and the data contains these fields:\n\ndiscipline: A record of the maximum discipline taken by the referee against the player in the game. ‘None’ means no discipline was taken, ‘Yellow’ means the player was issued a yellow card (warned), ‘Red’ means the player was issued a red card and ordered off the field of play.\nn_yellow_25 is the total number of yellow cards issued to the player in the previous 25 games they played prior to this game.\nn_red_25 is the total number of red cards issued to the player in the previous 25 games they played prior to this game.\nposition is the playing position of the player in the game: ‘D’ is defense (including goalkeeper), ‘M’ is midfield and ‘S’ is striker/attacker.\nlevel is the skill level of the competition in which the game took place, with 1 being higher and 2 being lower.\ncountry is the country in which the game took place—England or Germany.\nresult is the result of the game for the team of the player—‘W’ is win, ‘L’ is lose, ‘D’ is a draw/tie.\n\nLet’s download the soccer data set and take a quick look at it.\n\n# if needed, download data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer &lt;- read.csv(url)\n\n\nhead(soccer)\n\n  discipline n_yellow_25 n_red_25 position result country level\n1       None           4        1        S      D England     1\n2       None           2        2        D      W England     2\n3       None           2        1        M      D England     1\n4       None           2        1        M      L Germany     1\n5       None           2        0        S      W Germany     1\n6       None           3        2        M      W England     1\n\n\nLet’s also take a look at the structure of the data.\n\nstr(soccer)\n\n'data.frame':   2291 obs. of  7 variables:\n $ discipline : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n\n\nWe see that there are numerous fields that need to be converted to factors before we can model them. Firstly, our outcome of interest is discipline and this needs to be an ordered factor, which we can choose to increase with the seriousness of the disciplinary action.\n\n# convert discipline to ordered factor\nsoccer$discipline &lt;- ordered(soccer$discipline, \n                             levels = c(\"None\", \"Yellow\", \"Red\"))\n\n# check conversion\nstr(soccer)\n\n'data.frame':   2291 obs. of  7 variables:\n $ discipline : Ord.factor w/ 3 levels \"None\"&lt;\"Yellow\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n\n\nWe also know that position, country, result and level are categorical, so we convert them to factors. We could in fact choose to convert result and level into ordered factors if we so wish, but this is not necessary for input variables, and the results are usually a little bit easier to read as nominal factors.\n\n# apply as.factor to four columns\ncats &lt;- c(\"position\", \"country\", \"result\", \"level\")\nsoccer[ ,cats] &lt;- lapply(soccer[ ,cats], as.factor)\n\n# check again\nstr(soccer)\n\n'data.frame':   2291 obs. of  7 variables:\n $ discipline : Ord.factor w/ 3 levels \"None\"&lt;\"Yellow\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n $ position   : Factor w/ 3 levels \"D\",\"M\",\"S\": 3 1 2 2 3 2 2 2 2 1 ...\n $ result     : Factor w/ 3 levels \"D\",\"L\",\"W\": 1 3 1 2 3 3 3 3 1 2 ...\n $ country    : Factor w/ 2 levels \"England\",\"Germany\": 1 1 1 2 2 1 2 1 2 1 ...\n $ level      : Factor w/ 2 levels \"1\",\"2\": 1 2 1 1 1 1 2 1 1 1 ...\n\n\nNow our data is in a position to run a model. You may wish to conduct some exploratory data analysis at this stage similar to previous chapters, but from this chapter onward we will skip this and focus on the modeling methodology.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#modeling-ordinal-outcomes-under-the-assumption-of-proportional-odds",
    "href": "ordinal_regression.html#modeling-ordinal-outcomes-under-the-assumption-of-proportional-odds",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "7.2 Modeling ordinal outcomes under the assumption of proportional odds",
    "text": "7.2 Modeling ordinal outcomes under the assumption of proportional odds\nFor simplicity, and noting that this is easily generalizable, let’s assume that we have an observation of an ordinal outcome variable \\(y_i\\) with three levels similar to our walkthrough example, and that we have an observation of one input variable \\(x_i\\). Let’s call the outcome levels 1, 2 and 3. To follow our intuition from Section 7.1.1, we can model a linear continuous variable \\(y_i' = \\alpha_1x_i + \\alpha_0 + \\epsilon_i\\), where \\(\\epsilon_i\\) is some error with a mean of zero, and two increasing cutoff values \\(\\tau_1\\) and \\(\\tau_2\\). We define \\(y_i\\) in terms of \\(y_i'\\) as follows: \\(y_i = 1\\) if \\(y_i' \\le \\tau_1\\), \\(y_i = 2\\) if \\(\\tau_1 &lt; y_i' \\le \\tau_2\\) and \\(y_i = 3\\) if \\(y_i' &gt; \\tau_2\\).\n\n7.2.1 Using a latent continuous outcome variable to derive a proportional odds model\nRecall from Section 4.5.3 that our linear regression approach assumes that our residuals \\(\\epsilon_i\\) around our line \\(y_i' = \\alpha_1x_i + \\alpha_0\\) have a normal distribution with a constant variance. Let’s modify that assumption slightly and instead assume that our residuals take a logistic distribution. Therefore, \\(y_i' = \\alpha_1x_i + \\alpha_0 + \\epsilon_i\\), where \\(\\epsilon_i\\) follows the shape of a logistic function. That is\n\\[\nP(\\epsilon_i \\leq z) = \\frac{1}{1 + e^{-z}}\n\\]\nLet’s look at the probability that our ordinal outcome observation \\(y_i\\) is in its lowest category.\n\\[\n\\begin{aligned}\nP(y_i = 1) &= P(y_i' \\le \\tau_1) \\\\\n&= P(\\alpha_1x_i + \\alpha_0 + \\epsilon_i \\leq \\tau_1) \\\\\n&= P(\\epsilon_i \\leq \\tau_1 - \\alpha_1x_i - \\alpha_0) \\\\\n&= P(\\epsilon_i \\le \\gamma_1 - \\beta{x_i}) \\\\\n&= \\frac{1}{1 + e^{-(\\gamma_1 - \\beta{x_i})}}\n\\end{aligned}\n\\]\nwhere \\(\\gamma_1 = \\tau_1 - \\alpha_0\\) and \\(\\beta = \\alpha_1\\).\nSince our only values for \\(y_i\\) are 1, 2 and 3, similar to our derivations in Section 5.2, we conclude that \\(P(y_i &gt; 1) = 1 - P(y_i = 1)\\), which calculates to\n\\[\nP(y_i &gt; 1) = \\frac{e^{-(\\gamma_1 - \\beta{x_i})}}{1 + e^{-(\\gamma_1 - \\beta{x_i})}}\n\\]\nTherefore\n\\[\n\\begin{aligned}\n\\frac{P(y_i = 1)}{P(y_i &gt; 1)} = \\frac{\\frac{1}{1 + e^{-(\\gamma_1 - \\beta{x_i})}}}{\\frac{e^{-(\\gamma_1 - \\beta{x_i})}}{1 + e^{-(\\gamma_1 - \\beta{x_i})}}}\n= e^{\\gamma_1 - \\beta{x_i}}\n\\end{aligned}\n\\]\nBy applying the natural logarithm, we conclude that the log odds of \\(y_i\\) being in our bottom category is\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i = 1)}{P(y_i &gt; 1)}\\right) = \\gamma_1 - \\beta{x_i}\n\\]\nIn a similar way we can derive the log odds of our ordinal outcome being in our bottom two categories as\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i \\leq 2)}{P(y_i = 3)}\\right) = \\gamma_2 - \\beta{x_i}\n\\]\nwhere \\(\\gamma_2 = \\frac{\\tau_2 - \\alpha_0}{\\sigma}\\). One can easily see how this generalizes to an arbitrary number of ordinal categories, where we can state the log odds of being in category \\(k\\) or lower as\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i \\leq k)}{P(y_i &gt; k)}\\right) = \\gamma_k - \\beta{x_i}\n\\] Alternatively, we can state the log odds of being in a category higher than \\(k\\) by simply inverting the above expression:\n\\[\n\\mathrm{ln}\\left(\\frac{P(y_i &gt; k)}{P(y_i \\leq k)}\\right) = -(\\gamma_k - \\beta{x_i}) = \\beta{x_i} - \\gamma_k\n\\]\nBy taking exponents we see that the impact of a unit change in \\(x_i\\) on the odds of \\(y_i\\) being in a higher ordinal category is \\(\\beta\\), irrespective of what category we are looking at. Therefore we have a single coefficient to explain the effect of \\(x_i\\) on \\(y_i\\) throughout the ordinal scale. Note that there are still different intercept coefficients \\(\\gamma_1\\) and \\(\\gamma_2\\) for each level of the ordinal scale.\n\n\n7.2.2 Running a proportional odds logistic regression model\nThe MASS package provides a function polr() for running a proportional odds logistic regression model on a data set in a similar way to our previous models. The key (and obvious) requirement is that the outcome is an ordered factor. Since we did our conversions in Section 7.1.3 we are ready to run this model. We will start by running it on all input variables and let the polr() function handle our dummy variables automatically.\n\n# run proportional odds model\nlibrary(MASS)\nmodel &lt;- polr(\n  formula = discipline ~ n_yellow_25 + n_red_25 + position + \n    country + level + result, \n  data = soccer\n)\n\n# get summary\nsummary(model)\n\nCall:\npolr(formula = discipline ~ n_yellow_25 + n_red_25 + position + \n    country + level + result, data = soccer)\n\nCoefficients:\n                  Value Std. Error t value\nn_yellow_25     0.32236    0.03308  9.7456\nn_red_25        0.38324    0.04051  9.4616\npositionM       0.19685    0.11649  1.6899\npositionS      -0.68534    0.15011 -4.5655\ncountryGermany  0.13297    0.09360  1.4206\nlevel2          0.09097    0.09355  0.9724\nresultL         0.48303    0.11195  4.3147\nresultW        -0.73947    0.12129 -6.0966\n\nIntercepts:\n            Value   Std. Error t value\nNone|Yellow  2.5085  0.1918    13.0770\nYellow|Red   3.9257  0.2057    19.0834\n\nResidual Deviance: 3444.534 \nAIC: 3464.534 \n\n\nWe can see that the summary returns a single set of coefficients on our input variables as we expect, with standard errors and t-statistics. We also see that there are separate intercepts for the various levels of our outcomes, as we also expect. In interpreting our model, we generally don’t have a great deal of interest in the intercepts, but we will focus on the coefficients. First we would like to obtain p-values, so we can add a p-value column using the conversion methods from the t-statistic which we learned in Section 3.3.12.\n\n# get coefficients (it's in matrix form)\ncoefficients &lt;- summary(model)$coefficients\n\n# calculate p-values\np_value &lt;- (1 - pnorm(abs(coefficients[ ,\"t value\"]), 0, 1))*2\n\n# bind back to coefficients\n(coefficients &lt;- cbind(coefficients, p_value))\n\n                     Value Std. Error    t value      p_value\nn_yellow_25     0.32236030 0.03307768  9.7455530 0.000000e+00\nn_red_25        0.38324333 0.04050515  9.4615948 0.000000e+00\npositionM       0.19684666 0.11648690  1.6898610 9.105456e-02\npositionS      -0.68533697 0.15011194 -4.5655061 4.982908e-06\ncountryGermany  0.13297173 0.09359946  1.4206464 1.554196e-01\nlevel2          0.09096627 0.09354717  0.9724108 3.308462e-01\nresultL         0.48303227 0.11195131  4.3146639 1.598459e-05\nresultW        -0.73947295 0.12129301 -6.0965834 1.083594e-09\nNone|Yellow     2.50850778 0.19182628 13.0769767 0.000000e+00\nYellow|Red      3.92572124 0.20571423 19.0833721 0.000000e+00\n\n\nNext we can convert our coefficients to odds ratios.\n\n# calculate odds ratios\nodds_ratio &lt;- exp(coefficients[ ,\"Value\"])\n\nWe can display all our critical statistics by combining them into a dataframe.\n\n# combine with coefficient and p_value\n(coefficients &lt;- cbind(\n  coefficients[ ,c(\"Value\", \"p_value\")],\n  odds_ratio\n))\n\n                     Value      p_value odds_ratio\nn_yellow_25     0.32236030 0.000000e+00  1.3803820\nn_red_25        0.38324333 0.000000e+00  1.4670350\npositionM       0.19684666 9.105456e-02  1.2175573\npositionS      -0.68533697 4.982908e-06  0.5039204\ncountryGermany  0.13297173 1.554196e-01  1.1422177\nlevel2          0.09096627 3.308462e-01  1.0952321\nresultL         0.48303227 1.598459e-05  1.6209822\nresultW        -0.73947295 1.083594e-09  0.4773654\nNone|Yellow     2.50850778 0.000000e+00 12.2865821\nYellow|Red      3.92572124 0.000000e+00 50.6896241\n\n\nTaking into consideration the p-values, we can interpret our coefficients as follows, in each case assuming that other coefficients are held still:\n\nEach additional yellow card received in the prior 25 games is associated with an approximately 38% higher odds of greater disciplinary action by the referee.\nEach additional red card received in the prior 25 games is associated with an approximately 47% higher odds of greater disciplinary action by the referee.\nStrikers have approximately 50% lower odds of greater disciplinary action from referees compared to Defenders.\nA player on a team that lost the game has approximately 62% higher odds of greater disciplinary action versus a player on a team that drew the game.\nA player on a team that won the game has approximately 52% lower odds of greater disciplinary action versus a player on a team that drew the game.\n\nWe can, as per previous chapters, remove the level and country variables from this model to simplify it if we wish. An examination of the coefficients and the AIC of the simpler model will reveal no substantial difference, and therefore we proceed with this model.\n\n\n7.2.3 Calculating the likelihood of an observation being in a specific ordinal category\nRecall from Section 7.2.1 that our proportional odds model generates multiple stratified binomial models, each of which has following form:\n\\[\nP(y_i \\leq k) = P(y_i' \\leq \\tau_k)\n\\]\nNote that for an ordinal observation \\(y_i\\), if \\(y_i \\leq k\\) and \\(y_i &gt; k-1\\), then \\(y_i = k\\). Therefore \\(P(y_i = k) = P(y_i \\leq k) - P(y_i \\leq k - 1)\\). This means we can calculate the specific probability of an observation being in each level of the ordinal variable in our fitted model by simply calculating the difference between the fitted values from each pair of adjacent stratified binomial models. In our walkthrough example, this means we can calculate the specific probability of no action from the referee, or a yellow card being awarded, or a red card being awarded. These can be viewed using the fitted() function.\n\nhead(fitted(model))\n\n       None     Yellow        Red\n1 0.8207093 0.12900184 0.05028889\n2 0.8514232 0.10799553 0.04058128\n3 0.7830785 0.15400189 0.06291964\n4 0.6609864 0.22844107 0.11057249\n5 0.9591298 0.03064719 0.01022301\n6 0.7887766 0.15027145 0.06095200\n\n\nIt can be seen from this output how ordinal logistic regression models can be used in predictive analytics by classifying new observations into the ordinal category with the highest fitted probability. This also allows us to graphically understand the output of a proportional odds model. Figure 7.3 shows the output from a simpler proportional odds model fitted against the n_yellow_25 and n_red_25 input variables, with the fitted probabilities of each level of discipline from the referee plotted on the different colored surfaces. We can see in most situations that no discipline is the most likely outcome and a red card is the least likely outcome. Only at the upper ends of the scales do we see the likelihood of discipline overcoming the likelihood of no discipline, with a strong likelihood of red cards for those with an extremely poor recent disciplinary record.\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: 3D visualization of a simple proportional odds model for discipline fitted against n_yellow_25 and n_red_25 in the soccer data set. Blue represents the probability of no discipline from the referee. Yellow and red represent the probability of a yellow card and a red card, respectively.\n\n\n\n\n7.2.4 Model diagnostics\nSimilar to binomial and multinomial models, pseudo-\\(R^2\\) methods are available for assessing model fit, and AIC can be used to assess model parsimony. Note that DescTools::PseudoR2() also offers AIC.\n\n# diagnostics of simpler model\nDescTools::PseudoR2(\n  model, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\n\n    McFadden     CoxSnell   Nagelkerke          AIC \n   0.1009411    0.1553264    0.1912445 3464.5339371 \n\n\nThere are numerous tests of goodness-of-fit that can apply to ordinal logistic regression models, and this area is the subject of considerable recent research. The generalhoslem package in R contains routes to four possible tests, with two of them particularly recommended for ordinal models. Each work in a similar way to the Hosmer-Lemeshow test discussed in Section Section 5.3.2, by dividing the sample into groups and comparing the observed versus the fitted outcomes using a chi-square test. Since the null hypothesis is a good model fit, low p-values indicate potential problems with the model. We run these tests below for reference. For more information, see Fagerland and Hosmer (2017), and for a really intensive treatment of ordinal data modeling Agresti (2010) is recommended.\n\n# lipsitz test \ngeneralhoslem::lipsitz.test(model)\n\n\n    Lipsitz goodness of fit test for ordinal response models\n\ndata:  formula:  discipline ~ n_yellow_25 + n_red_25 + position + country + level + formula:      result\nLR statistic = 10.429, df = 9, p-value = 0.3169\n\n\n\n# pulkstenis-robinson test \n# (requires the vector of categorical input variables as an argument)\ngeneralhoslem::pulkrob.chisq(model, catvars = cats)\n\n\n    Pulkstenis-Robinson chi-squared test\n\ndata:  formula:  discipline ~ n_yellow_25 + n_red_25 + position + country + level + formula:      result\nX-squared = 129.29, df = 137, p-value = 0.668",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#testing-the-proportional-odds-assumption",
    "href": "ordinal_regression.html#testing-the-proportional-odds-assumption",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "7.3 Testing the proportional odds assumption",
    "text": "7.3 Testing the proportional odds assumption\nAs we discussed earlier, the suitability of a proportional odds logistic regression model depends on the assumption that each input variable has a similar effect on the different levels of the ordinal outcome variable. It is very important to check that this assumption is not violated before proceeding to declare the results of a proportional odds model valid. There are two common approaches to validating the proportional odds assumption, and we will go through each of them here.\n\n7.3.1 Sighting the coefficients of stratified binomial models\nAs we learned above, proportional odds regression models effectively act as a series of stratified binomial models under the assumption that the ‘slope’ of the logistic function of each stratified model is the same. We can verify this by actually running stratified binomial models on our data and checking for similar coefficients on our input variables. Let’s use our walkthrough example to illustrate.\nLet’s create two columns with binary values to correspond to the two higher levels of our ordinal variable.\n\n# create binary variable for \"Yellow\" or \"Red\" versus \"None\"\nsoccer$yellow_plus &lt;- ifelse(soccer$discipline == \"None\", 0, 1)\n\n# create binary variable for \"Red\" versus \"Yellow\" or \"None\"\nsoccer$red &lt;- ifelse(soccer$discipline == \"Red\", 1, 0)\n\nNow let’s create two binomial logistic regression models for the two higher levels of our outcome variable.\n\n# model for at least a yellow card\nyellowplus_model &lt;- glm(\n  yellow_plus ~ n_yellow_25 + n_red_25 + position + \n    result + country + level, \n  data = soccer, \n  family = \"binomial\"\n)\n\n# model for a red card\nred_model &lt;- glm(\n  red ~ n_yellow_25 + n_red_25 + position + \n    result + country + level,\n  data = soccer, \n  family = \"binomial\"\n)\n\nWe can now display the coefficients of both models and examine the difference between them.\n\n(coefficient_comparison &lt;- data.frame(\n  yellowplus = summary(yellowplus_model)$coefficients[ , \"Estimate\"],\n  red = summary(red_model)$coefficients[ ,\"Estimate\"],\n  diff = summary(red_model)$coefficients[ ,\"Estimate\"] - \n    summary(yellowplus_model)$coefficients[ , \"Estimate\"]\n))\n\n                yellowplus         red        diff\n(Intercept)    -2.63646519 -3.89865929 -1.26219410\nn_yellow_25     0.34585921  0.32468746 -0.02117176\nn_red_25        0.41454059  0.34213238 -0.07240822\npositionM       0.26108978  0.06387813 -0.19721165\npositionS      -0.72118538 -0.44228286  0.27890252\nresultL         0.46162324  0.64295195  0.18132871\nresultW        -0.77821530 -0.58536482  0.19285048\ncountryGermany  0.13136665  0.10796418 -0.02340247\nlevel2          0.08056718  0.12421593  0.04364875\n\n\nIgnoring the intercept, which is not of concern here, the differences appear relatively small. Large differences in coefficients would indicate that the proportional odds assumption is likely violated and alternative approaches to the problem should be considered.\n\n\n7.3.2 The Brant-Wald test\nIn the previous method, some judgment is required to decide whether the coefficients of the stratified binomial models are ‘different enough’ to decide on violation of the proportional odds assumption. For those requiring more formal support, an option is the Brant-Wald test. Under this test, a generalized ordinal logistic regression model is approximated and compared to the calculated proportional odds model. A generalized ordinal logistic regression model is simply a relaxing of the proportional odds model to allow for different coefficients at each level of the ordinal outcome variable.\nThe Wald test is conducted on the comparison of the proportional odds and generalized models. A Wald test is a hypothesis test of the significance of the difference in model coefficients, producing a chi-square statistic. A low p-value in a Brant-Wald test is an indicator that the coefficient does not satisfy the proportional odds assumption. The brant package in R provides an implementation of the Brant-Wald test, and in this case supports our judgment that the proportional odds assumption holds.\n\nlibrary(brant)\nbrant::brant(model)\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     14.16   8   0.08\nn_yellow_25 0.24    1   0.62\nn_red_25    1.83    1   0.18\npositionM   1.7 1   0.19\npositionS   2.33    1   0.13\ncountryGermany  0.04    1   0.85\nlevel2      0.13    1   0.72\nresultL     1.53    1   0.22\nresultW     1.3 1   0.25\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\nA p-value of less than 0.05 on this test—particularly on the Omnibus plus at least one of the variables—should be interpreted as a failure of the proportional odds assumption.\n\n\n7.3.3 Alternatives to proportional odds models\nThe proportional odds model is by far the most utilized approach to modeling ordinal outcomes (not least because of neglect in the testing of the underlying assumptions). But as we have learned, it is not always an appropriate model choice for ordinal outcomes. When the test of proportional odds fails, we need to consider a strategy for remodeling the data. If only one or two variables fail the test of proportional odds, a simple option is to remove those variables. Whether or not we are comfortable doing this will depend very much on the impact on overall model fit.\nIn the event where the option to remove variables is unattractive, alternative models for ordinal outcomes should be considered. The most common alternatives (which we will not cover in depth here, but are explored in Agresti (2010)) are:\n\nBaseline logistic model. This model is the same as the multinomial regression model covered in the previous chapter, using the lowest ordinal value as the reference.\nAdjacent-category logistic model. This model compares each level of the ordinal variable to the next highest level, and it is a constrained version of the baseline logistic model. The brglm2 package in R offers a function bracl() for calculating an adjacent category logistic model.\nContinuation-ratio logistic model. This model compares each level of the ordinal variable to all lower levels. This can be modeled using binary logistic regression techniques, but new variables need to be constructed from the data set to allow this. The R package rms has a function cr.setup() which is a utility for preparing an outcome variable for a continuation ratio model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#ordinal-logistic-regression-using-python",
    "href": "ordinal_regression.html#ordinal-logistic-regression-using-python",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "7.4 Ordinal logistic regression using Python",
    "text": "7.4 Ordinal logistic regression using Python\nIn Python, statsmodels offers an ordinal model based on proportional odds logistic regression. It is important to define the order of your outcome categories before running this model3.\n\nimport pandas as pd\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\nfrom pandas.api.types import CategoricalDtype\n\n# load soccer data\nurl = \"https://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer = pd.read_csv(url)\n\n# fill NaN with \"None\" in discipline column\nsoccer.discipline = soccer.discipline.fillna(\"None\")\n\n# turn discipline column into ordered category\ncat = CategoricalDtype(categories=[\"None\", \"Yellow\", \"Red\"], ordered=True)\nsoccer.discipline = soccer.discipline.astype(cat)\n\n# run proportional odds logistic regression\nprop_odds_logit = OrderedModel.from_formula(\n  \"discipline ~ 1 + n_yellow_25 + n_red_25 + position + result + country + level\", \n  soccer, \n  distr='logit'\n)\nsoccer_model = prop_odds_logit.fit(method='bfgs')\nprint(soccer_model.summary())\n\n\n\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:             discipline   Log-Likelihood:                -1722.3\nModel:                   OrderedModel   AIC:                             3465.\nMethod:            Maximum Likelihood   BIC:                             3522.\nDate:                Mon, 22 Dec 2025                                         \nTime:                        13:32:00                                         \nNo. Observations:                2291                                         \nDf Residuals:                    2281                                         \nDf Model:                           8                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nposition[T.M]          0.1969      0.116      1.690      0.091      -0.031       0.425\nposition[T.S]         -0.6854      0.150     -4.566      0.000      -0.980      -0.391\nresult[T.L]            0.4830      0.112      4.314      0.000       0.264       0.702\nresult[T.W]           -0.7394      0.121     -6.096      0.000      -0.977      -0.502\ncountry[T.Germany]     0.1329      0.094      1.420      0.156      -0.051       0.316\nn_yellow_25            0.3224      0.033      9.745      0.000       0.258       0.387\nn_red_25               0.3832      0.041      9.461      0.000       0.304       0.463\nlevel                  0.0910      0.094      0.973      0.331      -0.092       0.274\nNone/Yellow            2.5995      0.232     11.187      0.000       2.144       3.055\nYellow/Red             0.3487      0.044      7.905      0.000       0.262       0.435\n======================================================================================",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#learning-exercises",
    "href": "ordinal_regression.html#learning-exercises",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "7.5 Learning exercises",
    "text": "7.5 Learning exercises\n\n7.5.1 Discussion questions\n\nDescribe what is meant by an ordinal variable.\nDescribe how an ordinal variable can be represented using a latent continuous variable.\nDescribe the series of binomial logistic regression models that are components of a proportional odds regression model. What can you say about their coefficients?\nIf \\(y\\) is an ordinal outcome variable with at least three levels, and if \\(x\\) is an input variable that has coefficient \\(\\beta\\) in a proportional odds logistic regression model, describe how to interpret the odds ratio \\(e^{\\beta}\\).\nDescribe some approaches for assessing the fit and goodness-of-fit of an ordinal logistic regression model.\nDescribe how you would use stratified binomial logistic regression models to validate the key assumption for a proportional odds model.\nDescribe a statistical significance test that can support or reject the hypothesis that the proportional odds assumption holds.\nDescribe some possible options for situations where the proportional odds assumption is violated.\n\n\n\n7.5.2 Data exercises\nLoad the managers data set via the peopleanalyticsdata package or download it from the internet4. It is a set of information of 571 managers in a sales organization and consists of the following fields:\n\nemployee_id for each manager\nperformance_group of each manager in a recent performance review: Bottom performer, Middle performer, Top performer\nyrs_employed: total length of time employed in years\nmanager_hire: whether or not the individual was hired directly to be a manager (Y) or promoted to manager (N)\ntest_score: score on a test given to all managers\ngroup_size: the number of employees in the group they are responsible for\nconcern_flag: whether or not the individual has been the subject of a complaint by a member of their group\nmobile_flag: whether or not the individual works mobile (Y) or in the office (N)\ncustomers: the number of customer accounts the manager is responsible for\nhigh_hours_flag: whether or not the manager has entered unusually high hours into their timesheet in the past year\ntransfers: the number of transfer requests coming from the manager’s group while they have been a manager\nreduced_schedule: whether the manager works part time (Y) or full time (N)\ncity: the current office of the manager.\n\nConstruct a model to determine how the data provided may help explain the performance_group of a manager by following these steps:\n\nConvert the outcome variable to an ordered factor of increasing performance.\nConvert input variables to categorical factors as appropriate.\nPerform any exploratory data analysis that you wish to do.\nRun a proportional odds logistic regression model against all relevant input variables.\nConstruct p-values for the coefficients and consider how to simplify the model to remove variables that do not impact the outcome.\nCalculate the odds ratios for your simplified model and write an interpretation of them.\nEstimate the fit of the simplified model using a variety of metrics and perform tests to determine if the model is a good fit for the data.\nConstruct new outcome variables and use a stratified binomial approach to determine if the proportional odds assumption holds for your simplified model. Are there any input variables for which you may be concerned that the assumption is violated? What would you consider doing in this case?\nUse the Brant-Wald test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\nWrite a full report on your model intended for an audience of people with limited knowledge of statistics.\n\n\n\n\n\nAgresti, Alan. 2010. Analysis of Ordinal Categorical Data.\n\n\nFagerland, Morten W., and David W. Hosmer. 2017. “How to Test for Goodness of Fit in Ordinal Logistic Regression Models.” The Stata Journal.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "ordinal_regression.html#footnotes",
    "href": "ordinal_regression.html#footnotes",
    "title": "7  Proportional Odds Logistic Regression for Ordered Category Outcomes",
    "section": "",
    "text": "This also leads to another term for the assumption—the parallel regression assumption.↩︎\nNote this is not totally necessary, as significance can be estimated from viewing the confidence intervals that are formed from two standard errors either side of the coefficient estimate. However, we show how to calculate p-values here for precision purposes.↩︎\nNote that intercept coefficients are presented a little differently compared to MASS::polr() in R. The base level intercept is similar, but to get higher level intercepts, their coefficients need to be exponentiated and added to the base level intercept.↩︎\nhttps://peopleanalytics-regression-book.org/data/managers.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Proportional Odds Logistic Regression for Ordered Category Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html",
    "href": "poisson_nb.html",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "",
    "text": "8.1 When to use it\nIn the previous chapters, we have explored how to model outcomes that are continuous, binary or multinomial in nature. We now turn our attention to another common type of data in people analytics: count data. Count data consists of non-negative integers (0, 1, 2, 3, …) that represent the number of times an event has occurred within a specific period of time, location, or group. Examples in the workplace are numerous: the number of safety incidents in a factory per month, the number of days an employee was absent in a year, the number of hires made by a recruiting team in a quarter, or the number of customer complaints received by a service agent in a week.\nBecause count data is not continuous and is bounded below at zero, linear regression is not appropriate. The assumptions of normality and constant variance of residuals are typically violated. A linear model could also illogically predict negative counts. While we could model a binary outcome (e.g., ‘had at least one absence’ vs. ‘had zero absences’) using logistic regression, this approach throws away a lot of valuable information about the magnitude of the count.\nTo properly model count outcomes, we turn to other members of the generalized linear model (GLM) family. First we will look at Poisson regression. Poisson regression is specifically designed for count data and, like logistic regression, uses a transformation–—in this case, the natural logarithm—–to establish a linear relationship between the input variables and the (transformed) outcome. However, Poisson regression rests on a very strong assumption that the mean and variance of the count variable are equal. When this assumption is violated, a condition known as overdispersion occurs, and the Poisson model’s estimates can become unreliable. In this case, we will see that related models, namely Quasi-Poisson or negative binomial regression models, provide more flexible alternatives. Mastering these techniques is essential for any analyst who wants to understand the drivers of event-based phenomena in the workplace.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#when-to-use-it",
    "href": "poisson_nb.html#when-to-use-it",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "",
    "text": "8.1.1 Origins and intuition of the Poisson distribution\nThe Poisson distribution is named after the French mathematician Siméon Denis Poisson, who introduced it in 1837 in a work that focused on the probability of wrongful convictions in court cases1. However, the distribution’s most famous early application came sixty years later from the Polish-Russian economist and statistician Ladislaus von Bortkiewicz.\nBortkiewicz analyzed data on the number of Prussian cavalry soldiers who were fatally kicked by their horses each year across 14 different army corps over a 20-year period. He observed that this was a “rare event.” In any given corps in any given year, the number of deaths was very low, but it was not zero. He found that the observed frequency of 0, 1, 2, or more deaths per corps-year closely matched the predictions of the Poisson distribution. The key insight is that the Poisson distribution effectively models the number of times an event occurs in a fixed interval of time or space, given that these events happen with a known constant mean rate (denoted by \\(\\lambda\\)) and are independent of the time since the last event. The shape of the distribution for different mean rates is shown in Figure 8.1. For a low mean, the distribution is highly skewed with most observations at or near zero. As the mean increases, the distribution becomes more symmetric and starts to resemble a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: Poisson distributions showing how the shape changes with different mean event rates \\(\\lambda\\)\n\n\n\nThe core idea of modeling rates is central to Poisson regression. We are not just modeling a count; we are modeling the expected rate at which events occur, and then examining how different factors or characteristics influence that rate.\n\n\n8.1.2 Use cases for count regression\nPoisson, Quasi-Poisson and negative binomial regression can be used when the outcome of interest is a count of events. Here are some example questions that could be approached using these methods:\n\nWhat are the characteristics of employees that are associated with a higher number of days of unplanned absence per year?\nWhat are the characteristics of job postings that are associated with a higher number of applicants in the first 30 days?\nWhat elements in a factory’s environment are related to the number of safety incidents per month?\n\n\n\n8.1.3 Walkthrough example\nYou are an analyst at a large technology firm. The HR leadership team is concerned about employee absenteeism and wants to understand its drivers to better target support and intervention programs. They believe that factors like an employee’s tenure with the company, their performance, and whether or not they are a manager could all play a role.\nYou are provided with a data set of 865 employees for the most recent full year. The absenteeism data set contains the following fields:\n\ndays_absent: The number of unscheduled days of absence for the employee in the last calendar year (our outcome variable).\ntenure: The employee’s tenure at the company in years.\nis_manager: A binary value indicating 1 if the employee is a manager and 0 if not.\nperformance_rating: The employee’s most recent performance score on a scale from 1 to 5.\n\nLet’s download the absenteeism data set and take a quick look at it.\n\n# if needed, download data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/absenteeism.csv\"\nabsenteeism &lt;- read.csv(url)\n\n\nhead(absenteeism)\n\n  days_absent tenure is_manager performance_rating\n1           2     16          0                  2\n2          13      4          0                  5\n3           0      0          0                  3\n4           8     24          0                  5\n5          12      9          0                  3\n6           8      3          0                  4\n\n\nThe data looks as expected, but we should do some type conversion for the is_manager column before we get a summary.\n\n# convert is_manager to factor\nabsenteeism$is_manager &lt;- as.factor(absenteeism$is_manager)\n\nsummary(absenteeism)\n\n  days_absent        tenure      is_manager performance_rating\n Min.   : 0.00   Min.   : 0.00   0:753      Min.   :1.000     \n 1st Qu.: 3.00   1st Qu.: 8.00   1:112      1st Qu.:3.000     \n Median : 7.00   Median :15.00              Median :3.000     \n Mean   :10.26   Mean   :15.16              Mean   :3.473     \n 3rd Qu.:14.00   3rd Qu.:23.00              3rd Qu.:4.000     \n Max.   :62.00   Max.   :30.00              Max.   :5.000     \n\n\nWe see that days_absent ranges from 0 to 62, with a mean of 10.26. The distribution of absences is shown in Figure 8.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Distribution of days_absent with histogram and density plot\n\n\n\nThis is a highly right-skewed distribution, with most employees having a low number of absences and a “long tail” of employees with a high number of absences. This is a classic shape for count data, and suggests that Poisson regression is a good initial approach for this problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#sec-mod-poisson",
    "href": "poisson_nb.html#sec-mod-poisson",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "8.2 Modeling count outcomes with Poisson regression",
    "text": "8.2 Modeling count outcomes with Poisson regression\nWe cannot directly model a count outcome with a linear equation because the expected count must be non-negative. As with logistic regression, we will need a transformation to connect the linear combination of our input variables to the expected count.\n\n8.2.1 The Poisson model and the log transformation\nThe formula for the Poisson distribution gives the probability of observing exactly \\(k\\) events in an interval, given a mean event rate of \\(\\lambda\\):\n\\[\nP(Y=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\nWe assume that the expected event count \\(\\lambda_i\\) for each individual observation \\(i\\) depends on that observation’s input variables \\(x_{1i}, x_{2i}, \\dots, x_{pi}\\). To ensure that our predicted \\(\\lambda_i\\) is always positive, we model the natural logarithm of \\(\\lambda_i\\) as a linear function of the input variables. This is the log transformation.\n\\[\n\\ln(\\lambda_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_px_{ip}\n\\]\nThis equation should look familiar. It’s a linear model, just like in linear regression. The crucial difference is that we are not modeling the expected count outcome directly; we are modeling the log of the expected count.\nBy exponentiating both sides, we can see how the input variables relate to the expected count itself:\n\\[\n\\begin{aligned}\n\\lambda_i &= e^{\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_px_{ip}} \\\\\n&= e^{\\beta_0}(e^{\\beta_1})^{x_{i1}}(e^{\\beta_2})^{x_{i2}}\\cdots(e^{\\beta_p})^{x_{ip}}\n\\end{aligned}\n\\]\nThis shows that the effects of the input variables are multiplicative on the expected count \\(\\lambda_i\\). This is analogous to work we have done in previous chapters, where exponentiated logistic regression coefficients have a multiplicative effect on the odds of an outcome.\n\n\n8.2.2 Fitting and interpreting a Poisson regression model\nLet’s fit a Poisson model to our absenteeism data to understand the drivers of absent days. We use the glm() function, specifying family = \"poisson\" which automatically uses the log transformation.\n\n# run a poisson model\nabsent_poisson_model &lt;- glm(formula = days_absent ~ tenure + is_manager + performance_rating,\n                            data = absenteeism,\n                            family = \"poisson\")\n\n# view the summary\nsummary(absent_poisson_model)\n\n\nCall:\nglm(formula = days_absent ~ tenure + is_manager + performance_rating, \n    family = \"poisson\", data = absenteeism)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.505418   0.054170  27.791  &lt; 2e-16 ***\ntenure              0.051144   0.001270  40.280  &lt; 2e-16 ***\nis_manager1        -0.157984   0.033025  -4.784 1.72e-06 ***\nperformance_rating -0.009937   0.013878  -0.716    0.474    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7544.2  on 864  degrees of freedom\nResidual deviance: 5799.6  on 861  degrees of freedom\nAIC: 8927\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe summary output is similar to what we’ve seen before. We have coefficient estimates, standard errors, z-values, and p-values. Let’s interpret the tenure coefficient. For a one-year increase in tenure, the log of the expected number of absent days increases by 0.0511, holding all other variables constant. A much more intuitive interpretation comes from exponentiating the coefficients. The exponentiated value is called an Incidence Rate Ratio (IRR). The IRR tells us the multiplicative factor by which the expected count changes for a one-unit increase in the predictor.\nLet’s calculate the IRRs for our model.\n\n# Exponentiate coefficients to get Incidence Rate Ratios (IRRs)\n(irr &lt;- exp(coef(absent_poisson_model)))\n\n       (Intercept)             tenure        is_manager1 performance_rating \n         4.5060347          1.0524742          0.8538633          0.9901123 \n\n\nNow we can interpret these more easily:\n\nIntercept: 4.51 is the expected number of absent days for a brand new non-manager with a performance rating of 1. This is not practically meaningful, but serves as the baseline for the model.\ntenure: The IRR is 1.0525. For each additional year of tenure, the expected count of absent days increases by about 5.25%, holding other factors constant.\nis_manager1: The IRR is 0.8539. This means that being a manager (compared to not being a manager) is associated with an expected absent day count that is 0.8539 times the count for a non-manager, or about 14.61% lower, holding other factors constant.\nperformance_rating: The IRR is 0.9901. For each one-point increase in performance score, the expected number of absent days decreases by about 0.99%, holding other factors constant. However, this effect is not statistically significant.\n\nJust as with logistic regression, we can calculate confidence intervals for these IRRs to understand the range of plausible values for the effect.\n\n# Confidence intervals for the IRRs\nexp(confint(absent_poisson_model))\n\n                       2.5 %    97.5 %\n(Intercept)        4.0513303 5.0097586\ntenure             1.0498634 1.0551020\nis_manager1        0.7999133 0.9104788\nperformance_rating 0.9635426 1.0174134\n\n\nFor example, we are 95% confident that for each additional year of tenure, the incidence rate ratio is between 1.05 and 1.055. Since this entire range is above 1, we can be confident that the relationship is positive and statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#sec-overdispersion",
    "href": "poisson_nb.html#sec-overdispersion",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "8.3 Model diagnostics: overdispersion",
    "text": "8.3 Model diagnostics: overdispersion\nBefore we can trust our Poisson model’s conclusions, we must check its primary assumption: that the mean (expected value) and variance of the count outcome are approximately equal. That is, \\(E(y) \\approx Var(y)\\). In real-world data, this assumption is rarely met. It is far more common for the variance to be larger than the mean. This is called overdispersion (\\(Var(y) &gt; E(y)\\))2.\nOverdispersion is common for a number of reasons, including:\n\nUnobserved Heterogeneity: There may be other unmeasured factors that influence the count. For instance, in our example some employees might have chronic health conditions that make them more prone to absence. This creates extra variability in the counts that is not captured by our input variables.\nClustering or Non-independence: Events may not be independent. If one person in a team gets a virus or flu, it might spread, leading to a cluster of absences. This violates the Poisson assumption that events are independent.\n\nIf we use a Poisson model on overdispersed data, the coefficient estimates (\\(\\beta\\)) are still unbiased (correct on average), but their standard errors are underestimated. This makes the z-scores artificially large and the p-values artificially small. We might therefore conclude that a predictor is statistically significant when, in fact, it is not. Therefore, the model can become overconfident in its findings.\n\n8.3.1 Detecting overdispersion\nThere are several ways to check for overdispersion.\n\nCompare Mean and Variance: A simple ‘rule of thumb’ check is to compare the raw mean and variance of our outcome variable.\n\nc(mean = mean(absenteeism$days_absent), \n  variance = var(absenteeism$days_absent))\n\n     mean  variance \n 10.26243 103.25397 \n\n\nThe variance (103.25) is much larger than the mean (10.26), which is a strong initial sign of overdispersion.\nCalculate the Dispersion Parameter: A more formal check comes from the model itself. The dispersion parameter is calculated by dividing the model’s residual deviance by its residual degrees of freedom. For a Poisson model, this value should be close to 1. A value significantly greater than 1 indicates overdispersion.\n\n# Calculate dispersion parameter\n(dispersion_param &lt;- summary(absent_poisson_model)$deviance / summary(absent_poisson_model)$df.residual)\n\n[1] 6.735929\n\n\nOur dispersion parameter of 6.74 is substantially greater than 1, confirming that our data is overdispersed and the Poisson model is likely not appropriate.\nFormal Hypothesis Test: The AER package provides a formal test for overdispersion. The null hypothesis is that the data is equidispersed (mean = variance), while the alternative is that it is overdispersed. A small p-value provides evidence for overdispersion.\n\nlibrary(AER)\ndispersiontest(absent_poisson_model)\n\n\n    Overdispersion test\n\ndata:  absent_poisson_model\nz = 13.31, p-value &lt; 2.2e-16\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  6.926057 \n\n\nThe very small p-value leads us to reject the null hypothesis and conclude that overdispersion is present. This suggests that we should search for an alternative approach that can handle overdispersed count data.\n\nWhen we have confirmed that our count data is overdispersed, there are two alternative methods we can use: Quasi-Poisson regression and negative binomial regression.\n\n\n8.3.2 Quasi-Poisson regression\nQuasi-Poisson regression, which is a simple extension of the Poisson model, relaxes the assumption that the mean and variance are equal, allowing the variance to be a linear function of the mean. That is\n\\[\nVar(y_i) = \\theta \\mu_i\n\\]\nwhere \\(\\theta\\) is a dispersion parameter estimated from the data and \\(\\mu_i\\) is the average event count (equivalent to \\(\\lambda_i\\) in the Poisson model). We expect \\(\\theta &gt; 1\\) in the presence of overdispersion, and a greater \\(\\theta\\) means greater dispersion. The coefficient estimates in a Quasi-Poisson model are identical to the Poisson model, but the standard errors are adjusted, which affects which input variables are returned as significant.\nQuasi-Poisson regression can be performed in R using the glm() function with family = \"quasipoisson\".\n\n# run a quasi-poisson model\nabsent_quasi_poisson_model &lt;- glm(formula = days_absent ~ tenure + is_manager + performance_rating,\n                                  data = absenteeism,\n                                  family = \"quasipoisson\")\n\n# view the summary\nsummary(absent_quasi_poisson_model)\n\n\nCall:\nglm(formula = days_absent ~ tenure + is_manager + performance_rating, \n    family = \"quasipoisson\", data = absenteeism)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.505418   0.142929  10.533   &lt;2e-16 ***\ntenure              0.051144   0.003350  15.266   &lt;2e-16 ***\nis_manager1        -0.157984   0.087137  -1.813   0.0702 .  \nperformance_rating -0.009937   0.036618  -0.271   0.7862    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 6.96191)\n\n    Null deviance: 7544.2  on 864  degrees of freedom\nResidual deviance: 5799.6  on 861  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThis model suggests that the is_manager variable is no longer statistically significant once we account for overdispersion. The coefficient estimates are the same as in the Poisson model, so the IRRs are unchanged.\n\n\n8.3.3 Negative binomial regression\nAnother option in the presence of overdispersion is to use negative binomial regression. The negative binomial distribution is a generalization of the Poisson distribution that includes an additional dispersion parameter which allows the variance to be greater than the mean.\nThe variance for a negative binomial distribution is modeled as:\n\\[\nVar(y_i) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n\\]\nwhere \\(\\theta\\) is a dispersion parameter, so the term \\(\\frac{\\mu_i^2}{\\theta}\\) allows for the extra variance. Note that in this model the variance is a quadratic function of the mean, rather than the linear function used in the Quasi-Poisson model. Note also that a greater \\(\\theta\\) means less dispersion, and as \\(\\theta\\) gets larger and approaches infinity, the term \\(\\frac{\\mu_i^2}{\\theta}\\) approaches zero, and the variance approaches the mean \\(\\mu_i\\). In this case, the negative binomial distribution converges to the Poisson distribution.\nWe can fit a negative binomial model in R using the glm.nb() function from the MASS package. The syntax is identical to glm().\n\nlibrary(MASS)\n\n# run a negative binomial model\nabsent_nb_model &lt;- glm.nb(formula = days_absent ~ tenure + is_manager + performance_rating,\n                          data = absenteeism)\n\n# view the summary\nsummary(absent_nb_model)\n\n\nCall:\nglm.nb(formula = days_absent ~ tenure + is_manager + performance_rating, \n    data = absenteeism, init.theta = 1.553571254, link = log)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.571059   0.147302  10.666   &lt;2e-16 ***\ntenure              0.049723   0.003338  14.894   &lt;2e-16 ***\nis_manager1        -0.138416   0.088722  -1.560    0.119    \nperformance_rating -0.022527   0.039176  -0.575    0.565    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.5536) family taken to be 1)\n\n    Null deviance: 1208.49  on 864  degrees of freedom\nResidual deviance:  978.86  on 861  degrees of freedom\nAIC: 5637.1\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.5536 \n          Std. Err.:  0.0894 \n\n 2 x log-likelihood:  -5627.0750 \n\n\nThe output looks very similar to the Poisson and Quasi-Poisson summaries, but with one key addition: at the bottom, we see “Theta: 1.5536”. This is the estimated dispersion parameter \\(\\theta\\). The summary() also provides a standard error for \\(\\theta\\) and tests whether it is significantly different from infinity (i.e., whether the negative binomial model is a significant improvement over the Poisson model).\nThe interpretation of the coefficients is exactly the same as in the Poisson model. They are on the log-count scale, and we exponentiate them to get Incidence Rate Ratios (IRRs).\n\n# IRRs for the negative binomial model\n(irr_nb &lt;- exp(coef(absent_nb_model)))\n\n       (Intercept)             tenure        is_manager1 performance_rating \n         4.8117411          1.0509798          0.8707365          0.9777244 \n\n\nLet’s compare these with the IRRs from our original Poisson model.\n\n# Compare IRRs side-by-side\ncbind(IRR_Poisson = irr, IRR_NB = irr_nb)\n\n                   IRR_Poisson    IRR_NB\n(Intercept)          4.5060347 4.8117411\ntenure               1.0524742 1.0509798\nis_manager1          0.8538633 0.8707365\nperformance_rating   0.9901123 0.9777244\n\n\nThe point estimates for the IRRs are quite similar. This is expected, as the coefficients are generally unbiased even when a Poisson model is used on overdispersed data. The real difference is in the standard errors and p-values.\n\n# Compare standard errors\ncbind(SE_Poisson = summary(absent_poisson_model)$coefficients[, \"Std. Error\"],\n      SE_NB = summary(absent_nb_model)$coefficients[, \"Std. Error\"])\n\n                    SE_Poisson       SE_NB\n(Intercept)        0.054169623 0.147301916\ntenure             0.001269716 0.003338447\nis_manager1        0.033024551 0.088721754\nperformance_rating 0.013878275 0.039175545\n\n\nNotice that the standard errors for the negative binomial model are larger across the board. By accounting for the overdispersion, the negative binomial model provides more realistic (and conservative) estimates of the uncertainty around our coefficients. This leads to larger p-values and more trustworthy inferences. In this case, while tenure remains a significant input variable, we see that is_manager is no longer significant.\n\n\n8.3.4 Comparing Poisson and negative binomial models\nWe have strong evidence that the negative binomial model is a better choice compared to the original Poisson model, but we can also formally compare the models. Since the Poisson model is a nested version of the negative binomial model (it’s what you get when \\(\\theta \\to \\infty\\)), we can use a likelihood ratio test using the lmtest package to see if the addition of the \\(\\theta\\) parameter provides a significant improvement in fit. This is a notable advantage of the negative binomial model over the Quasi-Poisson model, which does not have a likelihood function and therefore cannot be compared in this way.\n\nlibrary(lmtest)\nlrtest(absent_poisson_model, absent_nb_model)\n\nLikelihood ratio test\n\nModel 1: days_absent ~ tenure + is_manager + performance_rating\nModel 2: days_absent ~ tenure + is_manager + performance_rating\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   4 -4459.5                        \n2   5 -2813.5  1  3292  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe extremely small p-value indicates that the negative binomial model provides a significantly better fit to the data than the Poisson model.\nWe can also compare the models using the Akaike Information Criterion (AIC), which as we have learned previously is a measure of model parsimony. Since both of our models use the same input variables, a lower AIC indicates a better fit.\n\nAIC(absent_poisson_model, absent_nb_model)\n\n                     df      AIC\nabsent_poisson_model  4 8927.050\nabsent_nb_model       5 5637.075\n\n\nThe AIC for the negative binomial model is substantially lower than for the Poisson model, again confirming that it is the superior and more parsimonious model for our overdispersed data.\nNote that it is not advisable to use Pseudo R-squared metrics to compare Poisson and negative binomial models, as these metrics can be misleading in the presence of overdispersion. These metrics are more appropriate for comparing models of the same family (e.g., two Poisson models with different input variables).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#other-considerations-in-count-regression",
    "href": "poisson_nb.html#other-considerations-in-count-regression",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "8.4 Other considerations in count regression",
    "text": "8.4 Other considerations in count regression\nIn this section we provide a brief overview of some additional topics that are relevant when working with count data. For deeper study on this topic, Hilbe (2014) is an excellent introductory text, while Friendly and Meyer (2016) provides a more advanced treatment on the analysis of discrete data more broadly.\n\n8.4.1 Prediction\nPredicting expected counts from a new set of data works just like in other glm models. We use the predict() function with type = \"response\" to get the prediction on the original count scale (i.e., the expected number of events, \\(\\lambda_i\\)).\n\n# define new observations\n(new_employees &lt;- data.frame(tenure = c(2, 25, 15),\n                             is_manager = as.factor(c(0, 1, 0)),\n                             performance_rating = c(4, 3, 3)))\n\n  tenure is_manager performance_rating\n1      2          0                  4\n2     25          1                  3\n3     15          0                  3\n\n# predict expected days absent using our final NB model\npredict(absent_nb_model, new_employees, type = \"response\")\n\n        1         2         3 \n 4.856874 13.573681  9.481285 \n\n\n\n\n8.4.2 Exposure and offsets\nSometimes we are not just modeling a raw count, but a rate relative to some measure of “exposure”. For example, we might want to model the number of safety incidents, but we know that a large factory with 1,000 employees is naturally going to have more incidents than a small one with 100 employees, even if the small factory is less safe. We need to control for this exposure.\nIn this case, our outcome of interest is really the rate of incidents per employee. Let’s say we model\n\\[\nE[\\mathrm{incidents}_i] = \\mathrm{rate}_i \\times \\mathrm{employees}_i\n\\]\nTaking logs, we get\n\\[\n\\ln(E[\\mathrm{incidents}_i]) = \\ln(\\mathrm{rate}_i) + \\ln(\\mathrm{employees}_i)\n\\]\nWe model the log-rate as a linear combination of input variables:\n\\[\n\\ln(\\mathrm{rate}_i) = \\beta_0 + \\beta_1x_{1i} + \\cdots\n\\]\nSubstituting this in gives:\n\\[\n\\ln(E[\\mathrm{incidents}_i]) = (\\beta_0 + \\beta_1x_{1i} + \\cdots) + \\ln(\\mathrm{employees}_i)\n\\]\nThe term \\(\\ln(\\mathrm{employees}_i)\\) is called an offset. It is an input variable whose coefficient is fixed to 1. We include it in the model to account for the varying exposure time or group size. In R’s glm() and glm.nb(), this is specified using the offset() function inside the formula. For example\nglm(incidents ~ X1 + X2 + offset(log(employees)), family=poisson)\n\n\n8.4.3 Zero-inflation\nSometimes count data has an excess of zeros beyond what would be expected from a Poisson or negative binomial distribution. This can occur when there are two processes at work: one determining whether the event can occur at all, and another determining how many times it occurs. For example, some employees might never be absent due to perfect health and proximity to work (structural zeros), while others have some probability of absence following a count distribution.\nIf you believe there may be structural reasons for an excess of zeros in your data, you can use the performance package in R to check for zero-inflation in a Poisson or negative binomial model. First we will check for our Poisson model.\n\n# check for zero-inflation in Poisson model\nlibrary(performance)\ncheck_zeroinflation(absent_poisson_model)\n\n# Check for zero-inflation\n\n   Observed zeros: 56\n  Predicted zeros: 2\n            Ratio: 0.04\n\n\nThe output indicates that our Poisson model may have too many zeros compared to what would normally be expected in a Poisson distribution. In this situation, we could consider using a zero-inflated model, which combines a binary model for the zero/non-zero outcome with a count model for the non-zero outcomes. The pscl package in R offers a function zeroinfl() to fit these models. The formula syntax is similar to glm(), but we can specify separate input variables for the zero-inflation part of the model.\nHere we fit a zero-inflated Poisson model to our absenteeism data.\n\nlibrary(pscl)\n\n# fit zero-inflated Poisson model using all variables for both the zero-inflated and poisson parts\n\nzip_model &lt;- zeroinfl(\n  # poisson input variables first, binomial second\n  days_absent ~ tenure + is_manager + performance_rating | tenure + is_manager + performance_rating,\n  data = absenteeism,\n  dist = \"poisson\"\n)\n\nsummary(zip_model)\n\n\nCall:\nzeroinfl(formula = days_absent ~ tenure + is_manager + performance_rating | \n    tenure + is_manager + performance_rating, data = absenteeism, dist = \"poisson\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-3.6724 -1.5248 -0.4967  0.9931  9.6365 \n\nCount model coefficients (poisson with log link):\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.625565   0.054553  29.798  &lt; 2e-16 ***\ntenure              0.048091   0.001281  37.542  &lt; 2e-16 ***\nis_manager1        -0.140534   0.033089  -4.247 2.16e-05 ***\nperformance_rating -0.012928   0.013904  -0.930    0.352    \n\nZero-inflation model coefficients (binomial with logit link):\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.39762    0.68731  -2.033 0.042007 *  \ntenure             -0.05497    0.01667  -3.298 0.000974 ***\nis_manager1         0.19858    0.40512   0.490 0.624014    \nperformance_rating -0.17071    0.18989  -0.899 0.368647    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -4179 on 8 Df\n\n\nThe first set of coefficients are the regression coefficients for a Poisson model fitted to the non-zero outcome values, confirming as per our previous Poisson model that higher tenure is significantly positively associated with increased absence, and that being a manager is significantly negatively associated. The second set of coefficients are from a binomial logistic regression model on an outcome of whether an observation is a structural zero. Here we see that higher tenure is significantly negatively associated with being a structural zero, meaning that employees with longer tenure are less likely to be in the always-zero group.\nHowever, we know that overdispersion is present in our data, which could be a better explanation for the excess zeros, especially if we have no basis to support a structural reason. So let’s check whether there is zero-inflation in our negative binomial model.\n\n# check for zero-inflation in negative binomial model\ncheck_zeroinflation(absent_nb_model)\n\n# Check for zero-inflation\n\n   Observed zeros: 56\n  Predicted zeros: 49\n            Ratio: 0.88\n\n\nIt seems unlikely based on this check that zero-inflation is an issue in the negative binomial model, so the excess zeros could well be a consequence of overdispersion. Nevertheless, for completeness, we can fit a zero-inflated negative binomial model as well.\n\n# fit zero-inflated negative binomial model\nzinb_model &lt;- zeroinfl(\n  # negative binomial input variables first, binomial second\n  days_absent ~ tenure + is_manager + performance_rating | tenure + is_manager + performance_rating,\n  data = absenteeism,\n  dist = \"negbin\"\n)\n\nsummary(zinb_model)\n\n\nCall:\nzeroinfl(formula = days_absent ~ tenure + is_manager + performance_rating | \n    tenure + is_manager + performance_rating, data = absenteeism, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2341 -0.7063 -0.2438  0.4825  4.6896 \n\nCount model coefficients (negbin with log link):\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.638010   0.155648  10.524  &lt; 2e-16 ***\ntenure              0.049110   0.003451  14.231  &lt; 2e-16 ***\nis_manager1        -0.123632   0.090017  -1.373    0.170    \nperformance_rating -0.035067   0.040599  -0.864    0.388    \nLog(theta)          0.511736   0.080944   6.322 2.58e-10 ***\n\nZero-inflation model coefficients (binomial with logit link):\n                   Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)        -0.94964    3.24416  -0.293    0.770\ntenure             -0.04573    0.09641  -0.474    0.635\nis_manager1         0.76056    1.25077   0.608    0.543\nperformance_rating -0.82878    0.90994  -0.911    0.362\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 1.6682 \nNumber of iterations in BFGS optimization: 20 \nLog-likelihood: -2812 on 9 Df\n\n\nThe more conservative negative binomial model suggests that there are no variables significantly associated with being a member of the structural zero group, and confirms the findings from our previous negative binomial model that only tenure is significantly positively associated with increased absence. This suggests that zero-inflation is not a concern in this case.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#poisson-and-negative-binomial-regression-using-python",
    "href": "poisson_nb.html#poisson-and-negative-binomial-regression-using-python",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "8.5 Poisson and negative binomial regression using Python",
    "text": "8.5 Poisson and negative binomial regression using Python\nIn Python, count models can be fit using the statsmodels library, very similar to the linear and logistic regression models we have already seen. First let’s get our data set.\n\nimport pandas as pd\n\nurl = \"https://peopleanalytics-regression-book.org/data/absenteeism.csv\"\nabsenteeism = pd.read_csv(url)\n\nNow, we can fit our Poisson model.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit Poisson Model\npoisson_model = smf.glm(\n    formula=\"days_absent ~ tenure + is_manager + performance_rating\",\n    data=absenteeism,\n    family=sm.families.Poisson()\n).fit()\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            days_absent   No. Observations:                  865\nModel:                            GLM   Df Residuals:                      861\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4459.5\nDate:                Mon, 22 Dec 2025   Deviance:                       5799.6\nTime:                        13:32:17   Pearson chi2:                 5.99e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.8669\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              1.5054      0.054     27.791      0.000       1.399       1.612\ntenure                 0.0511      0.001     40.280      0.000       0.049       0.054\nis_manager            -0.1580      0.033     -4.784      0.000      -0.223      -0.093\nperformance_rating    -0.0099      0.014     -0.716      0.474      -0.037       0.017\n======================================================================================\n\n\nWe exponentiate the coefficients to get Incidence Rate Ratios (IRRs):\n\nimport numpy as np\nprint(np.exp(poisson_model.params))\n\nIntercept             4.506035\ntenure                1.052474\nis_manager            0.853863\nperformance_rating    0.990112\ndtype: float64\n\n\nWe can also fit our negative binomial model.\n\n# Fit negative binomial Model\nnb_model = smf.glm(\n    formula=\"days_absent ~ tenure + is_manager + performance_rating\",\n    data=absenteeism,\n    family=sm.families.NegativeBinomial()\n).fit()\n\nprint(nb_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            days_absent   No. Observations:                  865\nModel:                            GLM   Df Residuals:                      861\nModel Family:        NegativeBinomial   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -2842.3\nDate:                Mon, 22 Dec 2025   Deviance:                       702.79\nTime:                        13:32:17   Pearson chi2:                     548.\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.1645\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept              1.5732      0.178      8.814      0.000       1.223       1.923\ntenure                 0.0496      0.004     12.290      0.000       0.042       0.058\nis_manager            -0.1373      0.107     -1.278      0.201      -0.348       0.073\nperformance_rating    -0.0228      0.048     -0.479      0.632      -0.116       0.070\n======================================================================================\n\n\nIf needed, zero-inflated models can be fit using the statsmodels library’s ZeroInflatedPoisson and ZeroInflatedNegativeBinomialP classes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#learning-exercises",
    "href": "poisson_nb.html#learning-exercises",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "8.6 Learning exercises",
    "text": "8.6 Learning exercises\n\n8.6.1 Discussion questions\n\nWhat are the key characteristics of count data? Why is linear regression not a suitable method for modeling it?\nWhat are the main assumptions of the Poisson distribution? Which one is most frequently violated in practice?\nIn Poisson regression, we model the log of the expected count as a linear function of the input variables. Why do we use the log transformation? What does this imply about the effect of input variables on the expected count itself?\nExplain the concept of an Incidence Rate Ratio (IRR). If a predictor has an IRR of 1.15, how would you interpret its effect? What about an IRR of 0.85?\nWhat is overdispersion? Describe two common reasons why it might occur in people-related data.\nWhat are the consequences of ignoring overdispersion and using a Poisson model regardless?\nDescribe three methods you could use to test for overdispersion in your data.\nHow does the negative binomial distribution differ from the Poisson distribution? Explain the role of the dispersion parameter \\(\\theta\\).\nWhen would you use an offset in a count model? Provide a people-related example.\nWhat are zero-inflated models and when might they be needed?\n\n\n\n8.6.2 Data exercises\nA retail company wants to understand factors affecting the number of customer complaints received about their telephone customer service representatives. Load the complaints data set via the peopleanalyticsdata package or download it from the internet3. It contains the following data for 376 representatives:\n\nn_complaints: Number of complaints received about the representative in the past year\nexperience: Years of experience in customer service\ntraining_hours: Hours of training received in the past year\n\nworkload: Average number of customer interactions per day\nshift: The primary shift worked (Day, Evening, or Night)\nremote: Whether the representative works remotely (1 = yes, 0 = no)\nsatisfaction_score: The representative’s job satisfaction score (1-10 scale of increasing satisfaction)\n\n\nLoad the data and obtain statistical summaries. Check data types and create an appropriate visualization to understand the distribution of n_complaints. How would you describe the distribution of n_complaints?\nCheck whether the mean and variance of n_complaints are similar. What does this tell you about potential overdispersion?\nFit a Poisson regression model using all input variables. Which variables have significant effects on the number of complaints?\nCalculate and interpret the incidence rate ratios for the significant input variables.\nPerform diagnostic checks for overdispersion in your Poisson model.\nFit a Quasi-Poisson regression model. How do the standard errors and significance of input variables change compared to the Poisson model?\nFit a negative binomial regression model with the same input variables. How do the results compare to the Poisson model?\nCompare the Poisson and negative binomial models using AIC and likelihood ratio tests. Which model is preferred and why?\nDetermine the expected number of complaints for three hypothetical customer service representatives with different characteristics. Compare the predictions from both models.\nWrite a brief report explaining your findings and recommendations for reducing customer complaints.\n\n\n\n\n\nFriendly, Michael, and David Meyer. 2016. Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "poisson_nb.html#footnotes",
    "href": "poisson_nb.html#footnotes",
    "title": "8  Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes",
    "section": "",
    "text": "In fact, the distribution had already been discovered by the famous French mathematician Abraham De Moivre in the early 18th century.↩︎\nIn more rare circumstances, the variance may be smaller than the mean, which is called underdispersion (\\(Var(y) &lt; E(y)\\)). Similar methods to those described in this chapter can also be used to treat underdispersed data.↩︎\nhttps://peopleanalytics-regression-book.org/data/complaints.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "hierarchical_data.html",
    "href": "hierarchical_data.html",
    "title": "9  Modeling Explicit and Latent Hierarchy in Data",
    "section": "",
    "text": "9.1 Mixed models for explicit hierarchy in data\nSo far in this book we have learned all of the most widely used and foundational regression techniques for inferential modeling. Starting with this chapter, we will look at situations where we need to adapt or combine techniques to address certain inference goals or data characteristics. In this chapter we look at some situations where data has a hierarchy and where we wish to consider this hierarchy in our modeling efforts.\nIt is very often the case that data has an explicit hierarchy. For example, each observation in our data may refer to a different individual and each such individual may be a member of a few different groups. Similarly, each observation might refer to an event involving an individual, and we may have data on multiple events for the same individual. For a particular problem that we are modeling, we may wish to take into consideration the effect of the hierarchical grouping. This requires a model which has a mixture of random effects and fixed effects—called a mixed model.\nSeparately, it can be the case that data we are given could have a latent hierarchy. The input variables in the data might be measures of a smaller set of higher-level latent constructs, and we may have a more interpretable model if we hypothesize, confirm and model those latent constructs against our outcome of interest rather than using a larger number of explicit input variables. Latent variable modeling is a common technique to address this situation, and in this chapter we will review a form of latent variable modeling called structural equation modeling, which is very effective especially in making inferences from survey instruments with large numbers of items.\nThese topics are quite broad, and there are many different approaches, techniques and terms involved in mixed modeling and latent variable modeling. In this chapter we will only cover some of the simpler approaches, which would suffice for the majority of common situations in people analytics. For a deeper treatment of these topics, see Jiang (2007) for mixed models and Bartholomew, Knott, and Moustaki (2011) or Skrondal and Rabe-Hesketh (2004) for latent variable models.\nThe most common explicit hierarchies that we see in data are group-based and time-based. A group-based hierarchy occurs when we are taking observations that belong to different groups. For example, in our first walkthrough example in Chapter 4, we modeled final examination performance against examination performance for the previous three years. In this case we considered each student observation to be independent and identically distributed, and we ran a linear regression model on all the students. If we were to receive additional information that these students were actually a mix of students in different degree programs, then we may wish to take this into account in how we model the problem—that is, we would want to assume that each student observation is only independent and identically distributed within each degree program.\nSimilarly, a time-based hierarchy occurs when we have multiple observations of the same subject taken at different times. For example, if we are conducting a weekly survey on the same people over the course of a year, and we are modeling how answers to some questions might depend on answers to others, we may wish to consider the effect of the person on this model.\nBoth of these situations introduce a new grouping variable into the problem we are modeling, thus creating a hierarchy. It is not hard to imagine that analyzing each group may produce different statistical properties compared to analyzing the entire population—for example, there could be correlations between the data inside groups which are less evident when looking at the overall population. Therefore in some cases a model may provide more useful inferences if this grouping is taken into account.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling Explicit and Latent Hierarchy in Data</span>"
    ]
  },
  {
    "objectID": "hierarchical_data.html#sec-mixed",
    "href": "hierarchical_data.html#sec-mixed",
    "title": "9  Modeling Explicit and Latent Hierarchy in Data",
    "section": "",
    "text": "9.1.1 Fixed and random effects\nLet’s imagine that we have a set of observations consisting of a continuous outcome variable \\(y\\) and input variables \\(x_1, x_2, \\dots, x_p\\). Let’s also assume that we have an additional data point for each observation where we assign it to a group \\(G\\). We are asked to determine the relationship between the outcome and the input variables. One option is to develop a linear model \\(y = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\\), ignoring the group data. In this model, we assume that the coefficients all have a fixed effect on the input variables—that is, they act on every observation in the same way. This may be fine if there is trust that group membership is unlikely to have any impact on the relationship being modeled, or if we are comfortable making inferences about variables at the observation level only.\nIf, however, there is a belief that group membership may have an effect on the relationship being modeled, and if we are interested in interpreting our model at the group and observation level, then we need to adjust our model to a mixed model for more accurate and reliable inference. The most common adjustment is a random intercept. In this situation, we imagine that group membership has an effect on the ‘starting point’ of the relationship: the intercept. Therefore, for a given observation \\(y = \\alpha_G + \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\\), where \\(\\alpha_G\\) is a random effect with a mean of zero associated with the group that the observation is a member of. This can be restated as:\n\\[\ny = \\beta_G + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\n\\]\nwhere \\(\\beta_G = \\alpha_G + \\beta_0\\), which is a random intercept with a mean of \\(\\beta_0\\).\nThis model is very similar to a standard linear regression model, except instead of having a fixed intercept, we have an intercept that varies by group. Therefore, we will essentially have two ‘levels’ in our model: one at the observation level to describe \\(y\\) and one at the group level to describe \\(\\beta_G\\). For this reason mixed models are sometimes known as multilevel models.\nIt is not too difficult to see how this approach can be extended. For example, suppose that we believe the groups also have an effect on the coefficient of the input variable \\(x_1\\) as well as the intercept. Then\n\\[\ny = \\beta_{G0} + \\beta_{G1}x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\n\\] where \\(\\beta_{G0}\\) is a random intercept with a mean of \\(\\beta_0\\), and \\(\\beta_{G1}\\) is a random slope with a mean of \\(\\beta_1\\). In this case, a mixed model would return the estimated coefficients at the observation level and the statistics for the random effects \\(\\beta_{G0}\\) and \\(\\beta_{G1}\\) at the group level.\nFinally, our model does not need to be linear for this to apply. This approach also extends to logistic models and other generalized linear models. For example, if \\(y\\) was a binary outcome variable and our model was a binomial logistic regression model, our last equation would translate to\n\\[\n\\mathrm{ln}\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_{G0} + \\beta_{G1}x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\n\\]\n\n\n9.1.2 Running a mixed model\nLet’s look at a fun and straightforward example of how mixed models can be useful. The speed_dating data set is a set of information captured during experiments with speed dating by students at Columbia University in New York1. Each row represents one meeting between an individual and a partner of the opposite sex. The data contains the following fields:\n\niid is an id number for the individual.\ngender is the gender of the individual with 0 as Female and 1 as Male.\nmatch indicates that the meeting resulted in a match.\nsamerace indicates that both the individual and the partner were of the same race.\nrace is the race of the individual, with race coded as follows: Black/African American=1, European/Caucasian-American=2, Latino/Hispanic American=3, Asian/Pacific Islander/Asian-American=4, Native American=5, Other=6.\ngoal is the reason why the individual is participating in the event, coded as follows: Seemed like a fun night out=1, To meet new people=2, To get a date=3, Looking for a serious relationship=4, To say I did it=5, Other=6.\ndec is a binary rating from the individual as to whether they would like to see their partner again (1 is Yes and 0 is No).\nattr is the individual’s rating out of 10 on the attractiveness of the partner.\nintel is the individual’s rating out of 10 on the intelligence level of the partner.\nprob is the individual’s rating out of 10 on whether they believe the partner will want to see them again.\nagediff is the absolute difference in the ages of the individual and the partner.\n\nThis data can be explored in numerous ways, but we will focus here on modeling options. We are interested in the binary outcome dec (the decision of the individual), and we would like to understand how it relates to the age difference, the racial similarity and the ratings on attr, intel and prob. First, let’s assume that we don’t care about how an individual makes up their mind about their speed date, and that we are only interested in the dynamics of speed date decisions. Then we would simply run a binomial logistic regression on our data set, ignoring iid and other grouping variables like race, goal and gender.\n\n# if needed, get data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/speed_dating.csv\"\nspeed_dating &lt;- read.csv(url)\n\n\n# run standard binomial model\nmodel &lt;- glm(dec ~ agediff + samerace + attr + intel + prob, \n             data = speed_dating, \n             family = \"binomial\")\n\nsummary(model)\n\n\nCall:\nglm(formula = dec ~ agediff + samerace + attr + intel + prob, \n    family = \"binomial\", data = speed_dating)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.812900   0.184340 -31.534   &lt;2e-16 ***\nagediff     -0.010518   0.009029  -1.165   0.2440    \nsamerace    -0.093422   0.055710  -1.677   0.0936 .  \nattr         0.661139   0.019382  34.111   &lt;2e-16 ***\nintel       -0.004485   0.020763  -0.216   0.8290    \nprob         0.270553   0.014565  18.575   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10647.3  on 7788  degrees of freedom\nResidual deviance:  8082.9  on 7783  degrees of freedom\n  (589 observations deleted due to missingness)\nAIC: 8094.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nIn general, we see that the factors which significantly influence the speed dating decision seem to be the attractiveness of the partner and the feeling of reciprocation of interest from the partner, and that age difference, racial similarity and intelligence do not seem to play a significant role at the level of the speed date itself.\nNow let’s say that we are interested in how a given individual weighs up these factors in coming to a decision. Different individuals may have different ingoing criteria for making speed dating decisions. As a result, each individual may have varying base likelihoods of a positive decision, and each individual may be affected by the input variables in different ways as they come to their decision. Therefore we will need to assign random effects for individuals based on iid. The lme4 package in R contains functions for performing mixed linear regression models and mixed generalized linear regression models. These functions take formulas with additional terms to define the random effects to be estimated. The function for a linear model is lmer() and for a generalized linear model is glmer().\nIn the simple case, let’s assume that each individual has a different ingoing base likelihood of making a positive decision on a speed date. We will therefore model a random intercept according to the iid of the individual. Here we would use the formula dec ~ agediff + samerace + attr + intel + prob + (1 | iid), where (1 | iid) means ‘a random effect for iid on the intercept of the model’.\n\n# run binomial mixed effects model\nlibrary(lme4)\n\niid_intercept_model &lt;- lme4:::glmer(\n  dec ~ agediff + samerace + attr + intel + prob + (1 | iid),\n  data = speed_dating,\n  family = \"binomial\"\n)\n\n\n# view summary without correlation table of fixed effects\nsummary(iid_intercept_model, \n        correlation = FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: dec ~ agediff + samerace + attr + intel + prob + (1 | iid)\n   Data: speed_dating\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   6420.3    6469.0   -3203.1    6406.3      7782 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-25.6966  -0.3644  -0.0606   0.3608  25.0369 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n iid    (Intercept) 5.18     2.276   \nNumber of obs: 7789, groups:  iid, 541\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -12.88882    0.42138 -30.587  &lt; 2e-16 ***\nagediff      -0.03671    0.01401  -2.621  0.00877 ** \nsamerace      0.20187    0.08140   2.480  0.01314 *  \nattr          1.07894    0.03334  32.364  &lt; 2e-16 ***\nintel         0.31592    0.03472   9.098  &lt; 2e-16 ***\nprob          0.61998    0.02873  21.581  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see the two levels of results in this summary. The fixed effects level gives the coefficients of the model at an observation (speed date) level, and the random effects tell us how the intercept (or base likelihood) of that model can vary according to the individual. We see that there is considerable variance in the intercept from individual to individual, and taking this into account, we now see that the decision of an individual on a given date is significantly influenced by all the factors in this model. If we had stuck with the simple binomial model, the effects of age difference, racial similarity and intelligence at an individual level would have gotten lost, and we could have reached the erroneous conclusion that none of these really matter in speed dating.\nTo illustrate this graphically, Figure 9.2 shows the speed_dating data for a subset of the three individuals with IIDs 252, 254 and 256. The curve represents a plain binomial logistic regression model fitted on the attr and prob input variables, irrelevant of the IID of the individual.\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: 3D visualization of the fitted plain binomial model against a subset of the speed_dating data for three specific iids\n\n\nFigure 9.4 shows the three separate curves for each IID generated by a mixed binomial logistic regression model with a random intercept fitted on the same two input variables. Here, we can see that different individuals process the two inputs in their decision making in different ways, leading to different individual formulas which determine the likelihood of a positive decision. While a plain binomial regression model will find the best single formula from the data irrelevant of the individual, a mixed model allows us to take these different individual formulas into account in determining the effects of the input variables.\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: 3D visualization of the individual-level binomial models created by iid_intercept_model against a subset of the speed_dating data for three specific iids\n\n\nIf we believe that different individuals are influenced differently by one or more of the various decision factors they consider during a speed date, we can extend our random effects to the slope coefficients of our model. For example we could use (1 + agediff | iid) to model a random effect of iid on the intercept and the agediff coefficient. Similarly, if we wanted to consider two grouping variables—like iid and goal—on the intercept, we could add both (1 | iid) and (1 | goal) to our model formula.\n\n\n9.1.3 Mixed effects models using Python\nThe pymer4 package provides a simple implementation of linear and generalized linear mixed effects models. The glmer() function is used for generalized linear mixed models, and the specific model is defined by the family parameter. This package is implemented in R under the hood, and also requires Polars rather than Pandas DataFrames. To avoid issues related to the back-end R code, it is recommended that you explicitly remove all rows containing NaN values before fitting a model, or else perform imputation to fill these values.\nTo recreate our random intercept model from the previous section using pymer4, we would use the following code:\n\nfrom pymer4.models import glmer\nimport pandas as pd\nimport polars as pl\n\nurl = \"https://peopleanalytics-regression-book.org/data/speed_dating.csv\"\n\n# load data and drop rows with NaN values, then convert to Polars\nspeed_dating = pd.read_csv(url).dropna()\nspeed_dating = pl.DataFrame(speed_dating) \n\n# fit binomial mixed effects model\nmodel = glmer(\"dec ~ agediff + samerace + attr + intel + prob + (1 | iid)\",\n             data = speed_dating, family = 'binomial')\nmodel.fit()\n\n# view the table of results\nmodel.summary()._tbl_data\n\n\nshape: (9, 10)\n\n\n\nrfx\nparam\nestimate\nconf_low\nconf_high\nstd_error\nz_stat\ndf\np_value\nstars\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nstr\nstr\n\n\n\n\n\"iid-sd\"\n\"(Intercept)\"\n2.278032\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\"Fixed Effects:\"\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\nnull\n\"(Intercept)\"\n-12.893847\n-13.721113\n-12.06658\n0.422082\n-30.548174\ninf\n\"&lt;.001\"\n\"***\"\n\n\nnull\n\"agediff\"\n-0.036687\n-0.064145\n-0.009229\n0.014009\n-2.618721\ninf\n\"0.008826\"\n\"**\"\n\n\nnull\n\"samerace\"\n0.198032\n0.038387\n0.357678\n0.081453\n2.431242\ninf\n\"0.01505\"\n\"*\"\n\n\nnull\n\"attr\"\n1.07863\n1.013248\n1.144011\n0.033359\n32.334247\ninf\n\"&lt;.001\"\n\"***\"\n\n\nnull\n\"intel\"\n0.316896\n0.248741\n0.385052\n0.034774\n9.113065\ninf\n\"&lt;.001\"\n\"***\"\n\n\nnull\n\"prob\"\n0.619761\n0.563438\n0.676085\n0.028737\n21.566648\ninf\n\"&lt;.001\"\n\"***\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling Explicit and Latent Hierarchy in Data</span>"
    ]
  },
  {
    "objectID": "hierarchical_data.html#sec-struc-eq-model",
    "href": "hierarchical_data.html#sec-struc-eq-model",
    "title": "9  Modeling Explicit and Latent Hierarchy in Data",
    "section": "9.2 Structural equation models for latent hierarchy in data",
    "text": "9.2 Structural equation models for latent hierarchy in data\nIn this section we will focus entirely on survey data use cases, as this is the most common application of structural equation modeling in people analytics. However it should be noted that survey data is not the only situation where latent variables may be modeled, and this technology has substantially broader applications. Indeed, advanced practitioners may see opportunities to experiment with this technology in other use cases.\nIt is a frequent occurrence with surveys conducted on large samples of people, such as a public survey or a large company survey, that attempts to run regression models can be problematic due to the large number of survey questions or items. Often many of the items are highly correlated, and even if they were not, high dimensionality makes interpretability very challenging. Decision-makers are not usually interested in explanations that involve 50 or 100 variables.\nUsually, such a large number of survey items are not each independently measuring a different construct. Many of the items can be considered to be addressing similar thematic constructs. For example, the items ‘I believe I am compensated well’ and ‘I am happy with the benefits offered by my employer’ could both be considered to be related to employee rewards. In some cases, survey instruments can be explicitly constructed around these themes, and in other cases, surveys have grown organically over time to include a disorganized set of items that could be grouped into themes after the fact.\nIt is a common request for an analyst to model a certain outcome using the many items in a complex survey as input variables. In some cases the outcome being modeled is an item in the survey itself—usually some overall measure of sentiment—or in other cases the outcome could be independent of the survey instrument, for example future attrition from the organization. In this situation, a model using the themes as input variables is likely to be a lot more useful and interpretable than a model using the items as input variables.\nStructural equation modeling is a technique that allows an analyst to hypothesize a smaller set of latent variables or factors that explain the responses to the survey items themselves (the ‘measured variables’), and then regresses the outcome of interest against these latent factors. It is a two-part approach, each part being a separate model in and of itself, as follows:\n\nMeasurement model: This is focused on how well the hypothesized factors explain the responses to the survey items using a technique called factor analysis. In the most common case, where a subject matter expert has pre-organized the items into several groups corresponding to hypothesized latent variables, the process is called confirmatory factor analysis, and the objective is to confirm that the groupings represent a high-quality measurement model, adjusting as necessary to refine the model. In the simplest case, items are fitted into separate independent themes with no overlap.\nStructural model: Assuming a satisfactory measurement model, the structural model is effectively a regression model which explains how each of the proposed factors relate to the outcome of interest.\n\nAs a walkthrough example, we will work with the politics_survey data set.\n\n# if needed, get data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/politics_survey.csv\"\npolitics_survey &lt;- read.csv(url)\n\nThis data set represents the results of a survey conducted by a political party on a set of approximately 2100 voters. The results are on a Likert scale of 1 to 4 where 1 indicates strong negative sentiment with a statement and 4 indicates strong positive sentiment. Subject matter experts have already grouped the items into proposed latent variables or factors, and the data takes the following form:\n\nOverall represents the overall intention to vote for the party in the next election.\nItems beginning with Pol are considered to be related to the policies of the political party.\nItems beginning with Hab are considered to be related to prior voting habits in relation to the political party.\nItems beginning with Loc are considered to be related to interest in local issues around where the respondent resided.\nItems beginning with Env are considered to be related to interest in environmental issues.\nItems beginning with Int are considered to be related to interest in international issues.\nItems beginning with Pers are considered to be related to the personalities of the party representatives/leaders.\nItems beginning with Nat are considered to be related to interest in national issues.\nItems beginning with Eco are considered to be related to interest in economic issues.\n\nLet’s take a quick look at the data.\n\nhead(politics_survey)\n\n  Overall Pol1 Pol2 Pol3 Hab1 Hab2 Hab3 Loc1 Loc2 Loc3 Loc4 Env1 Env2 Int1 Int2\n1       3    2    2    2    2    2    2    3    3    2    2    2    3    3    3\n2       4    4    4    4    4    4    4    4    4    4    4    4    4    4    3\n3       4    4    4    4    3    2    2    4    4    4    4    4    4    4    4\n4       3    4    4    4    3    2    2    4    3    3    4    4    4    4    3\n5       3    3    3    4    4    3    3    3    4    3    3    4    4    3    4\n6       4    3    3    4    3    2    3    3    3    2    2    3    3    4    3\n  Pers1 Pers2 Pers3 Nat1 Nat2 Nat3 Eco1 Eco2\n1     3     4     4    3    3    4    3    3\n2     4     4     4    4    4    4    3    4\n3     4     4     4    4    4    4    4    4\n4     2     3     3    4    4    2    4    4\n5     4     3     3    4    3    4    3    3\n6     3     4     3    3    4    3    4    4\n\n\nThe outcome of interest here is the Overall rating. Our first aim is to confirm that the eight factors suggested by the subject matter experts represent a satisfactory measurement model (that they reasonably explain the responses to the 22 items), adjusting or refining if needed. Assuming we can confirm a satisfactory measurement model, our second aim is to run a structural model to determine how each factor relates to the overall intention to vote for the party in the next election.\n\n9.2.1 Running and assessing the measurement model\nThe proposed measurement model can be seen in Figure 9.5. In this path diagram, we see the eight latent variables or factors (circles) and how they map to the individual measured items (squares) in the survey using single headed arrows. Here we are making a simplifying assumption that each latent variable influences an independent group of survey items. The diagram also notes that the latent variables may be correlated with each other, as indicated by the double-headed arrows at the top. Dashed-line paths indicate that a specific item will be used to scale the variance of the latent factor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.5: Simple path diagram showing proposed measurement model for politics_survey\n\n\n\nThe lavaan package in R is a specialized package for running analysis on latent variables. The function cfa() can be used to perform a confirmatory factor analysis on a specified measurement model. The measurement model can be specified using an appropriately commented and formatted text string as follows. Note the =~ notation, and note also that each factor is defined on a new line.\n\n# define measurement model\n\nmeas_mod &lt;- \"\n# measurement model\nPol =~ Pol1 + Pol2 + Pol3\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc1 + Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers1 + Pers2 + Pers3\nNat =~ Nat1 + Nat2 + Nat3\nEco =~ Eco1 + Eco2\n\"\n\nWith the measurement model defined, the confirmatory factor analysis can be run and a summary viewed. The lavaan summary functions used in this section produce quite large outputs. We will proceed to highlight which parts of this output are important in interpreting and refining the model.\n\nlibrary(lavaan)\n\ncfa_meas_mod &lt;- lavaan::cfa(model = meas_mod, data = politics_survey)\nlavaan::summary(cfa_meas_mod, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6-20 ended normally after 108 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        70\n\n  Number of observations                          2108\n\nModel Test User Model:\n                                                      \n  Test statistic                               838.914\n  Degrees of freedom                               161\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             17137.996\n  Degrees of freedom                               210\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.960\n  Tucker-Lewis Index (TLI)                       0.948\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -37861.518\n  Loglikelihood unrestricted model (H1)     -37442.061\n                                                      \n  Akaike (AIC)                               75863.036\n  Bayesian (BIC)                             76258.780\n  Sample-size adjusted Bayesian (SABIC)      76036.383\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.045\n  90 Percent confidence interval - lower         0.042\n  90 Percent confidence interval - upper         0.048\n  P-value H_0: RMSEA &lt;= 0.050                    0.998\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pol =~                                                                \n    Pol1              1.000                               0.568    0.772\n    Pol2              0.883    0.032   27.431    0.000    0.501    0.737\n    Pol3              0.488    0.024   20.575    0.000    0.277    0.512\n  Hab =~                                                                \n    Hab1              1.000                               0.623    0.755\n    Hab2              1.207    0.032   37.980    0.000    0.752    0.887\n    Hab3              1.138    0.031   36.603    0.000    0.710    0.815\n  Loc =~                                                                \n    Loc1              1.000                               0.345    0.596\n    Loc2              1.370    0.052   26.438    0.000    0.473    0.827\n    Loc3              1.515    0.058   26.169    0.000    0.523    0.801\n  Env =~                                                                \n    Env1              1.000                               0.408    0.809\n    Env2              0.605    0.031   19.363    0.000    0.247    0.699\n  Int =~                                                                \n    Int1              1.000                               0.603    0.651\n    Int2              1.264    0.060   20.959    0.000    0.762    0.869\n  Pers =~                                                               \n    Pers1             1.000                               0.493    0.635\n    Pers2             1.048    0.041   25.793    0.000    0.517    0.770\n    Pers3             0.949    0.039   24.440    0.000    0.468    0.695\n  Nat =~                                                                \n    Nat1              1.000                               0.522    0.759\n    Nat2              0.991    0.032   31.325    0.000    0.518    0.744\n    Nat3              0.949    0.035   27.075    0.000    0.495    0.638\n  Eco =~                                                                \n    Eco1              1.000                               0.525    0.791\n    Eco2              1.094    0.042   26.243    0.000    0.575    0.743\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pol ~~                                                                \n    Hab               0.165    0.011   14.947    0.000    0.466    0.466\n    Loc               0.106    0.007   15.119    0.000    0.540    0.540\n    Env               0.089    0.007   12.101    0.000    0.385    0.385\n    Int               0.146    0.012   12.248    0.000    0.425    0.425\n    Pers              0.162    0.010   15.699    0.000    0.577    0.577\n    Nat               0.177    0.010   17.209    0.000    0.596    0.596\n    Eco               0.150    0.010   15.123    0.000    0.504    0.504\n  Hab ~~                                                                \n    Loc               0.069    0.006   11.060    0.000    0.323    0.323\n    Env               0.051    0.007    7.161    0.000    0.200    0.200\n    Int               0.134    0.012   11.395    0.000    0.357    0.357\n    Pers              0.121    0.010   12.619    0.000    0.393    0.393\n    Nat               0.105    0.009   11.271    0.000    0.324    0.324\n    Eco               0.089    0.009    9.569    0.000    0.273    0.273\n  Loc ~~                                                                \n    Env               0.076    0.005   15.065    0.000    0.541    0.541\n    Int               0.091    0.007   12.192    0.000    0.438    0.438\n    Pers              0.098    0.007   14.856    0.000    0.574    0.574\n    Nat               0.116    0.007   16.780    0.000    0.642    0.642\n    Eco               0.090    0.006   14.354    0.000    0.496    0.496\n  Env ~~                                                                \n    Int               0.075    0.008    9.506    0.000    0.303    0.303\n    Pers              0.075    0.007   11.482    0.000    0.375    0.375\n    Nat               0.093    0.007   13.616    0.000    0.439    0.439\n    Eco               0.078    0.007   11.561    0.000    0.365    0.365\n  Int ~~                                                                \n    Pers              0.156    0.012   13.349    0.000    0.525    0.525\n    Nat               0.186    0.012   14.952    0.000    0.592    0.592\n    Eco               0.137    0.011   12.374    0.000    0.432    0.432\n  Pers ~~                                                               \n    Nat               0.185    0.010   17.898    0.000    0.717    0.717\n    Eco               0.153    0.010   15.945    0.000    0.590    0.590\n  Nat ~~                                                                \n    Eco               0.196    0.010   19.440    0.000    0.715    0.715\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Pol1              0.219    0.012   19.015    0.000    0.219    0.404\n   .Pol2              0.211    0.010   21.455    0.000    0.211    0.457\n   .Pol3              0.216    0.007   29.384    0.000    0.216    0.737\n   .Hab1              0.293    0.011   25.855    0.000    0.293    0.430\n   .Hab2              0.153    0.011   14.434    0.000    0.153    0.213\n   .Hab3              0.254    0.012   21.814    0.000    0.254    0.335\n   .Loc1              0.217    0.007   29.063    0.000    0.217    0.645\n   .Loc2              0.103    0.006   18.226    0.000    0.103    0.316\n   .Loc3              0.153    0.007   20.463    0.000    0.153    0.358\n   .Env1              0.088    0.008   10.643    0.000    0.088    0.345\n   .Env2              0.064    0.003   18.407    0.000    0.064    0.511\n   .Int1              0.495    0.021   23.182    0.000    0.495    0.576\n   .Int2              0.188    0.025    7.653    0.000    0.188    0.244\n   .Pers1             0.361    0.013   27.065    0.000    0.361    0.597\n   .Pers2             0.184    0.009   20.580    0.000    0.184    0.408\n   .Pers3             0.234    0.009   24.865    0.000    0.234    0.517\n   .Nat1              0.201    0.009   23.320    0.000    0.201    0.425\n   .Nat2              0.215    0.009   24.119    0.000    0.215    0.446\n   .Nat3              0.357    0.013   27.981    0.000    0.357    0.593\n   .Eco1              0.165    0.010   16.244    0.000    0.165    0.374\n   .Eco2              0.268    0.013   20.071    0.000    0.268    0.448\n    Pol               0.323    0.018   18.035    0.000    1.000    1.000\n    Hab               0.389    0.020   19.276    0.000    1.000    1.000\n    Loc               0.119    0.009   13.906    0.000    1.000    1.000\n    Env               0.166    0.011   15.560    0.000    1.000    1.000\n    Int               0.364    0.026   13.846    0.000    1.000    1.000\n    Pers              0.243    0.017   14.619    0.000    1.000    1.000\n    Nat               0.273    0.015   18.788    0.000    1.000    1.000\n    Eco               0.276    0.015   17.974    0.000    1.000    1.000\n\n\nThis is a large set of results, but we can focus in on some important parameters to examine. First, we note that the results did not come attached to a warning. One particular warning to look out for relates to the covariance matrix being non-positive definite. This renders some of the attempted measurement invalid and is usually caused by too small a sample size for the complexity of the measurement model. Since we did not receive this warning, we can proceed safely.\nSecond, we examine the fit statistics. Numerous statistics are reported2, but for larger samples such as this data set, the following measures should be examined:\n\nCFI and TLI, which compare the proposed model to a baseline (null or random) model to determine if it is better. Ideally we look for both of these measures to exceed 0.95. We see that our measurement model comes very close to meeting these criteria.\nRMSEA should ideally be less than 0.06, which is met by our measurement model.\nSRMR should ideally be less than 0.08, which is met by our measurement model.\n\nFinally, the parameter estimates for the latent variables should be examined. In particular the Std.all column which is similar to standardized regression coefficients. These parameters are commonly known as factor loadings—they can be interpreted as the extent to which the item response is explained by the proposed latent variable. In general, factor loadings of 0.7 or above are considered reasonable. Factor loadings less than this may be introducing unacceptable measurement error. One option if this occurs is to drop the item completely from the measurement model, or to explore an alternative measurement model with the item assigned to another latent variable. In any case the analyst will need to balance these considerations against the need to have factors measured against multiple items wherever possible in order to minimize other aspects of measurement error.\nIn our case we could consider dropping Pol3, Loc1, Pers1 and Nat3 from the measurement model as they have factor loadings of less than 0.7 and are in factors that contain three items. We will fit this revised measurement model, and rather than printing the entire output again, we will focus here on our CFI, TLI, RMSEA and SRMR statistics to see if they have improved. It is advisable, however, that factor loadings are also checked, especially where primary items that scale the variance of latent factors have been removed.\n\nmeas_mod_revised &lt;- \"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\"\n\ncfa_meas_mod_rev &lt;- lavaan::cfa(model = meas_mod_revised, \n                                data = politics_survey)\n\nfits &lt;- lavaan::fitmeasures(cfa_meas_mod_rev)\n\nfits[c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")]\n\n       cfi        tli      rmsea       srmr \n0.97804888 0.96719394 0.03966962 0.02736629 \n\n\nWe now see that our measurement model comfortably meets all fit requirements. In our case we chose to completely drop four items from the model. Analysts may wish to experiment with relaxing criteria on dropping items, or on reassigning items to other factors to achieve a good balance between fit and factor measurement reliability.\n\n\n9.2.2 Running and interpreting the structural model\nWith a satisfactory measurement model, the structural model is a simple regression formula. The sem() function in lavaan can be used to perform a full structural equation model including the measurement model and structural model. Like for cfa(), an extensive output can be expected from this function, but assuming that our measurement model is satisfactory, our key interest is now in the structural model elements of this output.\n\n# define full SEM using revised measurement model\nfull_sem &lt;- \"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\n# structural model\nOverall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco\n\"\n\n# run full SEM \nfull_model &lt;- lavaan::sem(model = full_sem, data = politics_survey)\nlavaan::summary(full_model, standardized = TRUE)\n\nlavaan 0.6-20 ended normally after 99 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        71\n\n  Number of observations                          2108\n\nModel Test User Model:\n                                                      \n  Test statistic                               465.318\n  Degrees of freedom                               100\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pol =~                                                                \n    Pol1              1.000                               0.626    0.850\n    Pol2              0.714    0.029   25.038    0.000    0.447    0.657\n  Hab =~                                                                \n    Hab1              1.000                               0.630    0.763\n    Hab2              1.184    0.031   38.592    0.000    0.746    0.879\n    Hab3              1.127    0.030   37.058    0.000    0.710    0.816\n  Loc =~                                                                \n    Loc2              1.000                               0.461    0.806\n    Loc3              1.179    0.036   32.390    0.000    0.544    0.833\n  Env =~                                                                \n    Env1              1.000                               0.411    0.815\n    Env2              0.596    0.031   19.281    0.000    0.245    0.695\n  Int =~                                                                \n    Int1              1.000                               0.605    0.653\n    Int2              1.256    0.062   20.366    0.000    0.760    0.867\n  Pers =~                                                               \n    Pers2             1.000                               0.520    0.774\n    Pers3             0.939    0.036   25.818    0.000    0.488    0.726\n  Nat =~                                                                \n    Nat1              1.000                               0.511    0.742\n    Nat2              1.033    0.034   29.958    0.000    0.527    0.758\n  Eco =~                                                                \n    Eco1              1.000                               0.529    0.797\n    Eco2              1.078    0.042   25.716    0.000    0.570    0.737\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Overall ~                                                             \n    Pol               0.330    0.036    9.281    0.000    0.206    0.307\n    Hab               0.255    0.024   10.614    0.000    0.161    0.240\n    Loc               0.224    0.047    4.785    0.000    0.103    0.154\n    Env              -0.114    0.042   -2.738    0.006   -0.047   -0.070\n    Int               0.046    0.028    1.605    0.108    0.028    0.041\n    Pers              0.112    0.047    2.383    0.017    0.058    0.087\n    Nat               0.122    0.071    1.728    0.084    0.063    0.093\n    Eco               0.002    0.043    0.041    0.967    0.001    0.001\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pol ~~                                                                \n    Hab               0.183    0.012   15.476    0.000    0.465    0.465\n    Loc               0.156    0.009   16.997    0.000    0.540    0.540\n    Env               0.096    0.008   12.195    0.000    0.374    0.374\n    Int               0.162    0.013   12.523    0.000    0.427    0.427\n    Pers              0.171    0.011   15.975    0.000    0.525    0.525\n    Nat               0.195    0.011   17.798    0.000    0.610    0.610\n    Eco               0.167    0.011   15.752    0.000    0.506    0.506\n  Hab ~~                                                                \n    Loc               0.091    0.008   11.218    0.000    0.315    0.315\n    Env               0.052    0.007    7.199    0.000    0.200    0.200\n    Int               0.138    0.012   11.426    0.000    0.361    0.361\n    Pers              0.112    0.010   11.484    0.000    0.341    0.341\n    Nat               0.105    0.010   11.045    0.000    0.327    0.327\n    Eco               0.091    0.009    9.608    0.000    0.273    0.273\n  Loc ~~                                                                \n    Env               0.103    0.006   16.413    0.000    0.544    0.544\n    Int               0.120    0.010   12.529    0.000    0.429    0.429\n    Pers              0.130    0.008   16.209    0.000    0.542    0.542\n    Nat               0.153    0.008   18.203    0.000    0.648    0.648\n    Eco               0.117    0.008   14.985    0.000    0.479    0.479\n  Env ~~                                                                \n    Int               0.075    0.008    9.505    0.000    0.303    0.303\n    Pers              0.075    0.007   11.058    0.000    0.351    0.351\n    Nat               0.091    0.007   13.181    0.000    0.434    0.434\n    Eco               0.079    0.007   11.583    0.000    0.364    0.364\n  Int ~~                                                                \n    Pers              0.153    0.012   13.118    0.000    0.486    0.486\n    Nat               0.173    0.012   14.159    0.000    0.560    0.560\n    Eco               0.138    0.011   12.293    0.000    0.431    0.431\n  Pers ~~                                                               \n    Nat               0.192    0.010   18.922    0.000    0.724    0.724\n    Eco               0.156    0.010   16.369    0.000    0.567    0.567\n  Nat ~~                                                                \n    Eco               0.192    0.010   18.968    0.000    0.710    0.710\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Pol1              0.150    0.013   11.205    0.000    0.150    0.277\n   .Pol2              0.263    0.010   25.479    0.000    0.263    0.569\n   .Hab1              0.285    0.011   25.570    0.000    0.285    0.418\n   .Hab2              0.163    0.010   15.746    0.000    0.163    0.227\n   .Hab3              0.253    0.011   22.046    0.000    0.253    0.334\n   .Loc2              0.114    0.006   18.168    0.000    0.114    0.350\n   .Loc3              0.130    0.008   15.729    0.000    0.130    0.306\n   .Env1              0.085    0.008   10.227    0.000    0.085    0.336\n   .Env2              0.064    0.003   18.701    0.000    0.064    0.518\n   .Int1              0.492    0.022   22.597    0.000    0.492    0.574\n   .Int2              0.192    0.025    7.547    0.000    0.192    0.249\n   .Pers2             0.181    0.010   17.472    0.000    0.181    0.401\n   .Pers3             0.215    0.010   21.151    0.000    0.215    0.474\n   .Nat1              0.213    0.009   22.690    0.000    0.213    0.450\n   .Nat2              0.205    0.010   21.502    0.000    0.205    0.425\n   .Eco1              0.160    0.010   15.413    0.000    0.160    0.364\n   .Eco2              0.273    0.014   20.156    0.000    0.273    0.457\n   .Overall           0.242    0.008   29.506    0.000    0.242    0.537\n    Pol               0.392    0.020   19.235    0.000    1.000    1.000\n    Hab               0.397    0.020   19.581    0.000    1.000    1.000\n    Loc               0.213    0.011   19.724    0.000    1.000    1.000\n    Env               0.169    0.011   15.606    0.000    1.000    1.000\n    Int               0.366    0.027   13.699    0.000    1.000    1.000\n    Pers              0.270    0.015   17.514    0.000    1.000    1.000\n    Nat               0.261    0.015   17.787    0.000    1.000    1.000\n    Eco               0.280    0.016   17.944    0.000    1.000    1.000\n\n\nThe Std.all column of the Regressions section of the output provides the fundamentals of the structural model—these are standardized estimates which can be approximately interpreted as the proportion of the variance of the outcome that is explained by each factor. Here we can make the following interpretations:\n\nPolicies, habit and interest in local issues represent the three strongest drivers of likelihood of voting for the party at the next election, and explain approximately 70% of the overall variance in the outcome.\nInterest in national or international issues, and interest in the economy each have no significant relationship with likelihood to vote for the party at the next election.\nInterest in the environment has a significant negative relationship with likelihood to vote for the party at the next election.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.6: Path diagram for full structural equation model on politics_survey\n\n\n\nThe full structural equation model can be seen in Figure 9.6. This simple example illustrates the value of structural equation modeling in both reducing the dimensions of a complex regression problem and in developing more intuitive and interpretable results for stakeholders. The underlying theory of latent variable modeling, and its implementation in the lavaan package, offer much more flexibility and parameter control options than illustrated here and further exploration is highly recommended. Bartholomew, Knott, and Moustaki (2011) and Skrondal and Rabe-Hesketh (2004) are excellent resources for a deeper study of the theory and a wider range of case examples.\n\n\n9.2.3 Structural equation models using Python\nThe semopy package is a specialized package for the implementation of Structural Equation Models in Python, and its implementation is very similar to the lavaan package in R. However, its reporting is not as intuitive compared to lavaan. A full tutorial is available here. Here is an example of how to run the same model as that studied in Section 9.2 using semopy.\n\nimport pandas as pd\nfrom semopy import Model\n\n\n# get data\nurl = \"https://peopleanalytics-regression-book.org/data/politics_survey.csv\"\npolitics_survey = pd.read_csv(url)\n\n\n# define full measurement and structural model\nmeasurement_model = \"\"\"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\n# structural model\nOverall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco\n\"\"\"\n\nfull_model = Model(measurement_model)\n\n\n# fit model to data and inspect\nfull_model.fit(politics_survey)\n\nThen to inspect the results:\n\n# inspect the results of SEM (first few rows)\nfull_model.inspect().head()\n\n   lval op rval  Estimate  Std. Err    z-value p-value\n0  Pol1  ~  Pol  1.000000         -          -       -\n1  Pol2  ~  Pol  0.713719  0.028505  25.038173     0.0\n2  Hab1  ~  Hab  1.000000         -          -       -\n3  Hab2  ~  Hab  1.183981  0.030679  38.592255     0.0\n4  Hab3  ~  Hab  1.127639  0.030429  37.057838     0.0",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling Explicit and Latent Hierarchy in Data</span>"
    ]
  },
  {
    "objectID": "hierarchical_data.html#learning-exercises",
    "href": "hierarchical_data.html#learning-exercises",
    "title": "9  Modeling Explicit and Latent Hierarchy in Data",
    "section": "9.3 Learning exercises",
    "text": "9.3 Learning exercises\n\n9.3.1 Discussion questions\n\nDescribe some common forms of explicit hierarchies in data. Can you think of some data sets that you have worked with recently that contain an explicit hierarchy?\nDescribe the meaning of ‘fixed effect’ and ‘random effect’ in a mixed regression model.\nWhich parameter in a mixed regression model is most commonly used when applying a random effect?\nDescribe why mixed models are sometimes referred to as multilevel models.\nIn a two-level mixed model, describe the two levels of statistics that are produced and how to interpret these statistics.\nIn latent variable modeling, what is the difference between a latent variable and a measured variable?\nDescribe some reasons why latent variable modeling can be valuable in practice.\nDescribe the two components of a structural equation model. What is the purpose of each component?\nWhat are the steps involved in a confirmatory factor analysis on a sufficiently large data set? Describe some fit criteria and the ideal standards for those criteria.\nDescribe a process for refining a factor analysis based on fit criteria and factor loadings. What considerations should be addressed during this process?\n\n\n\n9.3.2 Data exercises\nFor Exercises 1–4, use the speed_dating set used earlier in this chapter3.\n\nSplit the data into two sets according to the gender of the participant. Run standard binomial logistic regression models on each set to determine the relationship between the dec decision outcome and the input variables samerace, agediff, attr, intel and prob.\nRun similar mixed models on these sets with a random intercept for iid.\nWhat different conclusions can you make in comparing the mixed models with the standard models?\nExperiment with some random slope effects to see if they reveal anything new about the input variables.\n\nFor exercises 5–10, load the employee_survey data set via the peopleanalyticsdata package or download it from the internet4. This data set contains the results of an engagement survey of employees of a technology company. Each row represents the responses of an individual to the survey and each column represents a specific survey question, with responses on a Likert scale of 1 to 4, with 1 indicating strongly negative sentiment and 4 indicating strongly positive sentiment. Subject matter experts have grouped the items into hypothesized latent factors as follows:\n\nHappiness is an overall measure of the employees current sentiment about their job.\nItems beginning with Ben relate to employment benefits.\nItems beginning with Work relate to the general work environment.\nItems beginning with Man relate to perceptions of management.\nItems beginning with Car relate to perceptions of career prospects.\n\n\nWrite out the proposed measurement model, defining the latent factors in terms of the measured items.\nRun a confirmatory factor analysis on the proposed measurement model. Examine the fit and the factor loadings.\nExperiment with the removal of measured items from the measurement model in order to improve the overall fit.\nOnce satisfied with the fit of the measurement model, run a full structural equation model on the data.\nInterpret the results of the structural model. Which factors appear most related to overall employee sentiment? Approximately what proportion of the variance in overall sentiment does the model explain?\nIf you dropped measured items from your measurement model, experiment with assigning them to other factors to see if this improves the fit of the model. What statistics would you use to compare different measurement models?\n\n\n\n\n\nBartholomew, David J., Martin Knott, and Irini Moustaki. 2011. Latent Variable Models and Factor Analysis: A Unified Approach.\n\n\nJiang, Jiming. 2007. Linear and Generalized Linear Mixed Models and Their Applications.\n\n\nSkrondal, Anders, and Sophia Rabe-Hesketh. 2004. Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling Explicit and Latent Hierarchy in Data</span>"
    ]
  },
  {
    "objectID": "hierarchical_data.html#footnotes",
    "href": "hierarchical_data.html#footnotes",
    "title": "9  Modeling Explicit and Latent Hierarchy in Data",
    "section": "",
    "text": "I have simplified the data set, and the full version can be found at https://www.openml.org/search?type=data&exact_name=SpeedDating↩︎\nOther reported measures include chi-square statistical tests of perfect fit for the measurement model and for the baseline model, and AIC and BIC. While these generally are not very helpful in determining the quality of a specific measurement model, they are valuable for comparing different measurement model options.↩︎\nhttps://peopleanalytics-regression-book.org/data/speed_dating.csv↩︎\nhttps://peopleanalytics-regression-book.org/data/employee_survey.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling Explicit and Latent Hierarchy in Data</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html",
    "href": "survival_analysis.html",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "",
    "text": "10.1 Tracking and illustrating survival rates over the study period\nIn previous chapters, the outcomes we have been modeling have or have not occurred at a particular point in time following when the input variables were measured. For example, in Chapter 4 input variables were measured in the first three years of an education program and the outcome was measured at the end of the fourth year. In many situations, the outcome we are interested in is a singular event that can occur at any time following when the input variables were measured, can occur at a different time for different individuals, and once it has occurred it cannot reoccur or repeat. In medical studies, death can occur or the onset of a disease can be diagnosed at any time during the study period. In employment contexts, an attrition event can occur at various times throughout the year.\nAn obvious and simple way to deal with this would be to simply agree to look at a specific point in time and measure whether or not the event had occurred at that point, for example, ‘How many employees had left at the three-year point?’. Such an approach allows us to use standard generic regression models like those studied in previous chapters. But this approach has limitations.\nFirstly, we are only able to infer conclusions about the likelihood of the event having occurred as at the end of the period of study. We cannot make inferences about the likelihood of the event throughout the period of study. Being able to say that attrition is twice as likely for certain types of individuals at any time throughout the three years is more powerful than merely saying that attrition is twice as likely at the three-year point.\nSecondly, our sample size is constrained by the state of our data at the end of the period of study. Therefore if we lose track of an individual after two years and six months, that observation needs to be dropped from our data set if we are focused only on the three-year point. Wherever possible, loss of data is something a statistician will want to avoid as it affects the accuracy and statistical power of inferences, and also means research effort was wasted.\nSurvival analysis is a general term for the modeling of a time-associated binary non-repeated outcome, usually involving an understanding of the comparative risk of that outcome between two or more different groups of interest. There are two common components in an elementary survival analysis, as follows:\nThose seeking a more in depth treatment of survival analysis should consult texts on its use in medical/clinical contexts, and a recommended source is Collett (2015). In this chapter we will use a walkthrough example to illustrate a typical use of survival analysis in a people analytics context.\nThe job_retention data set shows the results of a study of around 3,800 individuals employed in various fields of employment over a one-year period. At the beginning of the study, the individuals were asked to rate their sentiment towards their job. These individuals were then followed up monthly for a year to determine if they were still working in the same job or had left their job for a substantially different job. If an individual was not successfully followed up in a given month, they were no longer followed up for the remainder of the study period.\nFor this walkthrough example, the particular fields we are interested in are:\nIn our example, we are defining ‘survival’ as ‘remaining in substantially the same job’‍. We can regard the starting point as month 0, and we are following up in each of months 1 through 12. For a given month \\(i\\), we can define a survival rate \\(S_i\\) as follows\n\\[\nS_i = S_{i - 1}(1 - \\frac{l_i}{n_i})\n\\] where \\(l_i\\) is the number reported as left in month \\(i\\), and \\(n_i\\) is the number still in substantially the same job after month \\(i - 1\\), with \\(S_0 = 1\\).\nThe survival package in R allows easy construction of survival rates on data in a similar format to that in our job_retention data set. A survival object is created using the Surv() function to track the survival rate at each time period.\nlibrary(survival)\n\n# create survival object with event as 'left' and time as 'month'\nretention &lt;- Surv(event = job_retention$left, \n                  time = job_retention$month)\n\n# view unique values of retention\nunique(retention)\n\n [1]  1  12+  5   2   3   6   8   4   8+  4+ 11  10   9   7+  5+  3+  7   9+ 11+\n[20] 12  10+  6+  2+  1+\nWe can see that our survival object records the month at which the individual had left their job if they are recorded as having done so in the data set. If not, the object records the last month at which there was a record of the individual, appended with a ‘+’ to indicate that this was the last record available.\nThe survfit() function allows us to calculate Kaplan-Meier estimates of survival for different groups in the data so that we can compare them. We can do this using our usual formula notation but using a survival object as the outcome. Let’s take a look at survival by gender.\n# kaplan-meier estimates of survival by gender\nkmestimate_gender &lt;- survival::survfit(\n  formula = Surv(event = left, time = month) ~ gender, \n  data = job_retention\n)\n\nsummary(kmestimate_gender)\n\nCall: survfit(formula = Surv(event = left, time = month) ~ gender, \n    data = job_retention)\n\n                gender=F \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1   1167       7    0.994 0.00226        0.990        0.998\n    2   1140      24    0.973 0.00477        0.964        0.982\n    3   1102      45    0.933 0.00739        0.919        0.948\n    4   1044      45    0.893 0.00919        0.875        0.911\n    5    987      30    0.866 0.01016        0.846        0.886\n    6    940      51    0.819 0.01154        0.797        0.842\n    7    882      43    0.779 0.01248        0.755        0.804\n    8    830      47    0.735 0.01333        0.709        0.762\n    9    770      40    0.697 0.01394        0.670        0.725\n   10    718      21    0.676 0.01422        0.649        0.705\n   11    687      57    0.620 0.01486        0.592        0.650\n   12    621      17    0.603 0.01501        0.575        0.633\n\n                gender=M \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1   2603      17    0.993 0.00158        0.990        0.997\n    2   2559      66    0.968 0.00347        0.961        0.975\n    3   2473     100    0.929 0.00508        0.919        0.939\n    4   2360      86    0.895 0.00607        0.883        0.907\n    5   2253      56    0.873 0.00660        0.860        0.886\n    6   2171     120    0.824 0.00756        0.810        0.839\n    7   2029      85    0.790 0.00812        0.774        0.806\n    8   1916     114    0.743 0.00875        0.726        0.760\n    9   1782      96    0.703 0.00918        0.685        0.721\n   10   1661      50    0.682 0.00938        0.664        0.700\n   11   1590     101    0.638 0.00972        0.620        0.658\n   12   1460      36    0.623 0.00983        0.604        0.642\nWe can see that the n.risk, n.event and survival columns for each group correspond to the \\(n_i\\), \\(l_i\\) and \\(S_i\\) in our formula above and that the confidence intervals for each survival rate are given. This can be very useful if we wish to illustrate a likely effect of a given input variable on survival likelihood.\nLet’s imagine that we wish to determine if the sentiment of the individual had an impact on survival likelihood. We can divide our population into two (or more) groups based on their sentiment and compare their survival rates.\n# create a new field to define high sentiment (&gt;= 7)\njob_retention$sentiment_category &lt;- ifelse(\n  job_retention$sentiment &gt;= 7, \n  \"High\", \n  \"Not High\"\n)\n\n# generate survival rates by sentiment category\nkmestimate_sentimentcat &lt;- survival::survfit(\n  formula = Surv(event = left, time = month) ~ sentiment_category,\n  data = job_retention\n)\n\nsummary(kmestimate_sentimentcat)\n\nCall: survfit(formula = Surv(event = left, time = month) ~ sentiment_category, \n    data = job_retention)\n\n                sentiment_category=High \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1   3225      15    0.995 0.00120        0.993        0.998\n    2   3167      62    0.976 0.00272        0.971        0.981\n    3   3075     120    0.938 0.00429        0.929        0.946\n    4   2932     102    0.905 0.00522        0.895        0.915\n    5   2802      65    0.884 0.00571        0.873        0.895\n    6   2700     144    0.837 0.00662        0.824        0.850\n    7   2532     110    0.801 0.00718        0.787        0.815\n    8   2389     140    0.754 0.00778        0.739        0.769\n    9   2222     112    0.716 0.00818        0.700        0.732\n   10   2077      56    0.696 0.00835        0.680        0.713\n   11   1994     134    0.650 0.00871        0.633        0.667\n   12   1827      45    0.634 0.00882        0.617        0.651\n\n                sentiment_category=Not High \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    545       9    0.983 0.00546        0.973        0.994\n    2    532      28    0.932 0.01084        0.911        0.953\n    3    500      25    0.885 0.01373        0.859        0.912\n    4    472      29    0.831 0.01618        0.800        0.863\n    5    438      21    0.791 0.01758        0.757        0.826\n    6    411      27    0.739 0.01906        0.703        0.777\n    7    379      18    0.704 0.01987        0.666        0.744\n    8    357      21    0.662 0.02065        0.623        0.704\n    9    330      24    0.614 0.02136        0.574        0.658\n   10    302      15    0.584 0.02171        0.543        0.628\n   11    283      24    0.534 0.02209        0.493        0.579\n   12    254       8    0.517 0.02218        0.476        0.563\nWe can see that survival seems to consistently trend higher for those with high sentiment towards their jobs. The ggsurvplot() function in the survminer package can visualize this neatly and also provide additional statistical information on the differences between the groups, as shown in Figure 10.1.\nThis confirms that the survival difference between the two sentiment groups is statistically significant and provides a highly intuitive visualization of the effect of sentiment on retention throughout the period of study.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#tracking-and-illustrating-survival-rates-over-the-study-period",
    "href": "survival_analysis.html#tracking-and-illustrating-survival-rates-over-the-study-period",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "",
    "text": "library(survminer)\n\n# show survival curves with p-value estimate and confidence intervals\nsurvminer::ggsurvplot(\n  kmestimate_sentimentcat,\n  pval = TRUE,\n  conf.int = TRUE,\n  palette = c(\"blue\", \"red\"),\n  linetype = c(\"solid\", \"dashed\"),\n  xlab = \"Month\",\n  ylab = \"Retention Rate\"\n)\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Survival curves by sentiment category in the job_retention data",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#sec-coxphmodel",
    "href": "survival_analysis.html#sec-coxphmodel",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "10.2 Cox proportional hazard regression models",
    "text": "10.2 Cox proportional hazard regression models\nLet’s imagine that we have a survival outcome that we are modeling for a population over a time \\(t\\), and we are interested in how a set of input variables \\(x_1, x_2, \\dots, x_p\\) influences that survival outcome. Given that our survival outcome is a binary variable, we can model survival at any time \\(t\\) as a binary logistic regression. We define \\(h(t)\\) as the proportion who have not survived at time \\(t\\), called the hazard function, and based on our work in Chapter 5:\n\\[\nh(t) = h_0(t)e^{\\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}\n\\] where \\(h_0(t)\\) is a base or intercept hazard at time \\(t\\), and \\(\\beta_i\\) is the coefficient associated with \\(x_i\\) .\nNow let’s imagine we are comparing the hazard for two different individuals \\(A\\) and \\(B\\) from our population. We make an assumption that our hazard curves \\(h^A(t)\\) for individual \\(A\\) and \\(h^B(t)\\) for individual \\(B\\) are always proportional to each other and never cross—this is called the proportional hazard assumption. Under this assumption, we can conclude that\n\\[\n\\begin{aligned}\n\\frac{h^B(t)}{h^A(t)} &= \\frac{h_0(t)e^{\\beta_1x_1^B + \\beta_2x_2^B + \\dots + \\beta_px_p^B}}{h_0(t)e^{\\beta_1x_1^A + \\beta_2x_2^A + \\dots + \\beta_px_p^A}} \\\\\n&= e^{\\beta_1(x_1^B-x_1^A) + \\beta_2(x_2^B-x_2^A) + \\dots \\beta_p(x_p^B-x_p^A)}\n\\end{aligned}\n\\]\nNote that there is no \\(t\\) in our final equation. The important observation here is that the hazard for person B relative to person A is constant and independent of time. This allows us to take a complicating factor out of our model. It means we can model the effect of input variables on the hazard without needing to account for changes over times, making this model very similar in interpretation to a standard binomial regression model.\n\n10.2.1 Running a Cox proportional hazard regression model\nA Cox proportional hazard model can be run using the coxph() function in the survival package, with the outcome as a survival object. Let’s model our survival against the input variables gender, field, level and sentiment.\n\n# run cox model against survival outcome\ncox_model &lt;- survival::coxph(\n  formula = Surv(event = left, time = month) ~ gender + \n    field + level + sentiment,\n  data = job_retention\n)\n\nsummary(cox_model)\n\nCall:\nsurvival::coxph(formula = Surv(event = left, time = month) ~ \n    gender + field + level + sentiment, data = job_retention)\n\n  n= 3770, number of events= 1354 \n\n                           coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ngenderM                -0.04548   0.95553  0.05886 -0.773 0.439647    \nfieldFinance            0.22334   1.25025  0.06681  3.343 0.000829 ***\nfieldHealth             0.27830   1.32089  0.12890  2.159 0.030849 *  \nfieldLaw                0.10532   1.11107  0.14515  0.726 0.468086    \nfieldPublic/Government  0.11499   1.12186  0.08899  1.292 0.196277    \nfieldSales/Marketing    0.08776   1.09173  0.10211  0.859 0.390082    \nlevelLow                0.14813   1.15967  0.09000  1.646 0.099799 .  \nlevelMedium             0.17666   1.19323  0.10203  1.732 0.083362 .  \nsentiment              -0.11756   0.88909  0.01397 -8.415  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                       exp(coef) exp(-coef) lower .95 upper .95\ngenderM                   0.9555     1.0465    0.8514    1.0724\nfieldFinance              1.2502     0.7998    1.0968    1.4252\nfieldHealth               1.3209     0.7571    1.0260    1.7005\nfieldLaw                  1.1111     0.9000    0.8360    1.4767\nfieldPublic/Government    1.1219     0.8914    0.9423    1.3356\nfieldSales/Marketing      1.0917     0.9160    0.8937    1.3336\nlevelLow                  1.1597     0.8623    0.9721    1.3834\nlevelMedium               1.1932     0.8381    0.9770    1.4574\nsentiment                 0.8891     1.1248    0.8651    0.9138\n\nConcordance= 0.578  (se = 0.008 )\nLikelihood ratio test= 89.18  on 9 df,   p=2e-15\nWald test            = 94.95  on 9 df,   p=&lt;2e-16\nScore (logrank) test = 95.31  on 9 df,   p=&lt;2e-16\n\n\nThe model returns the following1\n\nCoefficients for each input variable and their p-values. Here we can conclude that working in Finance or Health is associated with a significantly greater likelihood of leaving over the period studied, and that higher sentiment is associated with a significantly lower likelihood of leaving.\nRelative odds ratios associated with each input variable. For example, a single extra point in sentiment reduces the odds of leaving by ~11%. A single less point increases the odds of leaving by ~12%. Confidence intervals for the coefficients are also provided.\nThree statistical tests on the null hypothesis that the coefficients are zero. This null hypothesis is rejected by all three tests which can be interpreted as meaning that the model is significant.\n\nImportantly, as well as statistically validating that sentiment has a significant effect on retention, our Cox model has allowed us to control for possible mediating variables. We can now say that sentiment has a significant effect on retention even for individuals of the same gender, in the same field and at the same level.\n\n\n10.2.2 Checking the proportional hazard assumption\nNote that we mentioned in the previous section a critical assumption for our Cox proportional hazard model to be valid, called the proportional hazard assumption. As always, it is important to check this assumption before finalizing any inferences or conclusions from your model.\nThe most popular test of this assumption uses a residual known as a Schoenfeld residual, which would be expected to be independent of time if the proportional hazard assumption holds. The cox.zph() function in the survival package runs a statistical test on the null hypothesis that the Schoenfeld residuals are independent of time. The test is conducted on every input variable and on the model as a whole, and a significant result would reject the proportional hazard assumption.\n\n(ph_check &lt;- survival::cox.zph(cox_model))\n\n           chisq df    p\ngender     0.726  1 0.39\nfield      6.656  5 0.25\nlevel      2.135  2 0.34\nsentiment  1.828  1 0.18\nGLOBAL    11.156  9 0.27\n\n\nIn our case, we can confirm that the proportional hazard assumption is not rejected. The ggcoxzph() function in the survminer package takes the result of the cox.zph() check and allows a graphical check by plotting the residuals against time, as seen in Figure 10.2.\n\n\n\n\nsurvminer::ggcoxzph(ph_check, \n                    font.main = 10, \n                    font.x = 10, \n                    font.y = 10)\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: Schoenfeld test on proportional hazard assumption for cox_model",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#frailty-models",
    "href": "survival_analysis.html#frailty-models",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "10.3 Frailty models",
    "text": "10.3 Frailty models\nWe noticed in our example in the previous section that certain fields of employment appeared to have a significant effect on the attrition hazard. It is therefore possible that different fields of employment have different base hazard functions, and we may wish to take this into account in determining if input variables have a significant relationship with attrition. This is analogous to a mixed model which we looked at in Section 9.1.\nIn this case we would apply a random intercept effect to the base hazard function \\(h_0(t)\\) according to the field of employment of an individual, in order to take this into account in our modeling. This kind of model is called a frailty model, taken from the clinical context, where different groups of patients may have different frailties (background risks of death).\nThere are many variants of how frailty models are run in the clinical context (see Collett (2015) for an excellent exposition of these), but the main application of a frailty model in people analytics would be to adapt a Cox proportional hazard model to take into account different background risks of the hazard event occurring among different groups in the data. This is called a shared frailty model. The frailtypack R package allows various frailty models to be run with relative ease. This is how we would run a shared frailty model on our job_retention data to take account of the different background attrition risk for the different fields of employment.\n\nlibrary(frailtypack)\n\n(frailty_model &lt;- frailtypack::frailtyPenal(\n  formula = Surv(event = left, time = month) ~ gender + \n    level + sentiment + cluster(field),\n  data = job_retention,\n  n.knots = 12, \n  kappa = 10000\n))\n\n\nBe patient. The program is computing ... \nThe program took 0.78 seconds \n\n\nCall:\nfrailtypack::frailtyPenal(formula = Surv(event = left, time = month) ~ \n    gender + level + sentiment + cluster(field), data = job_retention, \n    n.knots = 12, kappa = 10000)\n\n\n  Shared Gamma Frailty model parameter estimates  \n  using a Penalized Likelihood on the hazard function \n\n                  coef exp(coef) SE coef (H) SE coef (HIH)         z          p\ngenderM     -0.0295295  0.970902   0.0593400     0.0593400 -0.497632 6.1874e-01\nlevelLow     0.1985599  1.219645   0.0927706     0.0927706  2.140332 3.2328e-02\nlevelMedium  0.2232823  1.250174   0.1043782     0.1043782  2.139166 3.2422e-02\nsentiment   -0.1082618  0.897393   0.0143427     0.0143427 -7.548237 4.4076e-14\n\n        chisq df global p\nlevel 5.17425  2   0.0752\n\n    Frailty parameter, Theta: 48.3192 (SE (H): 33.2451 ) p = 0.073053 \n \n      penalized marginal log-likelihood = -5510.36\n      Convergence criteria: \n      parameters = 6.55e-05 likelihood = 9.89e-06 gradient = 6.43e-09 \n\n      LCV = the approximate likelihood cross-validation criterion\n            in the semi parametrical case     = 1.46587 \n\n      n= 3770\n      n events= 1354  n groups= 6\n      number of iterations:  18 \n\n      Exact number of knots used:  12 \n      Value of the smoothing parameter:  10000, DoF:  6.31\n\n\nWe can see that the frailty parameter is significant, indicating that there is sufficient difference in the background attrition risk to justify the application of a random hazard effect. We also see that the level of employment now becomes more significant in addition to sentiment, with Low and Medium level employees more likely to leave compared to High level employees.\nThe frailtyPenal() function can also be a useful way to observe the different baseline survivals for groups in the data. For example, a simple stratified Cox proportional hazard model based on sentiment category can be constructed2.\n\nstratified_base &lt;- frailtypack::frailtyPenal(\n  formula = Surv(event = left, time = month) ~ \n    strata(sentiment_category),\n  data = job_retention,\n  n.knots = 12,\n  kappa = rep(10000, 2)\n)\n\nThis can then be plotted to observe how baseline retention differs by group, as in Figure 10.33.\n\n\n\n\nplot(stratified_base, type.plot = \"Survival\", \n     pos.legend = \"topright\", Xlab = \"Month\",\n     Ylab = \"Baseline retention rate\",\n     color = 1)\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Baseline retention curves for the two sentiment categories in the job_retention data set\n\n\n\nNULL NULL",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#survival-analysis-using-python",
    "href": "survival_analysis.html#survival-analysis-using-python",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "10.4 Survival analysis using Python",
    "text": "10.4 Survival analysis using Python\nThe lifelines package in Python is designed to support survival analysis, with functions to calculate survival estimates, plot survival curves, perform Cox proportional hazard regression and check proportional hazard assumptions. A full tutorial is available here.\nHere is an example of how to plot Kaplan-Meier survival curves in Python using this chapter’s walkthrough example. The survival curves are displayed in Figure 10.4.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lifelines import KaplanMeierFitter\n\n\n# get data\nurl = \"https://peopleanalytics-regression-book.org/data/job_retention.csv\"\njob_retention = pd.read_csv(url)\n\n# fit our data to Kaplan-Meier estimates\nT = job_retention[\"month\"]\nE = job_retention[\"left\"]\nkmf = KaplanMeierFitter()\nkmf.fit(T, event_observed = E)\n# split into high and not high sentiment\nhighsent = (job_retention[\"sentiment\"] &gt;= 7)\n\n# set up plot\nsurvplot = plt.subplot()\n\n# plot high sentiment survival function\nkmf.fit(T[highsent], event_observed = E[highsent],\nlabel = \"High Sentiment\")\nkmf.plot_survival_function()\n# plot not high sentiment survival function\nkmf.fit(T[~highsent], event_observed = E[~highsent],\nlabel = \"Not High Sentiment\")\n# render plot\nkmf.plot_survival_function()\n\n\n\n\n\n\n\n\n\n\nFigure 10.4: Survival curves by sentiment category in the job_retention data\n\n\n\nAnd here is an example of how to fit a Cox Proportional Hazard model similarly to Section 10.24.\n\nfrom lifelines import CoxPHFitter\n\n# fit Cox PH model to job_retention data\ncph = CoxPHFitter()\ncph.fit(job_retention, duration_col = 'month', event_col = 'left', \n        formula = \"gender + field + level + sentiment\")\n\n\n# view results\ncph.print_summary()\n\n&lt;lifelines.CoxPHFitter: fitted with 3770 total observations, 2416 right-censored observations&gt;\n             duration col = 'month'\n                event col = 'left'\n      baseline estimation = breslow\n   number of observations = 3770\nnumber of events observed = 1354\n   partial log-likelihood = -10724.52\n         time fit was run = 2025-12-17 23:41:22 UTC\n\n---\n                            coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\ncovariate                                                                                                                   \ngender[T.M]                -0.05      0.96      0.06           -0.16            0.07                0.85                1.07\nfield[T.Finance]            0.22      1.25      0.07            0.09            0.35                1.10                1.43\nfield[T.Health]             0.28      1.32      0.13            0.03            0.53                1.03                1.70\nfield[T.Law]                0.11      1.11      0.15           -0.18            0.39                0.84                1.48\nfield[T.Public/Government]  0.11      1.12      0.09           -0.06            0.29                0.94                1.34\nfield[T.Sales/Marketing]    0.09      1.09      0.10           -0.11            0.29                0.89                1.33\nlevel[T.Low]                0.15      1.16      0.09           -0.03            0.32                0.97                1.38\nlevel[T.Medium]             0.18      1.19      0.10           -0.02            0.38                0.98                1.46\nsentiment                  -0.12      0.89      0.01           -0.14           -0.09                0.87                0.91\n\n                            cmp to     z      p  -log2(p)\ncovariate                                                \ngender[T.M]                   0.00 -0.77   0.44      1.19\nfield[T.Finance]              0.00  3.34 &lt;0.005     10.24\nfield[T.Health]               0.00  2.16   0.03      5.02\nfield[T.Law]                  0.00  0.73   0.47      1.10\nfield[T.Public/Government]    0.00  1.29   0.20      2.35\nfield[T.Sales/Marketing]      0.00  0.86   0.39      1.36\nlevel[T.Low]                  0.00  1.65   0.10      3.32\nlevel[T.Medium]               0.00  1.73   0.08      3.58\nsentiment                     0.00 -8.41 &lt;0.005     54.49\n---\nConcordance = 0.58\nPartial AIC = 21467.04\nlog-likelihood ratio test = 89.18 on 9 df\n-log2(p) of ll-ratio test = 48.58\n\n\nProportional Hazard assumptions can be checked using the check_assumptions() method5.\n\ncph.check_assumptions(job_retention, p_value_threshold = 0.05)\n\nThe ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\ncovariates will be below the threshold by chance. This is compounded when there are many covariates.\nSimilarly, when there are lots of observations, even minor deviances from the proportional hazard\nassumption will be flagged.\n\nWith that in mind, it's best to use a combination of statistical tests and visual tests to determine\nthe most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\nand looking for non-constant lines. See link [A] below for a full example.\n\n&lt;lifelines.StatisticalResult: proportional_hazard_test&gt;\n null_distribution = chi squared\ndegrees_of_freedom = 1\n             model = &lt;lifelines.CoxPHFitter: fitted with 3770 total observations, 2416 right-censored observations&gt;\n         test_name = proportional_hazard_test\n\n---\n                                 test_statistic    p  -log2(p)\nfield[T.Finance]           km              1.20 0.27      1.88\n                           rank            1.09 0.30      1.76\nfield[T.Health]            km              4.27 0.04      4.69\n                           rank            4.10 0.04      4.54\nfield[T.Law]               km              1.14 0.29      1.81\n                           rank            0.85 0.36      1.49\nfield[T.Public/Government] km              1.92 0.17      2.59\n                           rank            1.87 0.17      2.54\nfield[T.Sales/Marketing]   km              2.00 0.16      2.67\n                           rank            2.22 0.14      2.88\ngender[T.M]                km              0.41 0.52      0.94\n                           rank            0.39 0.53      0.91\nlevel[T.Low]               km              1.53 0.22      2.21\n                           rank            1.52 0.22      2.20\nlevel[T.Medium]            km              0.09 0.77      0.38\n                           rank            0.13 0.72      0.47\nsentiment                  km              2.78 0.10      3.39\n                           rank            2.32 0.13      2.97\n\n\n1. Variable 'field[T.Health]' failed the non-proportional test: p-value is 0.0387.\n\n   Advice: with so few unique values (only 2), you can include `strata=['field[T.Health]', ...]` in\nthe call in `.fit`. See documentation in link [E] below.\n\n---\n[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n\n[]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#learning-exercises",
    "href": "survival_analysis.html#learning-exercises",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "10.5 Learning exercises",
    "text": "10.5 Learning exercises\n\n10.5.1 Discussion questions\n\nDescribe some of the reasons why a survival analysis is a useful tool for analyzing data where outcome events happen at different times.\nDescribe the Kaplan-Meier survival estimate and how it is calculated.\nWhat are some common uses for survival curves in practice?\nWhy is it important to run a Cox proportional hazard model in addition to calculating survival estimates when trying to understand the effect of a given variable on survival?\nDescribe the assumption that underlies a Cox proportional hazard model and how this assumption can be checked.\nWhat is a frailty model, and why might it be useful in the context of survival analysis?\n\n\n\n10.5.2 Data exercises\nFor these exercises, use the same job_retention data set as in the walkthrough example for this chapter, which can be loaded via the peopleanalyticsdata package or downloaded from the internet6. The intention field represents a score of 1 to 10 on the individual’s intention to leave their job in the next 12 months, where 1 indicates an extremely low intention and 10 indicates an extremely high intention. This response was recorded at the beginning of the study period.\n\nCreate three categories of intention as follows: High (score of 7 or higher), Moderate (score of 4–6), Low (score of 3 or less)\nCalculate Kaplan-Meier survival estimates for the three categories and visualize these using survival curves.\nDetermine the effect of intention on retention using a Cox proportional hazard model, controlling for gender, field and level.\nPerform an appropriate check that the proportional hazard assumption holds for your model.\nRun a similar model, but this time include the sentiment input variable. How would you interpret the results?\nExperiment with running a frailty model to take into account the different background attrition risk by field of employment.\n\n\n\n\n\nCollett, David. 2015. Modelling Survival Data in Medical Research.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "survival_analysis.html#footnotes",
    "href": "survival_analysis.html#footnotes",
    "title": "10  Survival Analysis for Modeling Singular Events Over Time",
    "section": "",
    "text": "The concordance measure returned is a measure of how well the model can predict in any given pair who will survive longer and is valuable in a number of medical research contexts.↩︎\nNote there needs to be a kappa for each level of the stratification.↩︎\nThis is another route to calculating survival curves similar to Figure 10.1.↩︎\nI am not aware of any way of running frailty models currently in Python.↩︎\nSchoenfeld residual plots can be seen by setting show_plots = True in the parameters.↩︎\nhttps://peopleanalytics-regression-book.org/data/job_retention.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Survival Analysis for Modeling Singular Events Over Time</span>"
    ]
  },
  {
    "objectID": "power_tests.html",
    "href": "power_tests.html",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "",
    "text": "11.1 Errors, effect sizes and statistical power\nIn the vast majority of situations in People Analytics, researchers and analysts have limited control over the size of their samples. The most common situation is, of course, that analyses are run with whatever data can be gleaned and cleaned in the time available. At the same time, as we have seen in all of our previous work, even if a certain difference might exist in real life in the populations being studied, it is by no means certain that a specific analysis on samples from these populations will elucidate that difference. Whether that difference is visible depends on the statistical properties of the samples used. Therefore, researchers and analysts are living in the reality that when they conduct inferential analysis, the usefulness of their work depends to a very large degree on the samples they have available.\nThis suggests that a conscientious analyst using classical statistical methods would be well advised to do some up-front work to determine if their samples have a chance of yielding results that are of some inferential value. In a practical context, however, this is only partly true. Estimating required sample sizes is an imprecise science. Although the mathematical theory suggests that it should be precise, in reality we are guessing most of the inputs to the mathematics. In many cases we are so clueless about those inputs that we move into the realms of pure speculation and then produce ranges of required sample sizes that are so wide as to be fairly meaningless in practice.\nThat said, there are situations where conducting power analysis—that is, analysis of the required statistical properties of samples in order to have a certain minimum probability of observing a true difference—makes sense. Power analysis is an important element of experimental design. Experiments in people analytics usually take one of two forms:\nBoth prospective and retrospective experiments can involve a lot of work—either in setting up experiments or in extracting data from history. There is a natural question as to whether the chances of success justify the required resources and effort. Before proceeding in these cases, it is sensible to get a point of view on the likely power of the experiment and what level of sample size might be needed in order to establish a meaningful inference. For this reason, power analysis is a common component of research proposals in the medical or social sciences.\nPower analysis is a relatively blunt instrument whose primary value is to make sure that substantial effort is not being wasted on foolhardy research. If the analyst already has reasonably available data and wants to test for the effect of a certain phenomenon, the most direct approach is to just go and run the appropriate model assuming that it is relatively straightforward to do so. Power analysis should only be considered if there is clearly some substantial labor involved in the proposed modeling work.\nBefore looking at practical ways to conduct power tests on proposed experiments, let’s review an example of the logical and mathematical principles behind power testing, so that we understand what the results of power tests mean. Recall from Section 3.3 the logical mechanisms for testing hypotheses of statistical difference. Given data on samples of two groups in a population, the null hypothesis \\(H_0\\) is the hypothesis that a difference does not exist between the groups in the overall population. If the null hypothesis is rejected, we entertain the alternative hypothesis \\(H_1\\) that a difference may exist between the groups in the population.\nRecall also that we use the statistical properties of the samples to make inferences about the null hypothesis based on statistical likelihood. This means that four possible situations can occur when we run hypothesis tests:\nStatistical power refers to the fourth situation and is the probability that \\(H_0\\) is rejected given that \\(H_1\\) is true. Statistical power depends at a minimum on three criteria:\nAs an example to illustrate the mathematical relationship between these criteria, let’s assume that we run an experiment on a group of employees of size \\(n\\) where we introduce a new benefit and then test their satisfaction levels before and after its introduction. As a statistic of a random variable, we can expect the mean difference in satisfaction to have a normal distribution. Let \\(\\mu_0\\) be the mean of the population under the null hypothesis and let \\(\\mu_1\\) be the mean of the population under the alternative hypothesis. Now let’s assume that in our sample we observe a mean satisfaction of \\(\\mu^*\\) after the experiment. Recall from Chapter 3 that to meet a statistical significance standard of \\(\\alpha\\), we will need \\(\\mu^*\\) to be greater than a certain multiple of the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) above \\(\\mu_0\\) based on the normal distribution. Let’s call that multiple \\(z_{\\alpha}\\). Therefore, we can say that the statistical power of our hypothesis test is:\n\\[\n\\begin{aligned}\n\\mathrm{Power} &= P(\\mu^* &gt; \\mu_0 + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}\\vert{\\mu = \\mu_1}) \\\\\n&= P(\\frac{\\mu^* - \\mu_1}{\\frac{\\sigma}{\\sqrt{n}}} &gt; -\\frac{\\mu_1 - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} + z_{\\alpha}\\vert{\\mu = \\mu_1}) \\\\\n&= 1 - \\Phi(-\\frac{\\mu_1 - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} + z_{\\alpha}) \\\\\n&= 1 - \\Phi(-\\frac{\\mu_1 - \\mu_0}{\\sigma}\\sqrt{n} + z_{\\alpha}) \\\\\n&=  1 - \\Phi(-d\\sqrt{n} + z_{\\alpha})\n\\end{aligned}\n\\]\nwhere \\(\\Phi\\) is the cumulative normal probability distribution function, and \\(d = \\frac{\\mu_1 - \\mu_0}{\\sigma}\\) is known as Cohen’s effect size. Therefore, we can see that power depends on a measure of the observed effect size \\(d\\) between our two samples, the significance level \\(\\alpha\\) and the sample size \\(n\\)1.\nThe reader may immediately observe that many of these measures are not known at the typical point at which we would wish to do a power analysis. We can assert a minimum level of statistical power that we would wish for—usually this is somewhere between 0.8 and 0.9. We can also assert our \\(\\alpha\\). But at a point of experimental design, we usually do not know the sample size and we do not know what difference would be observed in that sample (the effect size). This implies that we are dealing with a single equation with more than one unknown, and this means that there is no unique solution2. Practically speaking, looking at ranges of values will be common in power analysis.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#errors-effect-sizes-and-statistical-power",
    "href": "power_tests.html#errors-effect-sizes-and-statistical-power",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "",
    "text": "We fail to reject \\(H_0\\), and in fact \\(H_1\\) is false. This is a good outcome.\nWe reject \\(H_0\\), but in fact \\(H_1\\) is false. This is known as a Type I error.\nWe fail to reject \\(H_0\\), but in fact \\(H_1\\) is true. This is known as a Type II error.\nWe reject \\(H_0\\), and in fact \\(H_1\\) is true. This is a good outcome and one which is most often the motivation for the hypothesis test in the first place.\n\n\n\nThe significance level \\(\\alpha\\) at which the analysis wishes to reject \\(H_0\\) (see Section 3.3). Usually \\(\\alpha = 0.05\\).\nThe size \\(n\\) of the sample being used.\nThe size of the difference observed in the sample, known as the effect size. There are numerous definitions of the effect size that depend on the specific type of power test being conducted.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#sec-simple-stats",
    "href": "power_tests.html#sec-simple-stats",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "11.2 Power analysis for simple hypothesis tests",
    "text": "11.2 Power analysis for simple hypothesis tests\nUsually we will run power analyses to get a sense of required sample sizes. Given the observations on unknowns in the previous section, we will have to assert certain possible statistical results in order to estimate required sample sizes. Most often, we will need to suggest the observed effect size in order to obtain the minimum sample size for that effect size to return a statistically significant result at a desired level of statistical power.\nUsing our example from the previous section, let’s assume that we would see a ‘medium’ effect size on our samples. Cohen’s Rule of Thumb for \\(d\\) states that \\(d = 0.2\\) is a small effect size, \\(d = 0.5\\) a medium effect size and \\(d = 0.8\\) a large effect size. We can use the wp.t() function from the WebPower package in R to do a power analysis on a paired two-sample \\(t\\)-test and return a minimum required sample size. We can assume \\(d = 0.5\\) and that we require a power of 0.8—that is, we want an 80% probability that the test will return an accurate rejection of the null hypothesis.\n\nlibrary(WebPower)\n\n# get minimum n for power of 0.8\n(n_test &lt;- WebPower::wp.t(d = 0.5, p = 0.8, type = \"paired\"))\n\nPaired t-test\n\n           n   d alpha power\n    33.36713 0.5  0.05   0.8\n\nNOTE: n is number of *pairs*\nURL: http://psychstat.org/ttest\n\n\nThis tells us that we need an absolute minimum of 34 individuals in our sample for an effect size of 0.5 to return a significant difference at an alpha of 0.05 with 80% probability. Alternatively we can test the power of a specific proposed sample size.\n\n# get power for n of 40\n(p_test &lt;- WebPower::wp.t(n1 = 40, d = 0.5, type = \"paired\"))\n\nPaired t-test\n\n     n   d alpha     power\n    40 0.5  0.05 0.8693981\n\nNOTE: n is number of *pairs*\nURL: http://psychstat.org/ttest\n\n\nThis tells us that a minimum sample size of 40 would result in a power of 0.87. A similar process can be used to plot the dependence between power and sample size under various conditions as in Figure 11.1. This is known as a power curve.\n\n\n\n\n# test a range of sample sizes\nsample_sizes &lt;- 20:100\npower &lt;- WebPower::wp.t(n1 = sample_sizes, d = 0.5, type = \"paired\")\n\nplot(power)\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Plot of power against sample size for a paired t-test\n\n\n\nWe can see a ‘sweet spot’ of approximately 40–60 minimum required participants, and a diminishing return on statistical power over and above this. Similarly we can plot a proposed minimum sample size against a range of effect sizes as in Figure 11.2.\n\n\n\n\n# test a range of effect sizes\neffect_sizes &lt;- 2:8/10\nsamples &lt;- WebPower::wp.t(n1 = rep(40, 7), \n                          d = effect_sizes, \n                          type = \"paired\")\nplot(samples$d, samples$power, type = \"b\",\n     xlab = \"Effect size\", ylab = \"Power\")\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Plot of power against effect size for a paired t-test\n\n\n\nSimilar power test variants exist for other common simple hypothesis tests. Let’s assume that we want to institute a screening test in a recruiting process, and we want to validate this test by running it on a random set of employees with the aim of proving that the test score has a significant non-zero correlation with job performance. If we assume that we will see a moderate correlation of \\(r = 0.3\\) in our sample3, we can use the wp.correlation() function in WebPower to do a power analysis, resulting in Figure 11.3.\n\n\n\n\nsample_sizes &lt;- 50:150\ncorrel_powers &lt;- WebPower::wp.correlation(n = sample_sizes, r = 0.3)\nplot(correl_powers)\n\n\n\n\n\n\n\n\n\n\nFigure 11.3: Plot of power against sample size for a correlation test\n\n\n\nFigure 11.3 informs us that we will likely want to be hitting at least 100 employees in our study to have any reasonable chance of establishing possible validity for our screening test.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#power-analysis-for-linear-regression-models",
    "href": "power_tests.html#power-analysis-for-linear-regression-models",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "11.3 Power analysis for linear regression models",
    "text": "11.3 Power analysis for linear regression models\nIn power tests of linear regression models, the effect size is a statistic of the difference in model fit between the two models being compared. Most commonly this will be a comparison of a ‘full’ fitted model involving specific input variables compared to a ‘reduced’ model with fewer input variables (often a random variance model with no input variables).\nThe \\(f^2\\) statistic is defined as follows:\n\\[\nf^2 = \\frac{R_{\\mathrm{full}}^2 - R_{\\mathrm{reduced}}^2}{1 - R_{\\mathrm{full}}^2}\n\\] where the formula refers to the \\(R^2\\) fit statistics for the two models being compared. As an example, imagine we already know that GPA in college has a significant relationship with job performance, and we wish to determine if our proposed screening test had incremental validity on top of knowing college GPA. We might run two linear regression models, one relating job performance to GPA, and another relating job performance to both GPA and screening test score. Assuming we would observe a relatively small effect size for our screening test, we assume \\(f^2 = 0.05\\)4, we can plot sample size against power in determining whether the two models are significantly different. We will also need to define the number of predictors in the full model (p1 = 2) and the reduced model (p2 = 1). The plot is shown in Figure 11.4.\n\n\n\n\nsample_sizes &lt;- 100:300\nf_sq_power &lt;- WebPower::wp.regression(n = sample_sizes, \n                                      p1 = 2, p2 = 1, f2 = 0.05)\n\nplot(f_sq_power)\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Plot of power against sample size for a small effect of a second input variable in a linear regression model",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#power-analysis-for-log-likelihood-regression-models",
    "href": "power_tests.html#power-analysis-for-log-likelihood-regression-models",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "11.4 Power analysis for log-likelihood regression models",
    "text": "11.4 Power analysis for log-likelihood regression models\nIn Chapter 5, we reviewed how measures of fit for log-likelihood models are still the subject of some debate. Given this, it is unsurprising that measures of effect size for log-likelihood models are not well established. The most well-developed current method appeared in Demidenko (2007), and works when we want to do a power test on a single input variable \\(x\\) using the Wald test on the significance of model coefficients (see Section 7.3.2 for a reminder of the Wald test).\nIn this method, the statistical power of a significance test on the input variable \\(x\\) is determined using multiple inputs as follows:\n\nThe likelihood of a positive outcome when \\(x = 0\\) is used to determine the intercept (p0 in the code below).\nThe likelihood of a positive outcome when \\(x = 1\\) is then used to determine the regression coefficient for \\(x\\) (p1 in the code below).\nA distribution for \\(x\\) is inputted (family below) and the parameters of that distribution are also entered (parameter below). For example, if the distribution is assumed to be normal then the mean and standard deviation would be entered as parameters.\nThis information is fed into the Wald test, and the power for specific sample sizes is calculated.\n\nFor example, let’s assume that we wanted to determine if our new screening test had a significant effect on promotion likelihood by running an experiment on employees who were being considered for promotion. We assume that our screening test is scored on a percentile scale and has a mean of 53 and a standard deviation of 21. We know that approximately 50% of those being considered for promotion will be promoted, and we believe that the screening test may have a small effect whereby those who score zero would still have a 40% chance of promotion and every additional point scored would increase this chance by 0.2 percentage points. We run the wp.logistic() function in WebPower to plot a power curve for various sample sizes as in Figure 11.5.\n\n\n\n\nsample_sizes &lt;- 50:2000\nlogistic_power &lt;- WebPower::wp.logistic(n = sample_sizes, \n                                        p0 = 0.4, p1 = 0.402,\n                                        family = \"normal\", \n                                        parameter = c(53, 21))\n\nplot(logistic_power)\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: Plot of power against sample size for a single input variable in logistic regression\n\n\n\nThis test suggests that we would need over 1000 individuals in our experiment in order to have at least an 80% chance of establishing the statistical significance of a true relationship between screening test score and promotion likelihood.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#power-analysis-for-hierarchical-regression-models",
    "href": "power_tests.html#power-analysis-for-hierarchical-regression-models",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "11.5 Power analysis for hierarchical regression models",
    "text": "11.5 Power analysis for hierarchical regression models\nPower tests for explicit hierarchical models usually originate from the context of the design of clinical trials, which not only concern themselves with the entire sample size of a study but also need to determine the split of that sample between treatment and control. It is rare that power analysis would need to be conducted for hierarchical models in people analytics but the technology is available in the WebPower package to explore this.\nCluster randomized trials are trials where it is not possible to allocate individuals randomly to treatment or control groups and where entire clusters have been allocated at random instead. This creates substantial additional complexity in understanding statistical power and required sample sizes. The wp.crt2arm() function in WebPower supports power analysis on 2-arm trials (treatment and control), and the wp.crt3arm() function supports power analysis on 3-arm trials (Two different treatments and a control).\nMultisite randomized trials are trials where individuals are assigned to treatment or control groups at random, but where these individuals also belong to different clusters which are important in modeling—for example, they may be members of clinical groups based on pre-existing conditions, or they may be being treated in different hospitals or outpatient facilities. Again, this makes for a substantially more complex calculation of statistical power. The wp.mrt2arm() and wp.mrt3arm() functions offer support for this.\nPower tests are also available for structural equation models. This involves comparing a more ‘complete’ structural model to a ‘subset’ model where some of the coefficients from the more ‘complete’ model are set to zero. Such power tests can be valuable when structural models have been applied previously on responses to survey instruments and there is an intention to test alternative models in the future. They can provide information on required future survey participation and response rates in order to establish whether the improved fit can be established for the alternative models.\nThere are two approaches to power tests for structural equation models, using a chi square test and a root mean squared error (RMSEA) approach. Both of these methods take a substantial number of input parameters, consistent with the complexity of structural equation model parameters and the various alternatives for measuring fit of these models. The chi square test approach is implemented by the wp.sem.chisq() function, and the RMSEA approach is implemented by the wp.sem.rmsea() function in WebPower.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#power-analysis-using-python",
    "href": "power_tests.html#power-analysis-using-python",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "11.6 Power analysis using Python",
    "text": "11.6 Power analysis using Python\nA limited set of resources for doing power analysis is available in the stats.power module of the statsmodels package. As an example, here is how we would conduct the power analysis for a paired \\(t\\)-test as in Section 11.2 above.\n\nimport math\nfrom statsmodels.stats.power import TTestPower\n\npower = TTestPower()\nn_test = power.solve_power(effect_size = 0.5,\n                           power = 0.8,\n                           alpha = 0.05)\nprint(math.ceil(n_test))\n\n34\n\n\nAnd a power curve can be constructed as in Figure 11.6.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nTTestPower().plot_power(dep_var = 'nobs',\n                              nobs = np.arange(20, 100),\n                              effect_size = np.array([0.5]),\n                              alpha = 0.05)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: Plot of power against sample size for a paired t-test\n\n\n\n\n\n\n\nDemidenko, Eugene. 2007. “Sample Size Determination for Logistic Regression Revisited.” Statistics in Medicine.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "power_tests.html#footnotes",
    "href": "power_tests.html#footnotes",
    "title": "11  Power Analysis for Estimating Required Sample Sizes for Modeling",
    "section": "",
    "text": "We will also need to know the expected distribution of the statistics that we are analyzing in order to determine the power probability.↩︎\nIn reality there are more unknowns than the mathematics would imply, due to the imperfection of what we are trying to measure—for example, measurement error and reliability will often be an unmeasurable unknown. For this reason you will often need a larger sample size than that indicated by power tests.↩︎\nCohen’s rule of thumb for correlation coefficients is Weak: 0.1, Moderate: 0.3 and Strong: 0.5.↩︎\nCohen’s rule of thumb for \\(f^2\\) effect sizes is Small: 0.02, Medium: 0.15, Large: 0.35.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power Analysis for Estimating Required Sample Sizes for Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html",
    "href": "bayesian_inference.html",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "",
    "text": "12.1 Bayes’ Theorem and the mechanics of Bayesian Inference\nAll of the methods and approaches we have seen in this book so far are based on a philosophy that repeated sampling from a population will yield information about that population. For example, in Chapter 3, we discussed how to use sample data to estimate population parameters such as means and correlation coefficients, how to construct confidence intervals around those parameter estimates and how to test hypotheses about our population parameters. In later chapters we learned how to do the same with model coefficients, allowing us to extract meaningful information from our models to help us with real-life problems.\nThe philosophy underlying this approach is that the population parameter that we seek is a single true unknown value, and that by taking repeated samples from the population and determining the frequency of occurrence of specific values for our parameter of interest, we can learn more and more about the true value of the parameter for our population. This is the basis of classical or frequentist statistics.\nIn this chapter we are going to ‘flip the script’ on this philosophy. Instead of our population parameter being a fixed unknown value which we estimate through many repeated samples of data, we are going to treat our population parameter as a random variable that can take on a range of possible values, each with some probability, and we will treat our data as fixed. This leads us to a different way of thinking about statistical inference, and provides us with a toolkit that has more flexibility than classical statistics. In particular, we can incorporate prior knowledge or beliefs about our parameters into our analyses, we can make direct probability statements about our parameters given the data we have availalable, and we can continuously update our beliefs about parameters as new data becomes available.\nThe critical theorem from probability theory that underpins this approach is Bayes’ Theorem, named after the Reverend Thomas Bayes, an 18th century statistician and theologian. As we will see, Bayes’ Theorem provides us with a mathematical way to combine our prior beliefs about a parameter with the evidence provided by our data to arrive at an updated posterior belief about the parameter. This mechanism is the essence of Bayesian inference.\nBayesian inference has become increasingly popular in recent years. There are a number of philosophical and epistemological reasons behind the recent growth in its use and application, but a key practical enabler for this growth has been the rapid development in computational technology in the early 21st century. Techniques used in Bayesian inference require simulation of data, which can demand substantially more computational resources compared to classical approaches. Only in the past two decades has widespread computational power become available that has made it more feasible to apply Bayesian methods to large datasets. Bayesian approaches are now widely used in fields such as epidemiology, econometrics, ecology, anthropology and the social sciences, to name but a small few. All of the methods that we have learned so far using classical statistics have Bayesian counterparts.\nIn People Analytics, Bayesian methods can be particularly useful when dealing with small sample sizes, when incorporating prior knowledge about employee behavior or organizational context, or when making probabilistic predictions about future outcomes. It is fair to say that not all individuals working in this field will need to have a knowledge of Bayesian inference, as classical approaches will often yield similar conclusions especially when sample sizes are larger and prior beliefs are limited. However, having a basic grasp of Bayesian methods can provide an analyst with a greater depth of understanding on how the processes they are trying to model play out statistically. For this reason, readers should regard this and subsequent chapters as optional, advanced material.\nIn this chapter we will explore the fundamental mechanics of how Bayesian inference works via Bayes’ Theorem, we will introduce how hypothesis testing can be done within the Bayesian framework, and we will outline some important concepts that we can take forward into subsequent chapters. For a more in depth treatment of Bayesian inference, Gelman et al. (2013) is highly recommended for theory and examples while McElreath (2020) is an excellent practical resource which includes R code.\nRecall that in earlier chapters we adopted a frequentist approach to statistical inference, where we treated our data as random and our parameters as fixed but unknown. In effect, this approach asks the question: Given a fixed but unknown parameter (such as a population mean), what is the probability of observing the sample data that we have collected?. Using probability notation, if we call our parameter \\(\\theta\\) and our observed data \\(D\\), we can express this question as \\(P(D \\mid \\theta)\\), which is known as the likelihood of the data given the parameter.\nIn Bayesian inference, we reverse this question to ask: Given the fixed data that we have observed, what is the probability that the parameter we are interested in has a certain value? That is, we are interested in \\(P(\\theta \\mid D)\\), which we call the posterior probability of the parameter given the data. Bayes’ Theorem provides us with a way to calculate this posterior probability by combining our prior beliefs about the parameter with the likelihood of the observed data. Bayes’ Theorem is expressed mathematically as follows:\n\\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\\]\nWhere:\nIn practice, when performing Bayesian inference, we often focus on the numerator of Bayes’ Theorem, \\(P(D \\mid \\theta) P(\\theta)\\), since the denominator \\(P(D)\\) is a normalizing constant that does not depend on \\(\\theta\\). Therefore, we can also express the posterior probability as being proportional to the product of the likelihood and the prior. That is:\n\\[P(\\theta \\mid D) \\propto P(D \\mid \\theta) P(\\theta)\\]\nLet’s consider a concrete walkthrough example to illustrate what each of these components of Bayes’ Theorem represent and to understand how Bayesian inference works. For this walkthrough, we will use basic R functions to perform the calculations and simulations, rather than relying on specialized Bayesian software packages, so that we can directly observe the mechanics of how Bayesian inference works under the hood. Once we have grasped a basic understanding of Bayesian inference, we will start to explore how we run Bayesian models with specialized software packages.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html#bayes-theorem-and-the-mechanics-of-bayesian-inference",
    "href": "bayesian_inference.html#bayes-theorem-and-the-mechanics-of-bayesian-inference",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "",
    "text": "\\(P(\\theta \\mid D)\\) is the posterior probability of the parameter \\(\\theta\\) given the data \\(D\\).\n\\(P(D \\mid \\theta)\\) is the likelihood of the data \\(D\\) given the parameter \\(\\theta\\).\n\\(P(\\theta)\\) is the prior probability of the parameter \\(\\theta\\) before observing the data.\n\\(P(D)\\) is the marginal likelihood or evidence, which is the total probability of observing the data \\(D\\) under all possible values of \\(\\theta\\). Using \\(P(D)\\) as a denominator in the formula for the posterior ensures that the posterior probabilities sum to 1 across all possible values of \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html#walkthrough-example",
    "href": "bayesian_inference.html#walkthrough-example",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "12.2 Walkthrough Example",
    "text": "12.2 Walkthrough Example\nYou have been asked to help a group of learning experts at a technology company understand the effectiveness of a new training program that they have implemented for their engineers. The training program helps the engineers prepare for a certification exam that is important for their career development. The company wants to know what proportion of engineers who complete the training program will go on to pass the certification exam. So far, only 20 engineers have completed the training program, and of those, 12 have passed the certification exam, with 8 failing the exam. However, the learning experts at the company have done market research on similar programs and their expectation is that they will see a pass rate of around 80-90% over time, though they admit that there is some uncertainty regarding this estimate.\n\n12.2.1 Defining the prior belief \\(P(\\theta)\\)\nIn Bayesian inference, we start by defining our prior belief about the parameter of interest before observing any data. In this case, our parameter of interest is the pass rate for the certification exam among engineers who complete the training program. Since our parameter involves a specified proportion of passes and fails in the certification exam, we can use a Beta distribution to represent our prior belief. A Beta distribution is a continuous probability distribution defined on the interval [0, 1], which makes it suitable for modeling proportions and probabilities.\nIf we call our pass rate parameter \\(\\theta\\), where \\(0 \\leq \\theta \\leq 1\\), we can define our prior belief about \\(\\theta\\) using a Beta distribution with shape parameters \\(\\alpha\\) and \\(\\beta\\), where \\(\\alpha\\) and \\(\\beta\\) represent the number of passes and fails respectively. That is:\n\\[\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\]\nNow we need to decide on the shape parameters \\(\\alpha\\) and \\(\\beta\\) for our prior distribution. If we have absolutely no information on the pass rate, we might choose a uniform prior with \\(\\alpha = 1\\) and \\(\\beta = 1\\), which reflects complete uncertainty about the pass rate. This kind of prior belief is called a non-informative prior. You can see the shape of this prior distribution in Figure 12.1, where it assumes that all values of \\(\\theta\\) are equally likely.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Non-informative Beta(1, 1) (uniform) prior on the distribution of the pass rate (\\(\\theta\\)) before observing any data.\n\n\n\nHowever, we do have some information from market research that suggests a high pass rate is likely, so we could choose an informative prior that reflects this belief. Based on the market research, we might choose \\(\\alpha = 8\\) and \\(\\beta = 2\\), which reflects our belief that the pass rate is likely to be high (around 80-90%), but with some uncertainty. Our informative prior distribution can therefore be expressed as:\n\\[\n\\theta \\sim \\text{Beta}(8, 2)\n\\]\nLet’s simulate and plot this prior distribution to visualize our initial beliefs about the pass rate before observing any data. First, we will take 1000 possible values of \\(\\theta\\) between 0 and 1 and simulate a Beta distribution. We can use the dbeta() function to calculate the density of the Beta distribution for different values of \\(\\theta\\).\n\n# create a vector with 1000 possible values for theta between 0 and 1\ntheta_values &lt;- seq(0, 1, length.out = 1000)\n\n# simulate the prior distribution using a Beta distribution with alpha = 8 and beta = 2\nunnormalized_prior &lt;- dbeta(theta_values, shape1 = 8, shape2 = 2)\n\nThis has provided us with the probability density function (PDF) values across our 1000 values of \\(\\theta\\). Let’s now plot our simulated prior distribution to visualize our initial beliefs about the pass rate before observing any data, as in Figure 12.2.\n\n\n\n\n# load ggplot2 for plotting\nlibrary(ggplot2)\n\n# create a data frame for plotting\nprior_df &lt;- data.frame(theta = theta_values, density = unnormalized_prior)\n\n# plot the prior distribution\nggplot(prior_df, aes(x = theta, y = density)) +\n  geom_line(color = \"blue\") +\n  geom_area(fill = \"blue\", alpha = 0.4) +\n  labs(x = \"Pass Rate (θ)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: Informative Beta(8, 2) prior distribution of pass rate (\\(\\theta\\)) before observing any data.\n\n\n\nComparing to the non-informative prior in Figure 12.1, we can see that the area has shifted significantly to the right, indicating our prior belief that pass rates are likely to be high.\nNow, before we proceed, let’s ensure we understand an important property about probability distributions and how we sample from them. There are two main types of probability distributions:\n\nProbability Mass Functions (PMFs): These are used for discrete random variables, where the variable can only take on specific, distinct values. The PMF gives the probability of the variable taking on each specific value. In this case, the probabilities for all possible values must sum to 1. The Binomial distribution that we saw in Chapter 5 is an example of a PMF.\nProbability Density Functions (PDFs): These are used for continuous random variables, where the variable can take on any value within a range. The PDF describes the relative likelihood of the variable taking on a specific value. However, because there are infinitely many possible values in a continuous range, the probability of the variable taking on any specific value is technically zero. Instead, we can calculate the probability of the variable falling within a certain interval by integrating the PDF over that interval. The total area under the PDF curve across all possible values must equal 1, but this is calculated using an integral rather than a discrete sum.\n\nWhen we sample from a PDF, we are taking discrete values, and hence we are effectively creating a PMF from the PDF. This means that if we want to have explicit probabilities for each value, we need to ensure that the sampled values sum to 1. In our case, since we are simulating the prior distribution using a continuous Beta distribution (which is a PDF), we can treat the simulated values as a PMF by normalizing them if needed. It is very simple to normalize a set of sampled values from a PDF—you just divide each value by the total sum of all values. Normalizing does not change the relative shape of the distribution, it just changes the scale to ensure that everything sums to 1.\nNormalizing your samples from a PDF is not absolutely necessary for the purposes of Bayes’ Theorem. We can always work directly with the density values as they are, since we will be normalizing the posterior distribution later in any case using \\(P(D)\\). Nevertheless, it is quite common to normalize these values so that the explicit probabilities sum to 1, and this can help with interpretation and later comparison of distributions. For this reason, we will normalize our prior distribution.\n\n# normalize the prior distribution to ensure it sums to 1\nprior_distribution &lt;- unnormalized_prior/sum(unnormalized_prior)\n\n\n\n12.2.2 Defining the likelihood \\(P(D \\mid \\theta)\\)\nNext, we need to define the likelihood of the observed data given our parameter of interest. Our observed data from the first learning program consists of 12 passes and 8 fails among the 20 engineers who completed the training program. Since our data involves counts of passes and fails, recall from Chapter 5 that we can use a Binomial distribution to model the likelihood of observing this data given different values of the pass rate \\(\\theta\\). In this case, the likelihood function can be expressed as:\n\\[\nP(D \\mid \\theta) = \\binom{20}{12} \\theta^{12}(1 - \\theta)^{8}\n\\]\nLet’s simulate and plot this likelihood function to visualize how likely our observed data is for the 1000 different values of the pass rate \\(\\theta\\). We can use the dbinom() function in R to calculate the likelihood of observing 12 passes out of 20 engineers for the different values of \\(\\theta\\).\n\n# simulate the likelihood function using a Binomial distribution\nlikelihood_function &lt;- dbinom(12, size = 20, prob = theta_values)\n\nLet’s now plot our simulated likelihood function in Figure 12.3. Note, importantly, that the likelihood function is not a probability distribution and does not need to sum to 1 across all our values of \\(\\theta\\).\n\n\n\n\n# create a data frame for plotting\nlikelihood_df &lt;- data.frame(theta = theta_values, likelihood = likelihood_function)\n\n# plot the likelihood function\nggplot(likelihood_df, aes(x = theta, y = likelihood)) +\n  geom_line(color = \"magenta\") +\n  geom_area(fill = \"magenta\", alpha = 0.4) +\n  labs(x = \"Pass Rate (θ)\",\n       y = \"Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: Likelihood of observing 12 passes out of 20 engineers for different values of pass rate (\\(\\theta\\)).\n\n\n\n\n\n12.2.3 Calculating the posterior \\(P(\\theta \\mid D)\\)\nNow that we have defined our prior belief and the likelihood of the observed data, we can use Bayes’ Theorem to calculate the posterior distribution of the pass rate \\(\\theta\\) given the observed data. Here we will take all 1000 simulated values of \\(\\theta\\) and apply Bayes’ theorem across each of them to calculate the posterior probability that \\(\\theta\\) is our population pass rate given the data we have observed. We multiply the prior distribution and the likelihood function for each value of \\(\\theta\\), and then normalize the result to ensure that the posterior distribution sums to 1:\n\n# calculate the unnormalized posterior distribution (numerator of Bayes' Theorem)\nunnormalized_posterior &lt;- prior_distribution * likelihood_function\n\n# normalize the posterior distribution (divide by total area to ensure it integates to 1)\nposterior_distribution &lt;- unnormalized_posterior / sum(unnormalized_posterior)\n\nLet’s now plot our calculated posterior distribution \\(P(\\theta \\mid D)\\) to visualize our updated beliefs about the pass rate \\(\\theta\\) after observing the data, as in Figure 12.4.\n\n\n\n\n# create a data frame for plotting\nposterior_df &lt;- data.frame(theta = theta_values, density = posterior_distribution)\n\n# plot the posterior distribution\nggplot(posterior_df, aes(x = theta, y = density)) +\n  geom_line(color = \"cyan\") +\n  geom_area(fill = \"cyan\", alpha = 0.4) +\n  labs(x = \"Pass Rate (θ)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: Posterior distribution of pass rate (\\(\\theta\\)) after observing the data.\n\n\n\nFrom the posterior distribution plot, we can see how our beliefs about the pass rate \\(\\theta\\) have been updated after observing the data, and we now expect our pass rate to most likely be lower than the prior distribution might suggest. The posterior distribution reflects both our prior beliefs and the evidence provided by the observed data. We can use this posterior distribution to make direct probabilistic statements about the pass rate given the observed data. If we normalize the likelihood function and plot our prior, likelihood and posterior together, we can see that the most likely (modal) posterior outcome (the peak of the distribution) is a pass rate of 0.68, which is a compromise between our most likely prior belief about the pass rate (0.87) and what the data tells us is the most likely pass rate (0.6), as seen in Figure 12.51.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.5: Prior, Normalized Likelihood and Posterior probabilities of pass rate (\\(\\theta\\))\n\n\n\n\n\n12.2.4 Bayesian Updating\nOne of the powerful features of Bayesian inference is the ability to continuously update our beliefs about a parameter as new data becomes available. In our example, suppose that after some time, an additional 30 engineers complete the training program, and of those, 25 pass the certification exam while 5 fail. We can use our current posterior distribution as the new prior distribution and incorporate this new data to update our beliefs about the pass rate \\(\\theta\\). A common saying in Bayesian statistics is that today’s posterior is tomorrow’s prior2.\nTo perform this Bayesian updating, we can follow the same steps as before. First, we will define our new prior distribution using the current posterior distribution. Then, we will define the likelihood of the new observed data using a Binomial distribution. Finally, we will calculate the new posterior distribution by combining the new prior and the new likelihood using Bayes’ Theorem, as in Figure 12.6.\n\n\n\n\n# new observed data\nnew_passes &lt;- 25\nnew_fails &lt;- 5\n\n# define new prior distribution using current posterior distribution\nnew_prior_distribution &lt;- posterior_distribution\n\n# simulate the likelihood function for the new observed data\nnew_likelihood_function &lt;- dbinom(new_passes, size = new_passes + new_fails, prob = theta_values)\n\n# calculate the unnormalized new posterior distribution\nunnormalized_new_posterior &lt;- new_prior_distribution * new_likelihood_function\n\n# normalize the new posterior distribution\nnew_posterior_distribution &lt;- unnormalized_new_posterior / sum(unnormalized_new_posterior)\n\n# plot the new posterior distribution\nnew_posterior_df &lt;- data.frame(theta = theta_values, density = new_posterior_distribution)\n\nggplot(new_posterior_df, aes(x = theta, y = density)) +\n  geom_line(color = \"purple\") +\n  geom_area(fill = \"purple\", alpha = 0.4) +\n  labs(x = \"Pass Rate (θ)\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.6: Posterior distribution of pass rate (\\(\\theta\\)) after updating with new data.\n\n\n\nWe see that the most likely (modal) pass rate has now increased to 0.76, reflecting the additional evidence provided by the new data. This demonstrates how Bayesian inference allows us to continuously update our beliefs about a parameter as new data becomes available.\n\n\n12.2.5 Summary statistics and credible intervals of the posterior distribution\nWe can calculate summary statistics of our posterior distribution to better state our updated beliefs about the pass rate \\(\\theta\\). For example, we can calculate the mean and median of the posterior distribution to get a sense of the central tendency of our updated beliefs. We can also calculate the mode of the posterior distribution, which represents the most likely value of \\(\\theta\\) given the observed data.\n\n# calculate the mean (expected value) of the new posterior distribution\n(mean_posterior &lt;- sum(theta_values * new_posterior_distribution))\n\n[1] 0.75\n\n# calculate the median of the new posterior distribution\n(median_posterior &lt;- theta_values[which(cumsum(new_posterior_distribution) &gt;= 0.5)[1]])\n\n[1] 0.7527528\n\n# calculate the mode of the posterior distribution\n(mode_posterior &lt;- theta_values[which.max(new_posterior_distribution)])\n\n[1] 0.7587588\n\n\nAdditionally, given that we have determined a specific distribution for the pass rate \\(\\theta\\), we can calculate credible intervals to express the uncertainty around our estimate. A credible interval is an interval within which the parameter is believed to lie with a certain probability, based on the posterior distribution. For example, a 95% credible interval for the pass rate \\(\\theta\\) would indicate that there is a 95% probability that the true pass rate lies within this interval, given the observed data. We can calculate the 95% credible interval for our updated posterior distribution using the quantile() function in R:\n\n# calculate the 95% credible interval for the updated posterior distribution\ninterval_boundaries &lt;- quantile(new_posterior_distribution, probs = c(0.025, 0.975))\n\n# Cumulative sum of probability\ncumsum_posterior &lt;- cumsum(new_posterior_distribution)\n\n# Find lower and upper bounds (approximate)\nlower_bound &lt;- theta_values[which(cumsum_posterior &gt;= 0.025)[1]]\nupper_bound &lt;- theta_values[which(cumsum_posterior &gt;= 0.975)[1]]\n\npaste0(\"95% Credible Interval: [\", round(lower_bound, 2), \", \", round(upper_bound, 2), \"]\")\n\n[1] \"95% Credible Interval: [0.63, 0.85]\"\n\n\nNote that credible intervals are quite different from the confidence intervals we have seen in previous chapters using classical statistics. A 95% confidence interval indicates that if we were to repeat our sampling process many times, 95% of the calculated confidence intervals would contain the true parameter value. In contrast, a 95% credible interval indicates that there is a 95% probability that the true parameter value lies within the interval, given the observed data and our prior beliefs. This distinction highlights one of the key differences between Bayesian and frequentist approaches to statistical inference.\n\n\n12.2.6 Computational Methods for Bayesian Inference\nIt is clear from our example above that, different from classical statistics, Bayesian inference focuses on deriving the entire posterior distribution of a parameter rather than just point estimates and confidence intervals. This requires that simulations are used to approximate the posterior distribution, and this explains why Bayesian inference is significantly more computationally demanding. In our walkthrough example, we have used one of the simpler methods of simulation known as grid approximation, where we evaluated the prior, likelihood and posterior based a grid of possible values for the parameter \\(\\theta\\). This method works well for simple models with a single parameter, but it can become computationally infeasible for more complex models with multiple parameters.\nIn practice, calculating posterior distributions analytically can be challenging, especially for complex models or large datasets. As a result, Bayesian inference often relies on specialized computational methods to simulate posterior distributions. These methods generate samples from the posterior distributions which can then be used to estimate summary statistics and credible intervals for the parameters of interest. There are several modern software packages now available providing user-friendly interfaces for specifying and estimating Bayesian models, making it easier for practitioners to apply Bayesian methods to real-world problems. We will use the stan software package3 to perform Bayesian inference in this and subsequent chapters, often via the brms package and other common R packages.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html#sec-bayes-hyp-test",
    "href": "bayesian_inference.html#sec-bayes-hyp-test",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "12.3 Bayesian hypothesis testing",
    "text": "12.3 Bayesian hypothesis testing\nIn classical statistics, we rely heavily on the \\(p\\)-value to test hypotheses. We set up a null hypothesis (usually that there is no effect or no difference) and an alternative hypothesis. If the \\(p\\)-value falls below a defined \\(\\alpha\\) (usually 0.05), we reject the null hypothesis. However, as we discussed in previous chapters, the \\(p\\)-value is often misunderstood. It tells us the frequency of observing data at least as extreme as ours over many random repeated samples, assuming the null hypothesis is true, that is \\(P(D \\mid H_0)\\). It does not tell us the probability that the hypothesis itself is true.\nUsing a Bayesian approach, we can calculate the probability of our hypotheses being true given the data we observe, that is \\(P(H \\mid D)\\). This allows us to test any defined hypothesis \\(H\\) against our data \\(D\\) and make direct statements about the probability that \\(H\\) is true.\nUsing Bayes’ theorem again, we can express this as:\n\\[\nP(H \\mid D) = \\frac{P(D \\mid H) P(H)}{P(D)}\n\\]\nWhere:\n\n\\(P(H \\mid D)\\) is the posterior probability of the hypothesis \\(H\\) given the data \\(D\\).\n\\(P(D \\mid H)\\) is the likelihood of the data \\(D\\) given the hypothesis \\(H\\).\n\\(P(H)\\) is the prior probability of the hypothesis \\(H\\) before observing the data.\n\\(P(D)\\) is the marginal likelihood or evidence, which is the total probability of observing the data \\(D\\) under all possible hypotheses—a normalizing denominator as before.\n\nNow let’s assume we have two competing hypotheses: the Null Hypothesis (\\(H_0\\)) and the Alternative Hypothesis (\\(H_1\\)). We want to compare these two hypotheses given our observed data \\(D\\). If we apply Bayes’ Theorem to both and find their quotient, we arrive at the following:\n\\[\n\\frac{P(H_1 \\mid D)}{P(H_0 \\mid D)} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)} \\times \\frac{P(H_1)}{P(H_0)}\n\\]\nThe left side of this equation is known as the posterior odds of \\(H_1\\), while the right side consists of two components: the Bayes Factor (the first term) and the prior odds (the second term). The Bayes Factor is a multiple which quantifies how much more likely the data is under one hypothesis compared to the other, while the prior odds represent our initial beliefs about the relative plausibility of the two hypotheses before observing the data. For example, if we have no prior preference for either hypothesis, the prior odds would be 1. Then if the Bayes Factor is 5, the posterior odds would also be 5, meaning that after observing the data, the Alternative Hypothesis is 5 times more likely than the Null Hypothesis.\nIf we are comparing an Alternative Hypothesis (\\(H_1\\)) against a Null Hypothesis (\\(H_0\\)), the Bayes Factor is usually denoted as \\(BF_{10}\\):\n\\[\nBF_{10} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)}\n\\]\nUsing the Bayes Factor, we can interpret the strength of evidence provided by the data in favor of one hypothesis over the other, as follows:\n\nIf \\(BF_{10} = 1\\), the data provides equal evidence for both hypotheses.\nIf \\(BF_{10} &gt; 1\\), the data favors the Alternative Hypothesis (\\(H_1\\)).\nIf \\(BF_{10} &lt; 1\\), the data favors the Null Hypothesis (\\(H_0\\)).\n\nA common rule of thumb4 for interpreting Bayes Factors which favor the Alternative Hypothesis is:\n\n1-3: Anecdotal evidence\n3-10: Moderate evidence\n10-30: Strong evidence\n&gt;30: Very strong evidence\n\nTo implement Bayesian hypothesis tests in R, we will utilize the BayesFactor package. This package allows us to perform statistical hypothesis tests within a Bayesian framework without needing to write complex custom simulations from scratch. These tests use standard minimally informed priors for the specific tests being used. To illustrate these Bayesian tests, we will use the same examples of hypothesis tests on our salespeople data set as we used in Chapter 3 for classical statistics, but now using smaller subsets of this data. Let’s download that set first.\n\n# if needed, use online url to download salespeople data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople &lt;- read.csv(url)\n\n# remove NAs\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n\n12.3.1 Bayesian \\(t\\)-test for difference in means between two independent groups\nAs in Chapter 3, we want to test if there is a difference in sales between high performing and low performing salespeople.\nWe will take random samples of 10 salespeople in the lowest performing group and 10 salespeople in the highest performing group. We will assume for our purposes in this chapter that we only have data for these 20 salespeople. Note that we set a random seed whenever we initiate a process that involves random sampling, in order to ensure that others can repeat the same randomization so that results are reproducible.\n\n# take samples of 10 people with performance ratings 1 and 4 \nset.seed(42)\nlow_perf &lt;- salespeople[salespeople$performance == 1, ]\nsample_low &lt;- low_perf[sample(nrow(low_perf), 10), ]\n\nhigh_perf &lt;- salespeople[salespeople$performance == 4, ]\nsample_high &lt;- high_perf[sample(nrow(high_perf), 10), ]\n\n# combine into a single data frame\ndf_sales &lt;- rbind(sample_low, sample_high)\n\nNext we will run a Bayesian \\(t\\)-test to compare the mean sales between the two performance groups. Our Null Hypothesis (\\(H_0\\)) is that there is no difference in mean sales between high and low performing salespeople, while our Alternative Hypothesis (\\(H_1\\)) is that there is a difference in mean sales between the two groups.\n\n# load BayesFactor package\nlibrary(BayesFactor)\n\n# run a Bayesian t-test\nttestBF(formula = sales ~ performance, data = df_sales)\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 : 3.244716 ±0.01%\n\nAgainst denominator:\n  Null, mu1-mu2 = 0 \n---\nBayes factor type: BFindepSample, JZS\n\n\nWe see that the Bayes Factor \\(BF_{10}\\) is just over 3, indicating that the data would approximately triple any prior belief we have about the likelihood of the alternative hypothesis5. This is moderate evidence in favor of the data supporting the Alternative Hypothesis (\\(H_1\\)) that there is a difference in mean sales between high and low performing salespeople.\nNote that because of the Bayes Factor, we did not even have to look at the posterior distributions of the difference in means to make this conclusion. However, we can still simulate the posterior distribution to visualize the difference in means between the two groups.\n\n# run Bayesian t-test and simulate posterior samples over 10000 points\nset.seed(42)\nbf_result &lt;- ttestBF(formula = sales ~ performance, data = df_sales)\nposterior_samples &lt;- posterior(bf_result, iterations = 10000)\n\nWe can view the first few rows of the posterior samples to see the estimated difference in sales between low and high performing salespeople.\n\nhead(posterior_samples)\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n           mu beta (1 - 4)     sig2      delta         g\n[1,] 579.5216    -228.9077 28722.91 -1.3506608 1.1093058\n[2,] 540.8704    -178.8018 19354.13 -1.2852428 2.4716510\n[3,] 488.6049    -178.0710 43802.93 -0.8508278 0.2621128\n[4,] 566.8170     -80.7543 25992.86 -0.5008856 4.1765651\n[5,] 514.4955    -318.5207 34458.29 -1.7158957 2.1923080\n[6,] 538.1033    -216.7078 23069.48 -1.4267756 1.1666705\n[7,] 493.3131    -170.4274 17088.88 -1.3037145 1.7585899\n\n\nThe key column we are interested in is the beta (1 - 4) column, which represents the difference in sales between low performing (performance = 1) and high performing (performance = 4) salespeople for each sample iteration. A negative value indicates that high performing salespeople have higher sales than low performing salespeople. We can use this data to plot our posterior as in Figure 12.7, negating our data to show a more intuitive difference between high performing and low performing salespeople.\n\n\n\n\n# create a data frame for plotting\nposterior_df &lt;- data.frame(\n  sales_diff = as.vector(-posterior_samples[ ,\"beta (1 - 4)\"])\n)\n\n# plot posterior distributions of sales for low and high performing salespeople\nggplot(posterior_df, aes(x = sales_diff)) +\n  geom_density(alpha = 0.5, color = \"blue\", fill = \"blue\") +\n  labs(x = \"Sales Difference\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.7: Posterior distribution of sales difference between high and low performing salespeople.\n\n\n\nWe can also calculate summary statistics and credible intervals for the posterior distribution of the sales difference between high and low performing salespeople, using the convenient describe_posterior() function from the bayestestR package.\n\nlibrary(bayestestR)\n(posterior_summary &lt;- describe_posterior(-posterior_samples))\n\nSummary of Posterior Distribution\n\nParameter    |    Median |                 95% CI |     pd |          ROPE | % in ROPE\n--------------------------------------------------------------------------------------\nmu           |   -525.86 | [  -608.67,   -445.46] |   100% | [-0.10, 0.10] |        0%\nbeta (1 - 4) |    156.48 | [     1.69,    321.02] | 97.63% | [-0.10, 0.10] |        0%\nsig2         | -32612.55 | [-68006.46, -17957.97] |   100% | [-0.10, 0.10] |        0%\ndelta        |      0.86 | [     0.01,      1.86] | 97.63% | [-0.10, 0.10] |     1.85%\ng            |     -0.94 | [   -26.60,     -0.12] |   100% | [-0.10, 0.10] |        0%\n\n\nThe output shows the posterior median sales difference (row beta (1 - 4)) between high and low performing salespeople is approximately 156.48, with a 95% credible interval provided. This allows us to say that there is a 95% probability that the difference in sales between high and low performing salespeople is within this range given the data that we observe. The pd column provides a ‘probability of direction’. Using this column, we can explicitly say that there is a 0.98 probability that the sales difference is greater than zero given the data we observe6. We also see a comparison with a ROPE (Region of Practical Equivalence) around zero, which is an indication of the practical range of values associated with the null hypothesis. We see that there is no overlap between the 95% credible interval and the ROPE.\nOne visual way of representing both the posterior distribution and its summary statistics is to use a halfeye plot (via the ggdist package), as in Figure 12.8. A halfeye plot combines a density plot of the posterior distribution with a point and interval summary of the distribution. By default, the point estimate is the median (this can be adjusted to mean and mode using the point_interval argument), and the intervals are the 66% credible interval and the 95% credible interval (these can be adjusted using the .width argument).\n\n\n\n\n# load ggdist package for halfeye plots\nlibrary(ggdist)\n\n# plot halfeye of posterior distribution of sales difference\nggplot(posterior_df, aes(x = sales_diff)) +\n  stat_halfeye(\n    fill = \"lightblue\",\n    color = \"blue\",\n  ) +\n  labs(x = \"Sales Difference\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.8: Halfeye plot of posterior distribution of sales difference between high and low performing salespeople.\n\n\n\nTo adapt the ttestBF() function to do a one-tailed test, you can use the nullInterval argument to specify the direction of the test. For example, if you want to test if high performing salespeople have higher sales than low performing salespeople (and noting that the default of the test is to test performance group 1 minus performance group 4), you can set nullInterval = c(0, Inf) to associate your null hypothesis with lower performers obtaining greater sales than higher performers. This will test the Alternative Hypothesis that the mean sales of high performing salespeople is greater than that of low performing salespeople.\n\n# run a one-tailed Bayesian t-test\nttestBF(formula = sales ~ performance, data = df_sales, nullInterval = c(0, Inf))\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 0&lt;d&lt;Inf    : 0.1474422 ±0%\n[2] Alt., r=0.707 !(0&lt;d&lt;Inf) : 6.34199   ±0%\n\nAgainst denominator:\n  Null, mu1-mu2 = 0 \n---\nBayes factor type: BFindepSample, JZS\n\n\nWe can see that, when our Alternative Hypothesis is that high performing salespeople have higher sales than low performing salespeople (the second hypothesis presented), the Bayes Factor \\(BF_{10}\\) is just over 6, indicating the data moderately supports this hypothesis. The opposite hypothesis (that low performing salespeople have higher sales than high performing salespeople) has a correspondingly low Bayes Factor, indicating the data provides moderate evidence against this hypothesis.\n\n\n12.3.2 Bayesian test for non-zero correlation\nNext, let’s take a look at whether there is a non-zero correlation between sales and average customer rating. As before, we will take a random sample of 10 salespeople from our data set for this test, and assume this is the only data we have available.\n\n# take a random sample of 10 salespeople\nset.seed(42)\nsample_salespeople &lt;- salespeople[sample(nrow(salespeople), 10), ]\n\nFirst, we will run a Bayesian correlation test to assess the evidence for a non-zero correlation between sales and average customer rating. Our Null Hypothesis (\\(H_0\\)) is that there is no correlation between sales and average customer rating, while our Alternative Hypothesis (\\(H_1\\)) is that there is a non-zero correlation between the two variables.\n\n# run a Bayesian correlation test\ncorrelationBF(sample_salespeople$sales, sample_salespeople$customer_rate)\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.333 : 10.14819 ±0%\n\nAgainst denominator:\n  Null, rho = 0 \n---\nBayes factor type: BFcorrelation, Jeffreys-beta*\n\n\nWe see that the Bayes Factor \\(BF_{10}\\) suggests strong evidence from the data supporting the Alternative Hypothesis \\(H_1\\). We can visualize our posterior distribution of the correlation coefficient using a halfeye plot as before.\n\n\n\n\n# run Bayesian correlation test and extract posterior samples over 10000 simulations\nset.seed(42)\nbf_corr_result &lt;- correlationBF(sample_salespeople$sales, sample_salespeople$customer_rate)\nposterior_corr_samples &lt;- posterior(bf_corr_result, iterations = 10000)\n\n# create a data frame for plotting\nposterior_corr_df &lt;- data.frame(\n  correlation = as.vector(posterior_corr_samples[ ,\"rho\"])\n)\n\n# create a halfeye plot of posterior distribution of correlation coefficient\nggplot(posterior_corr_df, aes(x = correlation)) +\n  stat_halfeye(\n    fill = \"pink\",\n    color = \"red\",\n  ) +\n  labs(x = \"Correlation Coefficient\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.9: Posterior distribution of correlation coefficient between sales and average customer rating.\n\n\n\nAnd, like before, we can view summary statistics and credible intervals for the posterior distribution of the correlation coefficient.\n\ndescribe_posterior(posterior_corr_samples)\n\nSummary of Posterior Distribution\n\nParameter | Median |       95% CI |   pd |          ROPE | % in ROPE\n--------------------------------------------------------------------\nrho       |   0.62 | [0.17, 0.86] | 100% | [-0.10, 0.10] |        0%\nzeta      |   0.72 | [0.17, 1.31] | 100% | [-0.10, 0.10] |        0%\n\n\nTherefore we are certain that there is a positive correlation between sales and average customer rating given the data we observe. We see that the 95% credible interval hs no overlap with the ROPE, further supporting our Alternative Hypothesis7.\n\n\n12.3.3 Bayesian chi-square test (Test for difference in frequency distribution between groups)\nFinally, let’s look at categorical data. We will test if there is a difference in likelihood of promotion between the performance categories of salespeople. Again we will take a random sample of 20 salespeople in each performance category (1 to 4) from our data set making a total sample of 80 salespeople, and we assume this is the only data we have available.\n\n# take samples of 10 people from each performance rating group\nset.seed(42)\n\nsample_data = data.frame()\n\nfor (i in 1:4) {\n  perf_group &lt;- salespeople[salespeople$performance == i, ]\n  sample_group &lt;- perf_group[sample(nrow(perf_group), 20), ]\n  sample_data &lt;- rbind(sample_data, sample_group)\n}\n\nIn the Bayesian framework, we test whether promotion likelihood is independent from performance category using a contingency table Bayes Factor. First we create our contingency table of promoted vs performance for our sample of 80 salespeople.\n\n# create contingency table of promoted vs performance\n(contingency &lt;- table(sample_data$promoted, sample_data$performance))\n\n   \n     1  2  3  4\n  0 15 18  9  9\n  1  5  2 11 11\n\n\nNow we can apply the hypothesis test, but before we do so we need to define the sampling method in order to determine the appropriate prior for the test to use. For this we have four choices:\n\njointMulti: Joint multinomial sampling. This means that the total sample size is fixed, but the row and column totals are free to vary. In our case this means that the total number of salespeople is fixed, but the number of promoted vs not promoted and the number of salespeople in each performance category are free to vary.\nindepMulti: Independent multinomial sampling. This means that the row totals are fixed, but the column totals are free to vary. In our case this means that the number of promoted vs not promoted salespeople is fixed, but the number of salespeople in each performance category are free to vary.\ncondMulti: Conditional multinomial sampling. This means that the column totals are fixed, but the row totals are free to vary. In our case this means that the number of salespeople in each performance category is fixed, but the number of promoted vs not promoted salespeople are free to vary.\npoisson: Poisson sampling. This means that nothing is fixed, and all counts are free to vary.\n\nThe choice you make for your sampling method requires a knowledge of the processes behind your data. In our case, we will use jointMulti, as we have a fixed total number of salespeople in our data, but we assume that promotion rate and performance ratings are not strictly controlled and can vary.\n\n# Run Bayesian Contingency Table Test\ncontingencyTableBF(contingency, sampleType = \"jointMulti\")\n\nBayes factor analysis\n--------------\n[1] Non-indep. (a=1) : 79.98032 ±0%\n\nAgainst denominator:\n  Null, independence, a = 1 \n---\nBayes factor type: BFcontingencyTable, joint multinomial\n\n\nHere we see a Bayes Factor \\(BF_{10}\\) which indicates very strong evidence from the data in favor of the Alternative Hypothesis (\\(H_1\\)) that the categories are not independent and there is a difference in promotion rates between performance rating groups. We can visualize the posterior distributions of the promotion rates using a halfeye plot, as in Figure 12.10. This requires some manipulation of the data that is produced when our posterior is simulated.\n\n\n\n\n# run Bayesian contingency table test and extract posterior samples over 10000 simulations\nset.seed(42)\nbf_contingency_result &lt;- contingencyTableBF(contingency, sampleType = \"jointMulti\")\nposterior_contingency_samples &lt;- posterior(bf_contingency_result, iterations = 10000) |&gt; \n  as.data.frame()\n\n# calculate relative promotion rates within each performance category\nfor (i in 1:4) {\n  posterior_contingency_samples[, paste0(\"pi[\", i, \"]\")] &lt;- posterior_contingency_samples[, paste0(\"pi[2,\", i, \"]\")] / \n    (posterior_contingency_samples[, paste0(\"pi[1,\", i, \"]\")] + posterior_contingency_samples[, paste0(\"pi[2,\", i, \"]\")])\n}\n  \n\n# create data frame for halfeye plot\nposterior_contingency_df &lt;- data.frame(\n  promotion_rate = c(as.vector(posterior_contingency_samples[, \"pi[1]\"]),\n                     as.vector(posterior_contingency_samples[, \"pi[2]\"]),\n                     as.vector(posterior_contingency_samples[, \"pi[3]\"]),\n                     as.vector(posterior_contingency_samples[, \"pi[4]\"])),\n  performance_rating = rep(1:4, each = nrow(posterior_contingency_samples))\n)\n\n\n# plot posterior distributions of promotion rates using halfeye plot\nggplot(posterior_contingency_df, aes(x = promotion_rate, y = as.factor(performance_rating))) +\n  stat_halfeye(\n    fill = \"lightblue\",\n    color = \"blue\"\n  ) +\n  labs(x = \"Promotion Rate\",\n       y = \"Performance Rating\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 12.10: Posterior distributions of promotion rates for each performance category.\n\n\n\nVisualizing the posteriors in this way gives us additional information regarding the promotion likelihood for each performance category. We see that the difference between performance category 2 and the higher two performance categories is quite pronounced, and is the main reason why the Bayes Factor indicates strong evidence for a difference in promotion rates between performance categories.\n\n\n12.3.4 Bayesian hypothesis testing using Python\nThe pingouin package in Python offers very simple functions for Bayes Factor estimations against a null hypothesis. These functions accept the relevant statistics (eg a \\(t\\)-statistic for a \\(t\\)-test as well as the size of both groups) and returns the Bayes Factor. For example, to calculate our \\(t\\)-test Bayes Factor as in Section 12.3.1, we would first determine the \\(t\\)-statistic for the difference in means of the two groups as follows8:\n\nimport pingouin as pg\nimport pandas as pd\n\n# load salespeople data\nurl = \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url).dropna()\n\n# take samples of 10 people with performance ratings 1 and 4\nlow_perf = salespeople[salespeople['performance'] == 1]\nsample_low = low_perf.sample(10, random_state=42)\n\nhigh_perf = salespeople[salespeople['performance'] == 4]\nsample_high = high_perf.sample(10, random_state=42)\n\n# calculate the t-statistic for the difference in means\nt_test_result = pg.ttest(sample_low['sales'], sample_high['sales'])\nt_statistic = t_test_result['T'].values[0]\n\n# calculate the Bayes Factor for the t-test\nn1 = len(sample_low)\nn2 = len(sample_high)\nbf_10 = pg.bayesfactor_ttest(t_statistic, n1, n2)\nprint(round(bf_10, 2))\n\n3.66\n\n\nAnd similarly, to calculate the Bayes Factor for the correlation test as in Section 12.3.2, we would do the following:\n\n# take a random sample of 10 salespeople\nsample_salespeople = salespeople.sample(10, random_state=42)\n\n# calculate the correlation coefficient\ncorr_result = pg.corr(sample_salespeople['sales'], sample_salespeople['customer_rate'])\n\ncorr_coefficient = corr_result['r'].values[0]\n\n# calculate the Bayes Factor for the correlation test\nn = len(sample_salespeople)\nbf_10_corr = pg.bayesfactor_pearson(corr_coefficient, n)\nprint(round(bf_10_corr, 2))\n\n4.48",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html#learning-exercises",
    "href": "bayesian_inference.html#learning-exercises",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "12.4 Learning exercises",
    "text": "12.4 Learning exercises\n\n12.4.1 Discussion questions\n\nIn classical (frequentist) statistics, the population parameter (e.g., the true mean of a population) is considered a fixed, unknown constant. How does the Bayesian approach view the population parameter differently? How does this change the way we interpret the results of an analysis?\nState Bayes’ Theorem and explain each of its components in the context of Bayesian inference.\nExplain the difference between an informative prior and a non-informative prior. Why might each be used in different scenarios? Provide an example of when you might choose one over the other.\nWhich of the three components of Bayes’ Theorem (prior, likelihood, posterior) are probability distributions? Which are not? Explain why.\n“Today’s posterior is tomorrow’s prior.” Explain this concept and how it applies to Bayesian updating.\nHow does the interpretation of a Credible Interval differ from a Confidence Interval?\nWhy does Bayesian inference allow us to make direct statements about the probability of our parameter taking specific values?\nExplain the meaning of the Bayes Factor \\(BF_{10}\\) in the context of hypothesis testing.\nIf you calculate a Bayes Factor \\(BF_{10}\\) of 12.5, how would you interpret this according to the classification scheme provided in this chapter?\nWhat is a halfeye plot and what are the key components it displays? Why are halfeye plots useful for visualizing posterior distributions?\n\n\n\n12.4.2 Data exercises\nLoad the ugtests data set which we used in Chapter 4 via the peopleanalyticsdata package or download it from the internet9. Remind yourself of what this data set contains.\nFor questions 1 to 5, focus on the Year 1 examination scores. Take a random sample of 20 students from the data set to use as your first set of observed data, and remove these from the full data. Then take another random sample of 30 students from the remaining data set to use as your second set of observed data for Bayesian updating.\n\nThe academic administrator informs you that a minimum score of 40 represents a Pass. It is their belief that ‘somewhere around 7 in 10’ students will pass the Year 1 examination. Simulate an informative prior distribution for the pass rate based on this belief. Plot the prior distribution.\nUsing your first set of observed data, simulate the likelihood function for the pass rate given the data in your first sample.\nUse your work from questions 1 and 2 to calculate the posterior distribution for the Year 1 pass rate given the data in your first sample. Plot the prior, likelihood and posterior together on a standardized plot to compare them.\nUsing the posterior distribution from question 3 as your new prior, and using your second sample of observed data, calculate the updated posterior distribution for the Year 1 pass rate. Plot the updated posterior distribution.\nCalculate summary statistics (mean, median, mode) and a 95% credible interval for the updated posterior distribution from Question 4.\n\nFor questions 6 to 10, take a random sample of 50 rows from the full ugtests data set and treat this as your observed data.\n\nTest the hypothesis that top quartile students from Year 1 score higher in Year 2 than their bottom quartile peers. Use a Bayesian \\(t\\)-test to perform this test. Report and interpret the Bayes Factor.\nVisualize the posterior distribution of the difference in Year 2 score means between the two groups using a halfeye plot.\nCalculate summary statistics and a 95% credible interval for the posterior distribution of the difference in Year 2 scores between the two groups.\nTest the hypothesis that there is a correlation between Year 2 and Final Year examination scores using a Bayesian correlation test. Report and interpret the Bayes Factor.\nVisualize the posterior distribution of the correlation coefficient using a halfeye plot. Calculate summary statistics and a 95% credible interval for the posterior distribution of the correlation coefficient. Write a summary of your findings from questions 6 to 10.\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis.\n\n\nJeffreys, Harold. 1961. The Theory of Probability (3rd Edition).\n\n\nLindley, Dennis V. 1972. Bayesian Statistics: A Review.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_inference.html#footnotes",
    "href": "bayesian_inference.html#footnotes",
    "title": "12  Bayesian Inference - A Modern Alternative to Classical Statistical Methods",
    "section": "",
    "text": "In fact, because the Beta distribution is a conjugate prior for the Binomial likelihood, we can derive the posterior distribution mathematically without needing to perform simulations. The posterior distribution will also be a Beta distribution with updated shape parameters \\(\\alpha + x\\) and \\(\\beta + n - x\\) where \\(x\\) is the number of success and \\(n-x\\) is the number of failures in the data. I leave it to the reader to verify independently that our simulated posterior closely resembles Beta(20, 10).↩︎\nLindley (1972)↩︎\nNamed after the mathematician Stanislaw Ulam, who was one of the inventors of the Monte Carlo method, a key computational method used in Bayesian statistics.↩︎\nJeffreys (1961)↩︎\nNote that the output also tell us about the standard prior distribution being used - in this case the JZS (Jeffreys-Zellner-Siow) distribution.↩︎\nThe output here is agnostic to direction, and will always return the probability of the most likely direction, akin to a one-tailed test without a specified direction.↩︎\nAs before, we can explicitly calculate the one-tailed Bayes Factor for a positive correlation by using nullInterval = c(-1, 0) in our test function↩︎\nNote that setting the same random seed in both R and Python does not produce the same randomization process, so you can expect analyses to differ between the two languages when randomization is required.↩︎\nhttps://peopleanalytics-regression-book.org/data/ugtests.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html",
    "href": "bayesian_linear_regression.html",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "",
    "text": "13.1 Applying Bayes’ Theorem to Linear Regression\nIn the previous chapter, we introduced the fundamental philosophy of Bayesian inference. We learned to view population parameters not as fixed, unknown constants to be estimated, but as random variables described by probability distributions. We explored how to combine prior beliefs with observed data to form posterior beliefs, and we applied this logic to basic hypothesis testing.\nIn this chapter, we will revisit the very first modeling method we learned about, linear regression, but this time through the lens of Bayesian inference. We will learn how to specify, fit, and interpret Bayesian linear regression models, as well as perform certain diagnostics specific to Bayesian estimation methods. As with classical linear regression, much of what we learn in this chapter will generalize to other types of Bayesian regression models, such as Bayesian logistic regression and Bayesian Poisson regression, which we will cover in the next chapter.\nWhile classical Ordinary Least Squares (OLS) regression—which we covered extensively in Chapter 4—remains a powerful tool, Bayesian linear regression offers distinct advantages which are precisely the advantages offered by Bayesian inference more generally. It allows us to incorporate prior knowledge, it functions well even with smaller sample sizes, and perhaps most importantly, it provides a rich, probabilistic interpretation of our model coefficients and predictions. On the other hand, Bayesian regression models can be computationally intensive and require careful consideration of priors and convergence diagnostics.\nTo implement Bayesian regression models in R, we will use the brms package. This package serves as an accessible interface to stan, a state-of-the-art platform for statistical modeling and high-performance statistical computation based on Markov Chain Monte Carlo (MCMC) methods. The beauty of brms is that it allows us to fit Bayesian models using syntax that is almost identical to the classic lm() and glm() functions we already know.\nRecall from Chapter 4 that the purpose of linear regression is to explain or predict an outcome measured on a continuous scale using one or more input variables. The assumption is that each observation \\(y_i\\) of the outcome variable is generated by a linear combination of the corresponding observations of the input variables \\(x_{1i}, x_{2i}, \\cdots, x_{pi}\\) plus some normally distributed error term. That is,\n\\[y_i = \\mu_i + \\epsilon_i\\]\nwhere\n\\[\n\\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\nand\nIn the classical (frequentist) framework, our goal is to find the point estimates for the \\(\\beta\\) coefficients that minimize the sum of squared errors. We assume there is one true line that best fits the data, and we calculate standard errors to construct confidence intervals around that line.\nIn the Bayesian framework, we conceptualize the problem differently. We assume that each observation \\(y_i\\) of the outcome variable comes from a normal distribution characterized by a mean \\(\\mu_i\\) and a variance \\(\\sigma^2\\):\n\\[y_i \\sim \\mathcal{N} (\\mu_i, \\sigma^2)\\]\nwhere the mean \\(\\mu_i\\) is still modeled as a linear combination of the input variable observations:\n\\[\\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\\]\nThe crucial difference is that \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) and \\(\\sigma\\) are not fixed values. They are random variables with their own distributions. Our goal is to learn the posterior distribution of each these parameters given our data and any prior beliefs we may have about them. If we use \\(\\beta\\) to represent the vector of all regression coefficients, and noting that our data consists of the outcome variable vector \\(y\\) and the matrix of input variables \\(X\\), what we are looking for is the posterior distribution \\(P(\\beta, \\sigma \\mid y, X)\\), where \\(\\sigma\\) is the standard deviation of our residuals \\(\\epsilon_i\\). In this context, Bayes’ Theorem can be restated as:\n\\[\nP(\\beta, \\sigma \\mid y, X) \\propto P(y \\mid X, \\beta, \\sigma)P(\\beta, \\sigma)\n\\]\nThis means that the output of a Bayesian regression model is not a single line or hyperplane that best fits the data, but a probability distribution of all possible lines or hyperplanes that are consistent with our data and our priors. We are simulating posteriors for multiple parameters, and as you can imagine, this will need some computational horsepower.\nCalculating the posterior distribution for complex models with multiple parameters is not possible using simple algebraic methods. To solve this, we will need to employ Markov Chain Monte Carlo (MCMC) simulation. MCMC is a class of algorithms used to sample from probability distributions. MCMC algorithms work by constructing Markov Chains, which are sequences of random steps whose long-run behavior mimics the target distribution. A Markov Chain starts from an initial point in the parameter space, and then proposes moves to new points, accepting or rejecting them according to rules that ensure the chain will eventually sample in proportion to the desired distribution. Over many iterations, the collected samples approximate the underlying distribution, enabling estimation of quantities like means, variances, and credible intervals even in high-dimensional problems.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#applying-bayes-theorem-to-linear-regression",
    "href": "bayesian_linear_regression.html#applying-bayes-theorem-to-linear-regression",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "",
    "text": "\\(\\beta_0\\) is the intercept coefficient\n\\(\\beta_1, \\beta_2, \\cdots, \\beta_p\\) are the ‘slope’ coefficients for each input variable\n\\(\\epsilon_i\\) is the error term or residual, with our residuals assumed to be normally distributed with mean 0 and variance \\(\\sigma^2\\), that is \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#running-a-bayesian-linear-regression-model",
    "href": "bayesian_linear_regression.html#running-a-bayesian-linear-regression-model",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.2 Running a Bayesian linear regression model",
    "text": "13.2 Running a Bayesian linear regression model\nWe will use a random subset of the ugtests dataset from Chapter 4, which contains information about undergraduate students’ scores over four years of their Biology degree program. Our goal is to explain the Final year score based on the Yr1,Yr2 and Yr3 scores using Bayesian linear regression. Initially, we will assume that this subset is all the data we have available for our model. As we will be running a lot of random sampling and simulations in this chapter, we will be setting random seeds throughout to ensure the calculations are reproducible.\n\n# Load necessary packages\nlibrary(brms)  \nlibrary(bayesplot)\nlibrary(ggplot2)  \n\n# if needed, download ugtests data\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests &lt;- read.csv(url)\n\n# take a random sample of 100 rows\nset.seed(123)\nsample_1 &lt;- sample(nrow(ugtests), 100)\nugtests_bayes &lt;- ugtests[sample_1, ]\n\n\n13.2.1 Specifying the model\nIn brms, we use brm() with the family = gaussian() argument to fit a Bayesian linear regression model. The syntax is very similar to lm() and glm() from our previous chapters, but with a few extra arguments regarding the Bayesian specifics. If brm() is not given any priors, it will use a set of default priors which assume we have no ingoing beliefs about any of our parameters.\nLet’s first fit a multiple linear regression model accepting the default priors. You may find that the MCMC simulations in brms models take some time to run, depending on the processing power and memory you have available as well as the choices you make about the scale of your simulation.\n\n# fit an uninformed Bayesian linear regression Model\nuninf_model &lt;- brm(\n  formula = Final ~ Yr1 + Yr2 + Yr3,\n  data = ugtests_bayes,\n  family = gaussian(), # Indicates linear regression (gaussian/normal distribution)\n  chains = 4,          # Number of independent MCMC chains \n  iter = 10000,         # Number of steps in each MCMC chain \n  refresh = 0,         # Suppress progress output (optional)\n  silent = 1,        # Reduce console output (optional)\n  save_pars = save_pars('all'), # save all parameters for later use (optional)\n  seed = 123 # set seed for reproducibility\n)\n\nLet’s review the various arguments we passed to brm():\n\nformula & data: This is the same as we use in lm().\nfamily = gaussian(): This tells stan that we are doing linear regression (assuming errors are normally distributed).\nchains: We run 4 independent MCMC simulations to ensure they all converge to the same answer.\niter: How many steps the simulation takes. The first 50% or so are usually discarded as “warm-up” iterations to learn the shape of the posterior, and the second half are kept for analysis. The more complex the model, the more iterations are usually needed to ensure convergence.\nrefresh: Controls how often the MCMC simulation progress is printed to the console. Setting to 0 suppresses this output.\nsilent: Reduces status output so that only the most critical messages appear.\nsave_pars: Saves all parameters from the model locally for later use.\nseed: Sets the random seed for reproducibility of results.\n\nIf we want to see the priors used by a brms model, we just need to use the get_prior() function:\n\n# check the priors used in the model\nget_prior(uninf_model)\n\n                     prior     class coef group resp dpar nlpar lb ub tag\n                    (flat)         b                                     \n                    (flat)         b  Yr1                                \n                    (flat)         b  Yr2                                \n                    (flat)         b  Yr3                                \n student_t(3, 146.5, 36.3) Intercept                                     \n     student_t(3, 0, 36.3)     sigma                             0       \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n\n\nThis output means that the following priors have been used in our initial model:\n\nIntercept Coefficient: A Student-\\(t\\) distribution with 3 degrees of freedom, and with the mean and scale automatically determined based on the data.\n‘Slope’ Coefficients (class = b): A Uniform distribution across all real numbers, each with equal probability. This is effectively the “flat” prior, indicating no ingoing knowledge about the coefficients.\nResidual standard deviation (sigma): A Student-\\(t\\) distribution with 3 degrees of freedom, and with the scale automatically determined based on the data. Note that the lb column for this prior contains zero. The lb and ub columns set lower and upper bounds for the prior1. Therefore in this case we actually have a half-distribution with a lower bound of zero to reflect that fact that sigma must be positive.\n\n\n\n13.2.2 Interpreting the Results\nOnce the model is fit, we can examine the results. Unlike lm(), which gives us t-statistics and p-values, brm() gives us summaries of the posterior distribution.\n\n# view summary of the model\nsummary(uninf_model)\n\n Family: gaussian \n  Links: mu = identity \nFormula: Final ~ Yr1 + Yr2 + Yr3 \n   Data: ugtests_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    40.33     16.55     7.72    72.83 1.00    24573    15080\nYr1          -0.27      0.17    -0.60     0.07 1.00    24176    14924\nYr2           0.36      0.10     0.17     0.55 1.00    24735    15626\nYr3           0.85      0.09     0.68     1.02 1.00    25017    15802\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    26.38      1.92    22.96    30.46 1.00    23058    15437\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can understand the basic summary output of estimates in a very similar way to OLS regression:\n\nThe Estimate column provides the means of the posterior distributions for each parameter. In this case:\n\nIntercept: This is the posterior mean of the final year score assuming there was a score of zero in all of the previous years.\nYr1: This is the posterior mean change in Final year score associated with a one point increase in Year 1 score, assuming no change in the other input variables.\nYr2: This is the posterior mean change in Final year score associated with a one point increase in Year 2 score, assuming no change in the other input variables.\nYr3: This is the posterior mean change in Final year score associated with a one point increase in Year 3 score, assuming no change in the other input variables.\nsigma: This is the posterior mean of the standard deviation of the residuals (the ‘noise’ in the data not explained by the model).\n\nThe Est.Error column is the standard deviation of the posterior distributions. It represents the uncertainty of our estimates.\nThe l-95% CI and u-95% CI columns provide lower and upper bounds of the 95% credible intervals for the parameters. To display a different credible interval you can use the prob argument in the summary command (e.g., prob = 0.8 for an 80% credible interval).\nThe remaining columns provide information about the MCMC simulation. In particular:\n\nRhat: The potential scale reduction factor. Values close to 1 indicate good convergence of the chains. Values significantly greater than 1 suggest that the chains have not converged well.\nBulk_ESS and Tail_ESS: The effective sample sizes. These indicate how many independent samples the MCMC simulation effectively produced in the ‘bulk’ (5-95% quantiles) of the distribution and in the tails (outside the 5-95% quantiles) . Higher values are better, with at least 1000 considered minimally sufficient. This ensures that there are sufficient samples representing random variables that are independent and identically distributed (iid).\n\n\n\n\n13.2.3 Specifying informative priors\nAlthough the default priors are generally fit for purpose in the brm() function, there are times when we may have strong prior knowledge that may cause us to adjust one or more of our priors. Imagine you are given the following information by expert faculty in the Biology department.\n\nNo scores are known to be negatively associated with final year scores.\n\nYear 1 scores are typically very weakly associated with final year scores or sometimes not at all associated.\nYear 2 and Year 3 scores are typically progressively more positively associated with final year scores.\n\nThis suggests that we could propose an informative prior for our Year 1 coefficient in our model. We can do this using the prior() function in brms. Let’s specify a normal prior for the Yr1 coefficient centered around 0 to reflect our belief that it has a weak or null effect on Final year scores. Let’s also specify a very narrow standard deviation to indicate a strong belief.\nTo assign a prior to a specific parameter in the prior() function, we use the class and coef arguments. The class argument indicates the type of parameter (b for slope coefficients, Intercept for the intercept, sigma for the residual standard deviation), while the coef argument specifies which b variable the prior applies to. If needed, upper and lower bounds can be set on classes of parameters using the ub and lb arguments respectively. For any parameters not explicitly assigned a prior, brms will use the default priors.\n\n# define informative priors on the Yr1 coefficient\npriors &lt;- c(\n  prior(normal(0, 0.05), class = b, coef = Yr1)\n)\n\n# run an informed model with this prior\ninf_model &lt;- brm(\n  formula = Final ~ Yr1 + Yr2 + Yr3,\n  data = ugtests_bayes,\n  family = gaussian(),\n  prior = priors,\n  chains = 4,\n  iter = 10000,\n  refresh = 0,\n  silent = 1,\n  save_pars = save_pars('all'),\n  seed = 123\n)\n\nLet’s view our informed model summary:\n\n# view summary of the informed prior model\nsummary(inf_model)\n\n Family: gaussian \n  Links: mu = identity \nFormula: Final ~ Yr1 + Yr2 + Yr3 \n   Data: ugtests_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    27.73     14.27    -0.63    55.64 1.00    22843    14356\nYr1          -0.02      0.05    -0.11     0.07 1.00    26315    16010\nYr2           0.37      0.10     0.18     0.56 1.00    24118    15612\nYr3           0.84      0.09     0.67     1.01 1.00    23879    15604\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    26.55      1.93    23.12    30.65 1.00    23370    16175\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe now see that our posterior estimates have shifted based on our prior beliefs. The Yr1 coefficient is now closer to zero, better reflecting our belief that it has a weak to null effect on the Final year score. Note that, although this new model has coefficients that are more aligned with our prior beliefs, this does not necessarily mean it will perform better than our uninformed model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#examining-posteriors",
    "href": "bayesian_linear_regression.html#examining-posteriors",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.3 Examining posteriors",
    "text": "13.3 Examining posteriors\n\n13.3.1 Visualizing posterior parameter distributions\nVisualization is a particularly important communication tool in Bayesian statistics, because we are communicating uncertainty about our parameters. We can plot the distributions of plausible values for our informed model coefficients using the mcmc_areas() function from the bayesplot package.\n\n\n\n\n# plot posterior distributions for coefficients\nlibrary(bayesplot)\n\nmcmc_areas(\n  inf_model, \n  pars = c(\"b_Yr1\", \"b_Yr2\", \"b_Yr3\"),\n  prob = 0.66, # 66% Credible Interval (shaded dark)\n  prob_outer = 0.95 # 95% Credible Interval (limits)\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Posterior distributions for the coefficients of our informed Bayesian linear regression model.\n\n\n\nIn Figure 13.1, the dark line at the peak of the density curve represents the most likely value for the coefficient. The width of the shaded area represents a 0.66 credible interval for the parameter, while the entire range plotted represents the 95% credible interval.\nWe can also look at the intervals explicitly using mcmc_intervals:\n\n\n\n\n# plot credible intervals for coefficients\nmcmc_intervals(\n  inf_model, \n  pars = c(\"b_Yr1\", \"b_Yr2\", \"b_Yr3\"),\n  prob = 0.66, \n  prob_outer = 0.95\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.2: 66% (dark lines) and 95% (light lines) credible intervals for the coefficients of our informed Bayesian linear regression model.\n\n\n\nBoth of these visualizations are helpful in verifying our interpretations of the coefficients. For example, we can see that the 95% credible interval for the b_Yr1 coefficient includes zero, suggesting that there is a reasonable probability that Year 1 scores have little to no effect on Final scores, consistent with our prior beliefs.\n\n\n13.3.2 Posterior predictive distribution\nBayesian regression allows us to more precisely quantify our uncertainty when predicting outcomes for new data. In OLS regression, we typically get a single predicted value (point prediction) and perhaps a prediction interval. In Bayesian analysis, we generate a full posterior predictive distribution (PPD). This distribution captures both our uncertainty about the model coefficients and the inherent variability in the data (the \\(\\sigma\\) parameter).\nFor every simulated set of coefficients in our posterior, the model generates a prediction. This results in a distribution of predicted final year scores. Let’s use our informed model to predict the final year score for an improving student, using 1000 draws from our posterior coefficients2.\n\n# define new data\nnew_student &lt;- data.frame(Yr1 = 43, Yr2 = 111, Yr3 = 143)\n\n# generate posterior predictive distribution using 1000 draws from inf_model\npost_pred &lt;- posterior_predict(inf_model, newdata = new_student, \n                               ndraws = 1000, seed = 123)\n\n# convert to data frame for plotting\npred_values &lt;- data.frame(Final = as.vector(post_pred))\n\nWe can now plot exactly what we expect this student’s final year score to look like, including our uncertainty about the model parameters and the natural noise in the data, using the halfeye plot in Figure 13.3.\n\n\n\n\nlibrary(ggdist)\n\n# plot the predictive distribution in a halfeye plot\nggplot(pred_values, aes(x = Final)) +\n  stat_halfeye(\n    .width = c(0.66, 0.95),\n    fill = \"lightblue\",\n    color = \"blue\"\n  ) +\n  labs(y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.3: Distribution of predicted Final year score for a new student\n\n\n\nNote that we can easily obtain posterior prediction intervals using the posterior_interval() function in brms. Let’s compute the 95% posterior prediction interval for this student’s final year score.\n\n# compute 95% posterior prediction interval for new data point\nposterior_interval(post_pred, prob = 0.95) |&gt; \n  round()\n\n     2.5% 97.5%\n[1,]  135   236\n\n\nThis interval tells us that, given our model and data, there is a 95% probability that this specific student’s final score will fall within this range.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#model-comparisons",
    "href": "bayesian_linear_regression.html#model-comparisons",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.4 Model Comparisons",
    "text": "13.4 Model Comparisons\nLet’s create a simple uninformed Bayesian linear regression model using just Yr2 as an input variable, so we can compare it to our full uninformed and informed models.\n\n# fit a simpler Bayesian Linear Regression Model\nsimple_model &lt;- brm(\n  formula = Final ~ Yr2,\n  data = ugtests_bayes,\n  family = gaussian(),\n  chains = 4,\n  iter = 10000,\n  refresh = 0,\n  silent = 1,\n  save_pars = save_pars('all'),\n  seed = 123\n)\n\nThere are a number of ways we can compare the fit of Bayesian linear models. Some common ways involve computing R-squareds, determining predictive accuracy, or performing hypothesis tests on model fits.\n\n13.4.1 Bayesian \\(R^2\\)\nWe can calculate a Bayesian version of \\(R^2\\). Unlike the single number in OLS regression, a set of values are drawn from our posterior coefficients, each allowing a calculation of the variance explained, and this provides a distribution for our \\(R^2\\). brms provides a convenient function bayes_R2() to compute this. Let’s use it to compare the fit of our models.\n\n\n\n\n# calculate full posterior R-squareds based on 1000 draws\nr2_uninf_full &lt;- bayes_R2(uninf_model, summary = FALSE, ndraws = 1000, seed = 123)\nr2_inf_full &lt;- bayes_R2(inf_model, summary = FALSE, ndraws = 1000, seed = 123)\nr2_simple &lt;- bayes_R2(simple_model, summary = FALSE, ndraws = 1000, seed = 123)\n\n# create data frame for visualization\nr2_df &lt;- data.frame(\n  R2 = c(r2_uninf_full[,1], r2_inf_full[,1], r2_simple[,1]),\n  Model = rep(c(\"Uninformed Full\", \"Informed Full\", \"Simple\"), each = nrow(r2_uninf_full))\n)\n\n# plot posterior r-squared for all models\nggplot(r2_df, aes(x = R2, fill = Model)) +\n  stat_halfeye(\n    .width = c(0.66, 0.95),\n    position = position_dodge(width = 0.5),\n    alpha = 0.6\n  ) +\n  labs(y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.4: Posterior distributions of Bayesian \\(R^2\\) for our three models. Note that the graph uses a ‘dodge’ position to separate the distributions vertically for clarity, and so the probability scale should not be viewed literally.\n\n\n\nIn Figure 13.4, we can see that our simple model has a very poor fit compared to both full models, which seem to be very similar in fit.\n\n\n13.4.2 LOO Cross-Validation\nWe can use Leave-One-Out Cross-Validation (LOO-CV), which estimates out-of-sample predictive accuracy, to compare the predictive fit of models.\n\n# compare models using LOO\nloo_simple &lt;- loo(simple_model, seed = 123)\nloo_informed_full &lt;- loo(inf_model, seed = 123)\nloo_uninformed_full &lt;- loo(uninf_model, seed = 123)\nloo_compare(loo_simple, loo_informed_full, loo_uninformed_full)\n\n             elpd_diff se_diff\nuninf_model    0.0       0.0  \ninf_model     -0.2       1.4  \nsimple_model -33.6       7.8  \n\n\nThe results will always rank the best performing model first, and shows the difference in expected log predictive density (ELPD) between the models. A difference of more than 4 points is generally considered meaningful, but only if the standard error of the difference is small enough (less than half the ELPD difference itself). In this case, we can see that our informed and uninformed models are virtually identical, while both full models are meaningfully better than the simple model.\n\n\n13.4.3 Hypothesis testing on model fit\nWe can test the hypothesis that a model is a better fit than another using the Bayes Factor, which we covered in Section 12.3. We can use the bayes_factor() function to compute Bayes Factors for model hypothesis tests. Let’s test that our informed full model is a better fit than our simple model.\n\n# compute Bayes Factor for model comparison\n(bayes_factor_test &lt;- bayes_factor(inf_model, simple_model, seed = 123))\n\n\n\nEstimated Bayes factor in favor of inf_model over simple_model: 101457061442159.81250\n\n\nWe can see that the Bayes Factor very strongly supports the informed full model over the simple model. Bayes Factor tests can be highly unreliable unless there has been extensive MCMC simulations performed during model fitting. It is recommended that at least 40,000 iterations are performed in model fitting to allow reliable Bayes Factor estimates. If you are unsure about the stability of your Bayes Factor estimates, you can rerun the model fitting with a higher number of iterations and see if the Bayes Factor changes significantly.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#variable-standardization-and-coefficient-hypothesis-testing",
    "href": "bayesian_linear_regression.html#variable-standardization-and-coefficient-hypothesis-testing",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.5 Variable standardization and coefficient hypothesis testing",
    "text": "13.5 Variable standardization and coefficient hypothesis testing\n\n13.5.1 Dealing with variable scale issues via standardization\nWhen our data variables are on different scales, it can complicate the interpretation of coefficients and the setting of priors. In our data, the input variable Yr1 is on a different scale to Yr2 and Yr3, and all of them are on a different scale to our outcome variable. This means that when we interpret our coefficients, the effect a one unit increase in Yr1 is not comparable with that of a one unit increase in Yr2 or Yr3. This can make it difficult to set meaningful priors for our coefficients, as the scale of the coefficients will be affected by the scale of the input variables. It can also make hypothesis testing on the coefficients more difficult, as the ROPE (Region of Practical Equivalence) may not be meaningful if the input variables are on different scales.\nExplicitly standardizing the scales of our variables before modeling can have benefits in defining our priors and in running hypothesis tests. We can standardize the scales of our variables using the scale() function in R. This function takes the mean of each variable and sets it to zero, and expresses each value as a positive or negative multiple of the standard deviation from the mean.\n\n#  standardize all variables\nlibrary(dplyr)\n\nugtests_bayes_std &lt;- ugtests_bayes |&gt; \n  mutate(across(everything(), scale))\n\nNow we can rerun our informed model on the scaled data:\n\n# fit the Bayesian Linear Regression Model on standardized data\npriors_std &lt;- c(\n  prior(normal(0, 0.05), class = b, coef = Yr1)\n)\n\ninf_model_std &lt;- brm(\n  formula = Final ~ Yr1 + Yr2 + Yr3,\n  data = ugtests_bayes_std,\n  family = gaussian(),\n  prior = priors_std,\n  chains = 4,\n  iter = 10000,\n  refresh = 0,\n  silent = 1,\n  save_pars = save_pars('all'),\n  seed = 123\n)\n\n\nsummary(inf_model_std)\n\n Family: gaussian \n  Links: mu = identity \nFormula: Final ~ Yr1 + Yr2 + Yr3 \n   Data: ugtests_bayes_std (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.07    -0.14     0.14 1.00    25032    15041\nYr1          -0.04      0.04    -0.12     0.04 1.00    24234    15898\nYr2           0.27      0.07     0.13     0.41 1.00    22593    14332\nYr3           0.70      0.07     0.56     0.84 1.00    24939    14240\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.71      0.05     0.61     0.82 1.00    21520    14650\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWorking with the standardized model has several advantages. First, the coefficients are now directly comparable to each other, since they are all expressed in terms of standard deviations. Previously, the effect of a one unit increase in Yr1 was not comparable to that of a one unit increase in Yr2, since they were on different scales. Now, the effect of a one standard deviation increase in Yr1 is directly comparable to that of a one standard deviation increase in Yr2. This makes it easier to interpret the relative importance of each input variable. However, this also means that we must be careful when interpreting the coefficients in the context of the original data. We must now refer to the effect of a change of one standard deviation rather than the effect of a change of one unit. Furthermore, predictions from our standardized model will be generated in scaled units, so we will need to back-transform them to the original scale for interpretation and practical use.\nOne of the most significant benefits of standardizing our variables is that it allows us to more safely perform hypothesis tests on the coefficients of our models, using some methods which we will now explore.\n\n\n13.5.2 Hypothesis testing on coefficients using ROPE\nWith our standardized model, we can more safely use the describe_posterior() function from the bayestestR package to get a description of the posterior distributions.\n\n# get descriptive statistics of posterior distributions\nlibrary(bayestestR)\ndescribe_posterior(inf_model_std)\n\nSummary of Posterior Distribution\n\nParameter   |    Median |        95% CI |     pd |          ROPE | % in ROPE\n----------------------------------------------------------------------------\n(Intercept) | -3.82e-04 | [-0.14, 0.14] | 50.20% | [-0.10, 0.10] |    88.97%\nYr1         |     -0.04 | [-0.12, 0.04] | 81.69% | [-0.10, 0.10] |    96.44%\nYr2         |      0.27 | [ 0.13, 0.41] | 99.98% | [-0.10, 0.10] |        0%\nYr3         |      0.70 | [ 0.56, 0.84] |   100% | [-0.10, 0.10] |        0%\n\nParameter   |  Rhat |   ESS\n---------------------------\n(Intercept) | 1.000 | 24960\nYr1         | 1.000 | 24231\nYr2         | 1.000 | 22462\nYr3         | 1.000 | 25014\n\n\nThe ROPE (Region of Practical Equivalence) is typically determined to be between -0.1 and 0.1 standard deviations of the output variable3. If the input variables are on a different scale to the output variable (as they were in our earlier modeling), the ROPE may not make sense. Because we have standardized the variables, we can be more confident that the ROPE is meaningful. This summary suggests that Yr2 and Yr3 are extremely likely to be positive predictors of Final, while Yr1 is not likely to have any predictive value. A more terse summary can be obtained by using the equivalence_test() function, which will explicitly reject coefficients that are practically different from zero:\n\n# test for equivalence of coefficients to zero\nequivalence_test(inf_model_std)\n\n# Test for Practical Equivalence\n\n  ROPE: [-0.10 0.10]\n\nParameter |        H0 | inside ROPE |       95% HDI\n---------------------------------------------------\nIntercept | Undecided |     88.97 % | [-0.14, 0.14]\nYr1       | Undecided |     96.44 % | [-0.12, 0.04]\nYr2       |  Rejected |      0.00 % |  [0.13, 0.41]\nYr3       |  Rejected |      0.00 % |  [0.56, 0.84]\n\n\n\n\n13.5.3 Hypothesis testing on coefficients using the hypothesis() function\nThe hypothesis() function can provide statistics related to a directional alternative hypothesis for each coefficient. For example, we can test if each slope coefficient is positive in our standardized model:\n\n# test alternative hypothesis that slope coefficients are greater than zero\nhypothesis(inf_model_std, hypothesis = c(\"Yr1 &gt; 0\", \"Yr2 &gt; 0\", \"Yr3 &gt; 0\"), class = \"b\")\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (Yr1) &gt; 0    -0.04      0.04    -0.10     0.03       0.22      0.18     \n2  (Yr2) &gt; 0     0.27      0.07     0.15     0.39    6665.67      1.00    *\n3  (Yr3) &gt; 0     0.70      0.07     0.59     0.82        Inf      1.00    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe evidence ratio (Evid.Ratio) is the Bayes Factor in favor of the alternative hypothesis, while the posterior probability (Post.Prob) is the probability that the alternative hypothesis is true given the data and priors. We see that two of our coefficients are virtually certain to be positive given our priors and data. However, as in Section 13.4.3, the Bayes Factor will be unreliable if not enough MCMC iterations have been generated.\n\n\n13.5.4 Testing specific hypotheses about coefficients\nWe may want to test very specific hypotheses about our coefficients, and Bayesian inference gives us a great deal of flexibility to do this. For example, if we wanted to know the probability that the Year 3 score has more than double the effect of the Year 2 score, we can do this by extracting the posterior draws and calculating the proportion of them where this condition holds.\n\n# extract posterior samples from standardized model\nposterior_samples &lt;- as_draws_df(inf_model_std, seed = 123)\n\n# calculate the probability that Yr3 &gt; 2*Yr2\nprob_Yr3_greater_Yr2 &lt;- mean(posterior_samples$b_Yr3 &gt; 2*posterior_samples$b_Yr2)\nround(prob_Yr3_greater_Yr2, 2)\n\n[1] 0.86\n\n\nThis allows us to say that there is a 86% probability that the effect of Yr3 is more than double that of Yr2, given our priors and data. Note that we can only make this statement because we are working with a model fitted to standardized data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#sec-bayes-reg-updating",
    "href": "bayesian_linear_regression.html#sec-bayes-reg-updating",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.6 Bayesian updating of regression models",
    "text": "13.6 Bayesian updating of regression models\nRecall from Section 12.2.4 that Bayesian inference allows us to update our beliefs as new data becomes available. In the context of a regression model, we can use the posterior distributions from an initial model as the priors for a new model fitted on new data. This is particularly useful in situations where data arrives sequentially or in batches. Let’s say we have a new batch of data for another 150 students. We can fit a new model using the posterior distributions from our informed model as priors.\n\n# remove our first sample from ugtests\nugtests_rm &lt;- ugtests[-sample_1, ]\n\n# take a new sample of 150 rows from remaining data\nset.seed(123)\nsample_2 &lt;- sample(nrow(ugtests_rm), 150)\nugtests_bayes_update &lt;- ugtests_rm[sample_2, ]\n\nNow we can use the function model_to_priors() from the bayestestR package to extract the posterior distributions from our previous model and use them as priors for our new model.\n\n# obtain our previous coefficent posteriors to use as priors\nlibrary(bayestestR)\nnew_priors &lt;- model_to_priors(\n  inf_model,\n  dpar = \"b\" # indicates we want priors for the coefficients\n)\n\nWe can view our new priors before we decide to use them:\n\n# view new priors extracted from previous model\nnew_priors\n\n                 prior     class coef group resp dpar nlpar lb ub tag  source\n                (flat)         b                                      default\n   normal(-0.02, 0.14)         b  Yr1                                    user\n    normal(0.37, 0.29)         b  Yr2                                 default\n    normal(0.84, 0.26)         b  Yr3                                 default\n  normal(27.73, 42.81) Intercept                                      default\n student_t(3, 0, 36.3)     sigma                             0        default\n\n\nNow we can utilize these as priors in our new model with updated new data:\n\n# fit updated model with new data and previous posteriors as priors\nupdated_model &lt;- brm(\n  formula = Final ~ Yr1 + Yr2 + Yr3,\n  data = ugtests_bayes_update,\n  family = gaussian(),\n  prior = new_priors,\n  chains = 4,\n  iter = 10000,\n  refresh = 0,\n  silent = 1,\n  save_pars = save_pars('all'),\n  seed = 123\n)\n\nNow we can compare the mean estimates of our original informed model and our updated model to see how our beliefs have changed with the new data.\n\n# compare mean fixed effects of original and updated models\ninf_model_effects &lt;- fixef(inf_model)[, \"Estimate\"]\nupdated_model_effects &lt;- fixef(updated_model)[, \"Estimate\"]\n\ncbind(inf_model_effects, updated_model_effects)\n\n          inf_model_effects updated_model_effects\nIntercept       27.72979101          13.133035333\nYr1             -0.02059574           0.002848416\nYr2              0.37042271           0.489271196\nYr3              0.83658769           0.851905429\n\n\nWe can see that our new data suggests new mean estimates for our coefficent posteriors. We can also compare our fits using one of our model comparison methods. Let’s use Bayesian \\(R^2\\) to compare the original informed model and the updated model.\n\n\n\n\n# compare original and updated models using bayes R2\nr2_updated &lt;- bayes_R2(updated_model, summary = FALSE, ndraws = 1000, seed = 123)\n\n# create data frame for visualization\nr2_update_df &lt;- data.frame(\n  R2 = c(r2_inf_full[,1], r2_updated[,1]),\n  Model = rep(c(\"Informed Original\", \"Informed Updated\"), each = nrow(r2_inf_full))\n)\n\n# plot posterior r-squared for both models\nggplot(r2_update_df, aes(x = R2, fill = Model)) +\n  stat_halfeye(\n    .width = c(0.66, 0.95),\n    alpha = 0.6\n  ) +\n  labs(y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.5: Bayesian R-squared distributions for the original informed model and the updated model after incorporating new data.\n\n\n\nWe can see from Figure 13.5 that our updated model has an improved fit, as well as a greater degree of certainty around the fit.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#model-diagnostics-and-validation",
    "href": "bayesian_linear_regression.html#model-diagnostics-and-validation",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.7 Model diagnostics and validation",
    "text": "13.7 Model diagnostics and validation\nBecause Bayesian models rely on simulations, we must verify that our predictions look like our fixed observed data. It is also critical to determine that the MCMC simulations behaved as expected and have converged properly.\n\n13.7.1 Posterior predictive checks\nA posterior predictive check (PPC) is used to determine if the data predicted by our model resembles our observed data. If our model is well specified and the simulation has worked as hoped, predictions drawn from the model’s posterior should resemble the Final column of our observed data. We can use the pp_check() function to check if this is the case.\n\n\n\n\n# posterior predictive check using 1000 posterior draws\npp_check(updated_model, ndraws = 1000) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.6: Posterior Predictive Check. The dark line (\\(y\\)) is the distribution of our actual final year data. The light blue lines (\\(y_{\\mathrm{rep}}\\)) are 1000 draws of final year distributions predicted from our model posteriors.\n\n\n\nIn Figure 13.6, we see that the distributions drawn from the posterior simulations (light blue) overlay the actual data (dark line) to within a reasonable range. This suggests our model structure (linear regression) fits the data generating process well.\n\n\n13.7.2 Trace plots\nTrace plots show the path of the MCMC sampling chains. We want to see “fuzzy caterpillars”-—-chains that mix well and oscillate around a stable mean. We do not want to see trends (going up or down) or chains that stay separate from each other. We can use the mcmc_trace() function from the bayesplot package to visualize the trace plots for our model coefficients.\n\n\n\n\n# plot trace plots for coefficients\nmcmc_trace(updated_model, pars = c(\"b_Yr1\", \"b_Yr2\", \"b_Yr3\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 13.7: Trace plots of the MCMC simulation. The four colors represent the four independent chains. The fact that they look like random noise and overlap perfectly, giving a “hairy caterpillar” appearance, indicates good convergence.\n\n\n\n\n\n13.7.3 Sensitivity to priors and other considerations\nIt is good practice to check how sensitive your results are to the choice of priors. You can do this by fitting the model with different priors (e.g., weakly informative vs. strongly informative) and comparing the posterior distributions of the coefficients. If the posteriors change significantly with different priors, it suggests that your data may not be strong enough to overcome the influence of the priors. If this is the case, it would be important to acknowledge this in your analysis and consider collecting more data if possible.\nOn the other hand, large samples can result in priors being ‘swamped’ or ‘overwhelmed’ by the data, leading to similar posterior distributions regardless of the priors used. This is not necessarily a problem, but it is important to be aware of this phenomenon when interpreting your results. In cases where priors are swamped, one of the values of Bayesian inference is reduced (namely the ability to incorporate prior knowledge). However, the other advantages of Bayesian inference (such as the rich probabilistic interpretation of coefficients and predictions) still hold. It is the choice of the analyst to decide whether Bayesian inference is appropriate in such cases.\nMany of the other model diagnostics we used in OLS regression (e.g., residual plots, tests for homoscedasticity, etc.) are still applicable in Bayesian regression. The mean of the posterior predictive distribution can be used to compute residuals, and these can be analyzed in the same way as in OLS regression to check residual distribution and homoscedasticity. As with classical models, the fitted() function can be used to extract the mean fitted values from the model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#bayesian-linear-regression-using-python",
    "href": "bayesian_linear_regression.html#bayesian-linear-regression-using-python",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.8 Bayesian linear regression using Python",
    "text": "13.8 Bayesian linear regression using Python\nThe PyMC library in Python can be used to fit Bayesian regression models. When used in combination with the Bambi package, it provides a high-level interface similar to brms in R. The arviz package can be used for visualization and diagnostics of Bayesian models.\nTo fit our uninformed Bayesian linear regression model from earlier in this chapter in Python, we first specify the model and then view it to see the default priors:\n\nimport pandas as pd\nimport pymc as pm\nimport bambi as bmb\nimport arviz as az\n\n# Load the data\nurl = \"https://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\n\n# take a random sample of 100 rows for faster computation\nugtests_bayes = ugtests.sample(n=100, random_state=123)\n\n# specify a Bayesian linear regression model with default priors\nmodel = bmb.Model(\"Final ~ Yr1 + Yr2 + Yr3\", data=ugtests_bayes, family = \"gaussian\")\nmodel.build()\nprint(model)\n\n       Formula: Final ~ Yr1 + Yr2 + Yr3\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 154.63, sigma: 626.7374)\n            Yr1 ~ Normal(mu: 0.0, sigma: 6.9113)\n            Yr2 ~ Normal(mu: 0.0, sigma: 3.6767)\n            Yr3 ~ Normal(mu: 0.0, sigma: 3.3494)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 44.2951)\n\n\nIf we wish, we can specify informative priors in the bmb.Model() constructor:\n\n# Define informative priors\npriors = {\n    \"Yr1\": bmb.Prior(\"Normal\", mu=0, sigma=0.05)\n}\n\n# Fit a Bayesian linear regression model with informative priors\nmodel_inf = bmb.Model(\"Final ~ Yr1 + Yr2 + Yr3\", data=ugtests_bayes, priors=priors)\nprint(model_inf)\n\n       Formula: Final ~ Yr1 + Yr2 + Yr3\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 154.63, sigma: 511.4135)\n            Yr1 ~ Normal(mu: 0.0, sigma: 0.05)\n            Yr2 ~ Normal(mu: 0.0, sigma: 3.6767)\n            Yr3 ~ Normal(mu: 0.0, sigma: 3.3494)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 44.2951)\n\n\nOnce we have specified our model, we can fit it using MCMC sampling:\n\n# Fit the model using MCMC sampling\nfitted = model_inf.fit(draws=10000, chains=4, random_seed=123)\n\nAnd then we can summarize the posteriors using a handy function from arviz:\n\n# Summarize the posterior distributions\naz.summary(fitted)\n\n             mean      sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\nsigma      29.353   2.131  25.422   33.323  ...    0.011   52652.0   31445.0    1.0\nIntercept  27.338  12.527   3.741   50.868  ...    0.063   70129.0   34453.0    1.0\nYr1         0.005   0.048  -0.087    0.094  ...    0.000   57849.0   33596.0    1.0\nYr2         0.284   0.100   0.093    0.469  ...    0.000   51064.0   33476.0    1.0\nYr3         0.931   0.091   0.761    1.103  ...    0.000   51219.0   34582.0    1.0\n\n[5 rows x 9 columns]\n\n\nWe can also use arviz functions to visualize the posterior distributions and trace plots as in Figure 13.7 and Figure 13.8:\n\n\n\n\n# Plot posterior distributions\naz.plot_posterior(fitted)\n\n\n\n\n\n\n\n\n\n\nFigure 13.7: Posterior distributions for the coefficients of our informed Bayesian linear regression model in Python.\n\n\n\n\n\n\n\n# Plot trace plots\naz.plot_trace(fitted)\n\n\n\n\n\n\n\n\n\n\nFigure 13.8: Trace plots of the MCMC simulation for our informed Bayesian linear regression model in Python.\n\n\n\nA posterior predictive check can also be performed using arviz, as in Figure 13.9. The predict() method generates posterior predictive samples which are added to the fitted model object, and the plot_ppc() function visualizes them against the observed data.\n\n\n\n\n# perform posterior predictive check\nmodel_inf.predict(fitted, kind=\"response\", random_seed=123)\naz.plot_ppc(fitted, num_pp_samples=1000, random_seed=123)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.9: Posterior Predictive Check for our informed Bayesian linear regression model in Python.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#learning-exercises",
    "href": "bayesian_linear_regression.html#learning-exercises",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.9 Learning exercises",
    "text": "13.9 Learning exercises\n\n13.9.1 Discussion questions\n\nWhat are “weakly informative priors”? Why might an analyst prefer these over “flat” (uninformative) priors or informative priors?\nBriefly explain the purpose of Markov Chain Monte Carlo (MCMC) simulation in Bayesian regression.\nHow does the interpretation of a regression coefficient (\\(\\beta\\)) differ between the classical regression framework and the Bayesian framework?\nWhat is a posterior predictive distribution (PPD), and how does it differ from a fitted point prediction in OLS regression?\nWhat information does the Rhat statistic provide about MCMC convergence? What values of Rhat indicate good convergence?\nWhat are three ways to compare the fit of Bayesian regression models? Briefly describe each.\nExplain how to interpret the results of a comparison between two models using Leave One Out Cross-Validation (LOO-CV).\nIn a Bayes Factor hypothesis test of regression models or model coefficients, what primarily determines how reliable the Bayes Factor estimate is?\nWhy is variable standardization helpful when defining informative priors or using a Region of Practical Equivalence (ROPE) for hypothesis testing?\nDescribe what a trace plot and a posterior predictive check (PPC) show, and what patterns (or lack thereof) indicate a healthy and well-specified model.\n\n\n\n13.9.2 Data exercises\nUse the sociological_data set from the exercises in Section 4.8. Ensure that categorical variables are coerced to factors and missing data is removed. Reduce the size of the dataset by taking a random sample of 500 rows. Ensure you set a seed to ensure you can replicate your work.\n\nFit a Bayesian linear regression model explaining annual_income_ppp using all other variables in the dataset. Use brm with chains = 4, iter = 10000, and the default priors.\nRun a summary of the model. Examine the Rhat and Bulk_ESS/Tail_ESS columns. Did the chains converge successfully?\nGenerate rrace plots for the coefficients of education_months and family_size. Do they resemble “fuzzy caterpillars”?\nVisualize the posterior distributions of the coefficients using mcmc_areas(). Is this helpful? What adjustments might you consider to make it more interpretable?\nInterpret the posterior mean and the 95% credible interval for the education_months coefficient. Can you state with at least 95% probability that education has a positive effect on income?\nImagine you possess expert knowledge from a previous study suggesting that family_size usually has a slightly negative effect on income, and work_distance generally has no effect. Construct and examine a new model using informative priors as follows:\n\nSet a normal prior for family_size with a mean of -500 and a small standard deviation.\nSet a normal prior for work_distance with a mean of 0 and a small standard deviation.\nRun an informed model using these priors.\n\nCompare your uninformed model (from Question 1) and your informed model (from Question 6) using bayes_R2(). Does the incorporation of prior beliefs significantly change the explained variance?\nUse leave-one-out cross-validation to compare the predictive performance of the two models. Is there a preferred model?\nUsing your choice of model, perform a posterior predictive check. Does the simulated data resemble the observed distribution of annual_income_ppp?\nRun your choice of model on a scaled version of your data. Determine which coefficients are practically different from zero based on the ROPE. Remember that you can only scale numeric columns in your data4.\nUsing your scaled model, calculate the specific probability that the effect of a standard deviation increase in education_months is larger than the the effect of living in Northern America (assuming no change in other variables).\nTake a further sample of 500 rows from the remaining rows of the sociological_data dataset. Fit an updated model using the posterior distributions from your (unstandardized) model of choice as priors. Compare the mean coefficient estimates of the updated model to your preferred model. How have the coefficients changed with the new data?\nCreate a hypothetical individual profile (a new dataframe) representing a male skilled worker with high education and no family living in Northern America. Use the means from your new data sample for the other variables. Using your updated model, generate the posterior predictive distribution for this individual’s income. Plot the distribution.\nReport the 90% prediction interval for this individual. Explain, in plain language, what this interval represents regarding the uncertainty of your prediction.\n\n\n\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” Advances in Methods and Practices in Psychological Science.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#footnotes",
    "href": "bayesian_linear_regression.html#footnotes",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "",
    "text": "Note that lower and upper bounds can only be set at the class level in brms and cannot be set for individual slope coefficient priors.↩︎\n‘Draws’ are a common term for samples taken from the simulated posteriors.↩︎\nKruschke (2018)↩︎\nIf you are copying the code from earlier in this chapter, you can use where(is.numeric) instead of everything() to ensure only numeric columns are scaled.↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html",
    "href": "other_bayesian_regression.html",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "",
    "text": "14.1 Bayesian binomial logistic regression\nIn the earlier chapters of this book, we dedicated significant attention to each variety of regression modeling technique, explaining the specifics of formulation, estimation, and interpretation. Moving forward, we should not need to do this for the Bayesian versions of these techniques. Given that we understand the underlying frequentist models from our earlier chapters, and now that we have a solid grasp of the principles and mechanics of Bayesian inference from the last two chapters, we can focus on how to implement these models in a Bayesian framework without delving too deeply into the foundational details again.\nIn this chapter, we will explore how to fit several other types of regression models using Bayesian methods. We will mainly focus on the mechanics of how to implement the models, and touch on any matters specific to the model type that are relevant in a Bayesian context. We will start with binomial logistic regression, and then move on to the other common logistic regression models, Poisson regression, negative binomial regression and finally we will touch on mixed effects models and Cox proportional hazards models. For each model type, we will provide code examples using the brms package in R which, as we have seen, allows for flexible Bayesian modeling using stan as the backend.\nBefore we proceed, let’s review a few technical tips to help ensure good performance from brms models in general, and to reduce the amount of time needed for these models to run. Readers may find the following helpful to avoid long wait times or crashes when running models using brms:\nIn preparation for fitting a variety of models in this chapter, we will set some default options for brms to save having to specify these in every model run.\nAs we saw in Chapter 5, binomial logistic regression is used when each observation of our outcome is a Bernoulli random variable that can take a value of 1 (e.g., ‘success’) with probability \\(p_i\\) and a value of 0 (e.g., ‘failure’) with probability \\(1-p_i\\). That is\n\\[\ny_i \\sim \\text{Bernoulli}(p_i)\n\\]\nRecall also that we model the log-odds of success as a linear function of our input variables, that is:\n\\[\n\\ln\\left(\\frac{P(y_i = 1)}{P(y_i = 0)}\\right) =  \\ln\\left(\\frac{p_i}{1-p_i}\\right) =  \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\nThis means that our slope coefficients must be interpreted in terms of changes in the log-odds of success for a one unit increase in the corresponding input variable, assuming no change in the other input variables. We learned that by exponentiating the coefficients, we can interpret them in terms of odds ratios, which represents the multiple of the odds of success associated with a unit change in the input variable, assuming no change in the other input variables.\nIn a Bayesian framework, we can fit a binomial logistic regression model using the brms package in a similar way to how we fit a linear regression model. The syntax is similar to that of frequentist logistic regression, but we specify the family as bernoulli() to explicitly indicate that each observation of our outcome is a Bernoulli random variable.\nTo illustrate, let’s use a random sample from our salespeople dataset from Chapter 5.\n# load the salespeople dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople &lt;- read.csv(url)\n\n# remove rows with NAs\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# take a random sample of 100 observations\nset.seed(123)\nsalespeople_bayes &lt;- salespeople[sample(nrow(salespeople), 100), ]\nWe are interested in how the sales, customer ratings and performance ratings of our 100 sales people affect their likelihood of promotion. We will fit a Bayesian binomial logistic regression model to this subset of data using the default flat priors.\n# fit Bayesian binomial logistic regression model\nlibrary(brms)\n\nuninf_binomial_model &lt;- brm(\n  formula = promoted ~ sales + customer_rate + performance,\n  data = salespeople_bayes,\n  family = bernoulli() # for binary outcome variable (logistic function is default link)\n)\nAfter fitting the model, we can examine the summary of the posterior distributions of the coefficients, which will represent log-odds coefficients.\nsummary(uninf_binomial_model)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: promoted ~ sales + customer_rate + performance \n   Data: salespeople_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       -28.44      8.56   -48.27   -14.81 1.00     8810     9159\nsales             0.06      0.02     0.03     0.10 1.00     7457     7339\ncustomer_rate    -1.88      1.26    -4.64     0.29 1.00     8790     8077\nperformance       1.40      0.79    -0.01     3.08 1.00    10645     9370\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nAnd we can transform these to posterior distributions of the odds ratios.\n# convert log-odds to odds ratios\n(odds_ratio_summary &lt;- fixef(uninf_binomial_model) |&gt; \n   exp())\n\n                  Estimate   Est.Error         Q2.5        Q97.5\nIntercept     4.468431e-13 5198.008019 1.092333e-21 3.711096e-07\nsales         1.057232e+00    1.017337 1.029548e+00 1.100415e+00\ncustomer_rate 1.518821e-01    3.510834 9.684400e-03 1.331130e+00\nperformance   4.060503e+00    2.203647 9.924827e-01 2.185864e+01\nWe can also visualize the posterior distributions of the odds ratios using the mcmc_areas() function from the bayesplot package. An example for the sales coefficient is shown in Figure 14.1, indicating the posterior distribution of the multiple of the odds of promotion associated with an extra thousand dollars in sales.\nPosterior predictive checks on a Bernoulli distributed outcome are not particularly intuitive unless converted into a summary statistic, such as the mean value (which is the proportion of successes). We can do this using the pp_check() function, but specifying the stat argument to indicate that we want to compare the mean in the observed data to that in the posterior predictive simulations. As can be seen in Figure 14.2, the model appears to fit the data reasonably well.\nFor a new data point, we can obtain the posterior predictive probability of success using the posterior_predict() function, which runs draws of our posterior parameters and predicts a success or failure for each draw based on our new data. For example, for a sales person with $575k in sales, an average customer rating of 3.9 and a performance rating of 3, we can determine the posterior predictive probability of promotion as follows:\n# determine posterior binary predictions new data point\nnew_data &lt;- data.frame(sales = 575, customer_rate = 3.9, performance = 3)\nposterior_preds &lt;- posterior_predict(uninf_binomial_model, newdata = new_data, \n                                     ndraws = 10000, seed = 123)\n\n# Convert to probability by taking mean\nmean(posterior_preds)\n\n[1] 0.5887",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-binomial-logistic-regression",
    "href": "other_bayesian_regression.html#bayesian-binomial-logistic-regression",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "",
    "text": "library(bayesplot)\nlibrary(ggplot2)\n\nmcmc_areas(\n  exp(as.matrix(uninf_binomial_model, pars = c(\"b_sales\"))),\n  prob = 0.66, \n  prob_outer = 0.95\n) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.1: Posterior distribution of the odds ratio for the sales coefficient in the Bayesian Binomial Logistic Regression model.\n\n\n\n\n\n\n\n\npp_check(uninf_binomial_model, type = \"stat\", ndraws = 1000, binwidth = 0.01) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: Posterior predictive check comparing the proportion of successes in the observed data (black line) to that in the posterior predictive simulations for the Bayesian Binomial Logistic Regression model (histogram).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-multinomial-logistic-regression",
    "href": "other_bayesian_regression.html#bayesian-multinomial-logistic-regression",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.2 Bayesian multinomial logistic regression",
    "text": "14.2 Bayesian multinomial logistic regression\nIn Chapter 6, we learned that multinomial logistic regression is used when the outcome variable is categorical with more than two categories that do not have a natural ordering. In this case, we model the log-odds of each category relative to a reference category as a linear function of our input variables. In a Bayesian framework, we can fit a multinomial logistic regression model using the brms package by specifying the family as categorical(). As usual we will take a sample from our health_insurance dataset from Chapter 6 to illustrate.\n\n# load the health insurance dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/health_insurance.csv\"\nhealth_insurance &lt;- read.csv(url)\n\n# take a sample of 100 observations\nset.seed(123)\nhealth_insurance_bayes &lt;- health_insurance[sample(nrow(health_insurance), 100), ]\n\n# fit Bayesian Multinomial Logistic Regression model with flat priors\nuninf_multinom_model &lt;- brm(\n  formula = product ~ age + household + position_level + gender + absent,\n  data = health_insurance_bayes,\n  family = categorical() # for multinomial outcome variable\n)\n\nNow we can view our summary.\n\nsummary(uninf_multinom_model)\n\n Family: categorical \n  Links: muB = logit; muC = logit \nFormula: product ~ age + household + position_level + gender + absent \n   Data: health_insurance_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmuB_Intercept        -11.23      3.55   -18.83    -4.87 1.00    10576    11510\nmuC_Intercept        -14.10      3.76   -22.40    -7.63 1.00    11670    11872\nmuB_age                0.54      0.12     0.34     0.80 1.00     7463     8822\nmuB_household         -2.39      0.56    -3.60    -1.40 1.00    10997    11884\nmuB_position_level    -0.39      0.54    -1.49     0.63 1.00    14824    12659\nmuB_genderMale        -4.04      1.35    -6.82    -1.54 1.00    12420    13286\nmuB_absent            -0.02      0.08    -0.17     0.13 1.00    14711    13102\nmuC_age                0.44      0.10     0.27     0.67 1.00     7668     8776\nmuC_household         -0.03      0.30    -0.60     0.58 1.00    13866    13238\nmuC_position_level    -0.26      0.43    -1.16     0.55 1.00    15988    13855\nmuC_genderMale        -1.26      1.07    -3.47     0.74 1.00    14399    14483\nmuC_absent            -0.04      0.07    -0.17     0.09 1.00    14367    12143\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see two sets of posterior coefficient distributions, one giving the log odds of a choice of product B over product A (beginning muB), and the other giving the log odds of a choice of product C over product A (beginning muC). As before, we can exponentiate these coefficients to obtain odds ratios.\nOur posterior predictions will be in the form of integers representing the three product categories (by default this will be A=1, B=2, C=3). To do a posterior predictive check, we can use the pp_bars() function to view the proportion of each product choice in the observed data compared to that in the posterior predictive simulations, as shown in Figure 14.3.\n\n\n\n\n# convert observations to numeric\ny_observed &lt;- as.numeric(as.factor(health_insurance_bayes$product))\n\n# get 1000 posterior predictions\ny_sim &lt;- posterior_predict(uninf_multinom_model, ndraws = 1000, seed = 123)\n\n# posterior predictive check showing proportions for each product\nppc_bars(y_observed, y_sim) +\n  scale_x_continuous(breaks = c(1, 2, 3), labels = c(\"A\", \"B\", \"C\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.3: Posterior predictive check comparing the proportions of each product choice in the observed data (light blue bars) to those in the posterior predictions (dark lines) for the Bayesian Multinomial Logistic Regression model.\n\n\n\nWe see that the model fits the data very well, with the median predictions closely matching the observed data.\nGiven new data for an individual, we can calculate the probability of the choice of each product using our posterior draws.\n\n# new data point\nnew_data &lt;- data.frame(\n  age = 35,\n  household = 3,\n  position_level = 3,\n  gender = \"Male\",\n  absent = 2\n)\n\n# posterior predictions for new data point\nposterior_preds &lt;- posterior_predict(uninf_multinom_model, newdata = new_data, \n                                     ndraws = 10000, seed = 123) |&gt; \n  factor(labels = c(\"A\", \"B\", \"C\"))\n\n# convert to probabilities of each product choice\n(product_probs &lt;- prop.table(table(posterior_preds)))\n\nposterior_preds\n     A      B      C \n0.6328 0.0181 0.3491 \n\n\nSince this model sees our first appearance of categorical input variables in the Bayesian modeling framework, it is worth making a note that brms will automatically create dummies for these variables, just as we saw in the lm() and glm() functions for frequentist regression models. If we wish to set an informed prior for the coefficient of a specific categorical input variable, we will need to specify the dummy variable associated with our prior. To determine the correct way to specify the prior, we can first look at all our priors in the model:\n\nget_prior(uninf_multinom_model)\n\n                prior     class           coef group resp dpar nlpar lb ub tag\n               (flat)         b                            muB                \n               (flat)         b         absent             muB                \n               (flat)         b            age             muB                \n               (flat)         b     genderMale             muB                \n               (flat)         b      household             muB                \n               (flat)         b position_level             muB                \n student_t(3, 0, 2.5) Intercept                            muB                \n               (flat)         b                            muC                \n               (flat)         b         absent             muC                \n               (flat)         b            age             muC                \n               (flat)         b     genderMale             muC                \n               (flat)         b      household             muC                \n               (flat)         b position_level             muC                \n student_t(3, 0, 2.5) Intercept                            muC                \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n\n\nWe can use this syntax to specify our informed prior. As an example, if we had information that being male is associated with a 20% increase in the odds of choosing product B over product A, we would set a prior for the b coefficient of genderMale in the muB set of parameters. We would specify the mean on the log-odds scale as log(1.2), and we might set a standard deviation of log(0.1) to reflect a strong belief about this effect. This prior would be specified as follows:\n\n# set a prior for the effect of being male on the log-odds of choosing product B over product A\npriors &lt;- c(\n  prior(normal(log(1.2), log(0.1)), class = \"b\", coef = \"genderMale\", dpar = \"muB\")\n)\n\nWe could then refit the model with this prior specified using the prior argument in the brm() function.\n\n14.2.1 Bayesian ordinal logistic regression\nIn Chapter 7, we learned that ordinal logistic regression is used when the outcome variable is categorical with more than two categories that have a natural ordering. In this case, we model the log-odds of being in a category less than or equal to a certain level as a linear function of our input variables. In particular, we focused on the proportional odds model, which assumes that the effect of each input variable is the same across all thresholds of the outcome variable, allowing us to have a single coefficient for each input variable to indicate the strength of effect on the log-odds of being in a higher category in general. We also learned that it is essential to check the proportional odds assumption when fitting a proportional odds logistic regression model, as violation of this assumption can lead to false inferences from the model.\nIn a Bayesian framework, we can fit a proportional odds logistic regression model using the brms package by specifying the family as cumulative(), representing a cumulative logistic model. As usual we will take a sample from our soccer dataset from Chapter 7 to illustrate. We are aiming to estimate the effect of numerous factors on the discliplinary record of a player in a game - a three level ordinal outcome: “None” for no discipline, “Yellow” for a formal warning and “Red” for a sending-off. As with the polr() function for the frequentist proportional odds logistic regression model, brms will expect us to ensure our outcome variable is an ordinal factor.\n\n# load the soccer dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer &lt;- read.csv(url)\n\n# take a sample of 100 observations\nset.seed(123)\nsoccer_bayes &lt;- soccer[sample(nrow(soccer), 100), ]\n\n# convert discipline to an ordered factor\nsoccer_bayes$discipline &lt;- ordered(\n  soccer_bayes$discipline,\n  levels = c(\"None\", \"Yellow\", \"Red\")\n)\n\n# fit Bayesian proportional odds logistic regression model with flat priors\nuninf_polr_model &lt;- brm(\n  formula = discipline ~ .,\n  data = soccer_bayes,\n  family = cumulative() # for ordinal outcome variable\n)\n\nNow we can view our summary.\n\nsummary(uninf_polr_model)\n\n Family: cumulative \n  Links: mu = logit \nFormula: discipline ~ n_yellow_25 + n_red_25 + position + result + country + level \n   Data: soccer_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]       4.42      1.32     1.91     7.13 1.00    17186    13646\nIntercept[2]       6.04      1.40     3.40     8.90 1.00    16741    14059\nn_yellow_25        0.39      0.18     0.05     0.74 1.00    23889    16660\nn_red_25           0.76      0.25     0.29     1.28 1.00    18895    13963\npositionM          1.22      0.66    -0.00     2.56 1.00    14877    13512\npositionS          0.63      0.85    -1.03     2.32 1.00    15380    14959\nresultL            1.25      0.64     0.04     2.52 1.00    16640    14769\nresultW            0.26      0.62    -0.96     1.50 1.00    17452    15415\ncountryGermany     0.54      0.49    -0.41     1.52 1.00    23871    15069\nlevel             -0.54      0.50    -1.51     0.43 1.00    22289    16376\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log-odds of being in a higher discipline category.\nIn a Bayesian regression framework, an effective way to test the proportional odds assumption is to run an alternative model that does not assume proportional odds, and then compare the two models using information criteria such as from leave-one-out cross-validation (LOO-CV) . If the non-proportional odds model has a significantly better fit according to these criteria, it suggests that the proportional odds assumption may be violated. A common alternative model for ordinal regression is an adjacent category model. Here is how we can fit an adjacent category ordinal model in brms.\n\n# fit Bayesian adjacent category logistic regression model with flat priors\nuninf_acat_model &lt;- brm(\n  formula = discipline ~ .,\n  data = soccer_bayes,\n  family = acat() # for adjacent category ordinal outcome variable\n)\n\nNow we can compare the fit using LOO:\n\n# compare LOO for both models\nloo_polr &lt;- loo(uninf_polr_model, seed = 123)\nloo_acat &lt;- loo(uninf_acat_model, seed = 123)\nloo_compare(loo_polr, loo_acat)\n\n                 elpd_diff se_diff\nuninf_polr_model  0.0       0.0   \nuninf_acat_model -1.2       1.8   \n\n\nThe LOO comparison suggests that there is no meaningful difference between the models, so we can be reasonably confident that the proportional odds assumption holds in this case.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-poisson-and-negative-binomial-regression",
    "href": "other_bayesian_regression.html#bayesian-poisson-and-negative-binomial-regression",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.3 Bayesian Poisson and negative binomial regression",
    "text": "14.3 Bayesian Poisson and negative binomial regression\nIn Chapter 8, we learned that Poisson regression is used when the outcome variable is a count of events that occur in a fixed period of time or space. We model the log of the expected count as a linear function of our input variables, to ensure that the count outcomes are always non-negative. This means that when we exponentiate the coefficients of our model, we see that the effects of the input variables are multiplicative on the count outcome. We also learned that Poisson regression assumes that the mean and variance of the outcome variable are equal, which may not hold in practice. When the variance exceeds the mean, we have overdispersion, which can lead to underestimated standard errors and misleading inferences from the model. In such cases, negative binomial regression can be used as an alternative, as it includes an additional shape parameter \\(\\theta\\) to account for overdispersion.\nIn a Bayesian framework, we can fit a Poisson regression model using the brms package by specifying the family as poisson(). As usual we will take a sample from our absenteeism dataset from Chapter 8 to illustrate.\n\n# load the absenteeism dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/absenteeism.csv\"\nabsenteeism &lt;- read.csv(url)\n\n# take a sample of 100 observations\nset.seed(123)\nabsenteeism_bayes &lt;- absenteeism[sample(nrow(absenteeism), 100), ]\n\n# fit Bayesian Poisson regression model with flat priors\nuninf_poisson_model &lt;- brm(\n  formula = days_absent ~ tenure + is_manager + performance_rating,\n  data = absenteeism_bayes,\n  family = poisson() # for count outcome variable\n)\n\nWe can now inspect the model summary.\n\nsummary(uninf_poisson_model)\n\n Family: poisson \n  Links: mu = log \nFormula: days_absent ~ tenure + is_manager + performance_rating \n   Data: absenteeism_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              1.21      0.18     0.86     1.55 1.00    19132    15803\ntenure                 0.05      0.00     0.05     0.06 1.00    18164    15843\nis_manager             0.01      0.09    -0.17     0.19 1.00    18816    14270\nperformance_rating     0.10      0.04     0.01     0.19 1.00    19624    14897\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log of the expected count of days absent. By exponentiating these coefficient distributions, we can interpret them as multiplicative effects on the expected count.\nWe can do a posterior predictive check in the usual way as in Figure 14.4.\n\n\n\n\npp_check(uninf_poisson_model, ndraws = 1000) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.4: Posterior predictive check for the Bayesian Poisson regression model.\n\n\n\nWe can see some issues with our predictive fit. This suggests that we may have overdispersion in our data. We can address this by fitting a negative binomial regression model instead, which we can do in brms by specifying the family as negbinomial().\n\n# fit Bayesian negative binomial regression model with flat priors\nuninf_negbin_model &lt;- brm(\n  formula = days_absent ~ tenure + is_manager + performance_rating,\n  data = absenteeism_bayes,\n  family = negbinomial() # for count outcome variable with overdispersion\n)\n\nWe can now inspect the model summary.\n\nsummary(uninf_negbin_model)\n\n Family: negbinomial \n  Links: mu = log \nFormula: days_absent ~ tenure + is_manager + performance_rating \n   Data: absenteeism_bayes (Number of observations: 100) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              1.22      0.50     0.22     2.21 1.00    24895    14617\ntenure                 0.05      0.01     0.04     0.07 1.00    22186    15271\nis_manager             0.03      0.33    -0.59     0.70 1.00    24454    14768\nperformance_rating     0.09      0.13    -0.17     0.36 1.00    24358    14950\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.55      0.27     1.09     2.13 1.00    21232    15738\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can see that our estimated errors have increased, leading to more conservative inferences, and we see a posterior distribution for the shape parameter \\(\\theta\\), with the low values indicating substantial overdispersion. We can do another posterior predictive check to verify if the model fits the data better as in Figure 14.5.\n\n\n\n\npp_check(uninf_negbin_model, ndraws = 1000) +\n  xlim(0, 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.5: Posterior predictive check for the Bayesian negative binomial regression model.\n\n\n\nFinally, we can also use LOO-CV to formally compare the fit of the two models, which establishes a substantially superior fit for the negative binomial model.\n\n# compare LOO for both models\nloo_poisson &lt;- loo(uninf_poisson_model, seed = 123)\nloo_negbin &lt;- loo(uninf_negbin_model, seed = 123)\nloo_compare(loo_poisson, loo_negbin)\n\n                    elpd_diff se_diff\nuninf_negbin_model     0.0       0.0 \nuninf_poisson_model -211.6      41.7 \n\n\nNote that zero-inflated versions of both Poisson and negative binomial regression models can also be fit in brms by specifying the families as zero_inflated_poisson() and zero_inflated_negbinomial() respectively. These models will generate the posterior of an additional zero-inflation parameter zi, representing the probability that a zero observation is an “excess zero”.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-mixed-effects-models",
    "href": "other_bayesian_regression.html#bayesian-mixed-effects-models",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.4 Bayesian Mixed Effects Models",
    "text": "14.4 Bayesian Mixed Effects Models\nIn Chapter 9, we learned that mixed effects models (also known as multilevel or hierarchical models) are used when our data has a hierarchical structure, such as students nested within schools or employees nested within departments. These models allow us to account for the non-independence of observations within groups by including random effects that capture the variability between groups. In a Bayesian framework, we can fit mixed effects models using the brms package by specifying random effects in the model formula using the (1 | group_variable) syntax which we learned in Chapter 9. There is no need to specify a different family for mixed effects models in bems; we simply include the random effects in the formula and apply the appropriate model family for the outcome variable type.\nTo illustrate, let’s use a random sample from our speed_dating dataset from Chapter 9 and run a Bayesian binomial mixed effects model to understand how the various factors considered by individuals over multiple speed dates relate to the overall binary decision they made about their dates.\n\n# Load the speed dating dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/speed_dating.csv\"\nspeed_dating &lt;- read.csv(url)\n\n# remove rows with NAs\nspeed_dating &lt;- speed_dating[complete.cases(speed_dating), ]\n\n# take a sample of 1000 observations\nset.seed(123)\nspeed_dating_bayes &lt;- speed_dating[sample(nrow(speed_dating), 1000), ]\n\n# Fit Bayesian Binomial Logistic Mixed Effects model with flat priors\n\nuninf_mixed_binomial_model &lt;- brm(\n  formula = dec ~ agediff + samerace + attr + intel + prob + (1 | iid),\n  data = speed_dating_bayes,\n  family = bernoulli(), # for binary outcome variable\n  save_pars = save_pars('all', group = TRUE) # save group-level parameters\n)\n\nNow we can view our summary.\n\nsummary(uninf_mixed_binomial_model)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: dec ~ agediff + samerace + attr + intel + prob + (1 | iid) \n   Data: speed_dating_bayes (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~iid (Number of levels: 449) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     2.95      0.44     2.17     3.91 1.00     3157     6004\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -14.32      1.92   -18.50   -10.93 1.00     3973     6472\nagediff      -0.03      0.05    -0.13     0.07 1.00    12761    14227\nsamerace     -0.01      0.30    -0.59     0.59 1.00    12586    13611\nattr          1.36      0.16     1.08     1.69 1.00     5088     8137\nintel         0.16      0.13    -0.07     0.42 1.00     8002     9609\nprob          0.78      0.12     0.57     1.02 1.00     4607     7221\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see the posterior coefficient distributions for each fixed effect, representing the effect of a unit change in that variable on the log-odds of a positive decision about a date. We can also see the estimated standard deviation of the random intercepts for each individual under the Multilevel Hyperparameters, which captures the variability in baseline log-odds of a positive decision across individuals. A posterior predictive check can be done in the same way as for a standard Bayesian binomial logistic regression model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-cox-proportional-hazards-models",
    "href": "other_bayesian_regression.html#bayesian-cox-proportional-hazards-models",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.5 Bayesian Cox proportional hazards models",
    "text": "14.5 Bayesian Cox proportional hazards models\nIn Chapter 10, we learned that Cox proportional hazards models are used in survival analysis to model an event that occurs over time, such as employee turnover or customer churn. These models estimate the hazard function, which represents the instantaneous risk of the event occurring at a given time, conditional on survival up to that time. The Cox model assumes that the hazard ratios between individuals are proportional over time, allowing us to estimate the effect of input variables on the hazard function without specifying the baseline hazard function.\nIn a Bayesian framework, we can fit Cox proportional hazards models using the brms package by specifying the family as cox(). If your data contains censored observations, the model formula should take the general form time | cens(1 - event) ~ x1 + x2 + ..., where time is the variable indicating the time of the observation and event is the binary variable indicating if the event had occurred. As usual we will take a sample from our job_retention dataset from Chapter 10 to illustrate.\n\n# load the job retention dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/job_retention.csv\"\njob_retention &lt;- read.csv(url)\n\n# take a sample of 1000 observations\nset.seed(123)\njob_retention_bayes &lt;- job_retention[sample(nrow(job_retention), 1000), ]\n\n# fit Bayesian Cox Proportional Hazards model with flat priors\nuninf_cox_model &lt;- brm(\n  formula = month | cens(1 - left) ~ gender + \n    field + level + sentiment,\n  data = job_retention_bayes,\n  family = cox() # for survival outcome variable\n)\n\nWe can now view the model summary.\n\nsummary(uninf_cox_model)\n\n Family: cox \n  Links: mu = log \nFormula: month | cens(1 - left) ~ gender + field + level + sentiment \n   Data: job_retention_bayes (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  0.54      0.40    -0.17     1.43 1.01      811\ngenderM                   -0.06      0.12    -0.29     0.16 1.00    10233\nfieldFinance               0.06      0.13    -0.20     0.31 1.00     8743\nfieldHealth                0.17      0.27    -0.37     0.67 1.01     1474\nfieldLaw                   0.03      0.33    -0.66     0.63 1.00     6705\nfieldPublicDGovernment     0.25      0.16    -0.07     0.57 1.00    10167\nfieldSalesDMarketing      -0.02      0.21    -0.43     0.38 1.00     7380\nlevelLow                  -0.08      0.17    -0.40     0.26 1.00     7261\nlevelMedium                0.01      0.19    -0.37     0.39 1.00     2783\nsentiment                 -0.13      0.03    -0.18    -0.07 1.00     4703\n                       Tail_ESS\nIntercept                   257\ngenderM                   10642\nfieldFinance               9481\nfieldHealth                 975\nfieldLaw                   9548\nfieldPublicDGovernment     9734\nfieldSalesDMarketing       8846\nlevelLow                   7792\nlevelMedium                3098\nsentiment                  7053\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log-odds of employee attrition, assuming no change in the other input variables. By exponentiating these coefficient distributions, we can interpret them as adds ratios describing the multiple on the attrition hazard associated with a unit change in the input variable assuming no change in the other input variables.\nNote that it is not currently possible to do posterior predictive checks for Cox proportional hazards models in brms, due to the complexity of the survival outcome and censoring mechanism. However, we can assess the fit of the model using other methods, such as examining the estimated survival curves for different groups or using LOO-CV to estimate the expected log predictive density (ELPD).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#learning-exercises",
    "href": "other_bayesian_regression.html#learning-exercises",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.7 Learning exercises",
    "text": "14.7 Learning exercises\n\n14.7.1 Discussion questions\n\nWhat is the correct family to specify in brms for fitting a Bayesian binomial logistic regression model with a binary outcome variable?\nWhat statistic is most appropriate for a posterior predictive check for a Bayesian binomial logistic regression model?\nWhat is the most appropriate way to do a posterior predictive check for a Bayesian multinomial logistic regression model?\nWhat is the correct family to specify for fitting a Bayesian proportional odds logistic regression model in brms?\nHow can we test the proportional odds assumption for a Bayesian proportional odds logistic regression model using brms?\nDescribe some ways of diagnosing overdispersion when fitting a Bayesian Poisson regression model.\nDescribe how to set informed priors for categorical input variables in a Bayesian regression model in brms.\nIf you are modeling a censored survival outcome using a Bayesian Cox proportional hazards model in brms, how would you write the outcome variable in your model formula?\n\n\n\n14.7.2 Data exercises\nFor each of the earlier (frequentist) chapters on the methods outlined in this chapter, use the problem and dataset provided in the exercises at the end of each chapter.\n\nGenerate two smaller versions of the dataset by taking random samples of the rows - aim for around 10-20% of the original observations in each dataset. Don’t forget to remove your first sample before taking the second sample.\nFit a Bayesian version of the appropriate regression model using the first dataset and flat priors.\nVisualize and examine the posterior distributions of the coefficients and interpret them in the context of the problem.\nFor each model, conduct appropriate posterior predictive checks to assess the fit of the model to the data. Comment on the results. Are you comfortable that the model is a good fit?\nIf you are concerned about the fit, experiment with alternative model specifications to improve the fit. Justify your choices and comment on the results.\nEnsure you check any underlying assumptions of your model type (eg proportional odds for ordinal logistic regression, equidispersion for Poisson regression) and take appropriate action if any assumptions are violated.\nExperiment with setting informed priors for one or two coefficients. Refit the model with these priors and compare the results to the model with flat priors.\nExperiment with generating posterior predictions for new data points. Comment on the results.\nUsing the approach described in Section 13.6, update your model with the second dataset.\nExamine the updated posteriors and perform an updated posterior predictive check to assess the fit of the updated model to the new data. Comment on the results.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "causal_inference.html",
    "href": "causal_inference.html",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "",
    "text": "15.1 Underlying theoretical frameworks for causal inference\nThis book is about statistics, and over many previous chapters we have covered numerous classical and modern approaches to explanatory statistical modeling. Historically, statisticians have been careful to view such models as descriptive tools for summarizing associations in data, rather than as tools for making causal claims. In our previous chapters, when interpreting the output of models, our language has been very intentional in relation to this. We have used language rooted in description and association, such as ‘x has an effect on y’ or ‘a change in x is associated with a change in y’. We have deliberately avoided using phrases with action verbs like ‘a change in x causes a change in y’ or ‘a change in x drives a change in y’. This caution reflects a long-standing principle in statistics, and one which has become a very popular refrain over the years: association (or correlation) does not imply causation.\nIn reality, however, most of us who practice statistics do not employ our toolkit and skillset to simply describe associations. We want to understand the dynamic mechanisms of the problem we are interested in, and often that means we want to understand whether causal relationships exist. For example, we may want to know if a new employee benefit has caused an increase in employee satisfaction, or if a change in the structure of an education program has caused better learning outcomes. Moreover, when our leaders and clients want us to help them make decisions, they often have questions like ‘Why is this happening?’ or ‘What would happen if…?’. Answering questions like these requires inferring causality. Realistically, if an analyst is simply not willing to have an opinion on questions of causality, their skills may be of limited value to decision-makers.\nBut inferring causality is hard and requires a lot of intellectual discipline. In the modeling examples and exercises in previous chapters of this book, we did not have this discipline in mind. Our focus was on the theory, execution and direct interpretation of statistical methods. In our quest for associations, we threw many variables into models without much thought about the role each variable might play in the causal mechanics of the problem we were trying to solve. As we will learn in this chapter, this is not a good practice when our goal is to infer causality. Causal inference requires careful thinking about the data generating process, the role of confounding variables, and the assumptions we are making when we use statistical models to find evidence to support or disregard a causal hypothesis.\nOver the last few decades there has been a growing body of work in statistics, psychology, clinical medicine and related fields that has developed rigorous frameworks and methods for causal inference. This chapter introduces some of these key concepts and methods with a focus on how they can be applied in practice. We will spend the first part of the chapter introducing the fundamental theories underlying causal inference, and then we will look at the toolkit of methods that can be used to infer causality from observational data. Finally, we will use a walkthrough example to illustrate how this toolkit can be applied in practice.\nThe modern field of causal inference has been primarily shaped by two complementary frameworks. Throughout this chapter, we will draw on the strengths of both.\nWhile historically there has been some friction between these two philosophies, they are now largely seen as two sides of the same coin. The RCM provides the rigorous definition of a causal effect, while the SCM give us the tools to think about the complex system of variables in which that effect is embedded. In this chapter, we will use the RCM to understand what we’re trying to estimate and we will use the SCM to figure out how to estimate it from observational data. For a more in-depth treatment of the Rubin causal model, see Imbens and Rubin (2015). For a more in-depth treatment of the Structural causal model, see Pearl, Glymour, and Jewell (2016) or Pearl and Mackenzie (2019). Brumback (2022) and McElreath (2020) are also excellent resources for learning about how to apply causal inference techniques in settings involving observational data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#underlying-theoretical-frameworks-for-causal-inference",
    "href": "causal_inference.html#underlying-theoretical-frameworks-for-causal-inference",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "",
    "text": "The Rubin causal model (RCM or Potential Outcomes framework): Popularized by Donald Rubin1, this model is rooted in the language of experiments. It forces us to think about the “potential” state of the world under different scenarios. For each individual, we imagine two potential outcomes: the outcome if they receive a “treatment” (e.g., attending a training program) and the outcome if they receive the “control” (e.g., not attending). The causal effect is the difference between these two potential outcomes. The core difficulty, which we will explore, is that we can only ever observe one of these outcomes for any given individual. This is what is called the Fundamental Problem of Causal Inference2. This framework is incredibly useful for defining what a causal effect is with mathematical precision.\nThe structural causal model (SCM): Championed by Judea Pearl3, this framework uses the intuitive language of graphs to represent our causal assumptions and hypotheses. Through using Directed Acyclic Graphs (DAGs), we can visualize the assumed data-generating process. By drawing nodes (variables) and arrows (causal effects), we create a causal map of the system we are studying. This map then provides us with a clear, visual guide for how to analyze our data. It tells us which variables we must control for, which we must not control for, and whether our causal question is even answerable with the data we have.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#the-rubin-causal-model-rcm",
    "href": "causal_inference.html#the-rubin-causal-model-rcm",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "15.2 The Rubin causal model (RCM)",
    "text": "15.2 The Rubin causal model (RCM)\nThroughout this book we have looked at statistical effects, but we have only defined these in the context of associations between variables. For example, in a linear regression model, we use coefficients to describe the effect of a one unit change in an input variable on the outcome variable, assuming no change in the other input variables. A natural question that arises from this is ‘how much, if any, of this effect is causal?’. For this, we need a definition of what we mean by a “causal effect.” The most intuitive and mathematically rigorous way to think about this is through the Rubin causal model (RCM).\n\n15.2.1 Treatment effects\nImagine a single manager of a sales team in a company. Let’s call her Jane. Jane has the option to volunteer for a new management training program. We are interested in the causal effect of this new program on the sales of Jane’s team. Let’s denote the treatment, which is participation in the training program, as \\(T\\). Jane can either receive the treatment (\\(T = 1\\)) or not receive it (\\(T = 0\\)).\nUnder RCM, we imagine that for Jane, there exist two potential outcomes right before her decision is made about her attendance at the training:\n\n\\(Y(1)\\): This is Jane’s team’s sales if she were to attend the training program.\n\\(Y(0)\\): This is Jane’s team’s sales if she were not to attend the training program.\n\nThe individual treatment effect (ITE) for Jane is simply the difference between these two potential outcomes—that is:\n\\[\n\\mathrm{ITE} = Y(1) - Y(0)\n\\]\nIf Jane’s team’s sales figure would be $2.1 million if she were to attend the training (\\(Y(1) = 2.1\\)) and if it would be $1.6 million if she were not to attend (\\(Y(0) = 1.6\\)), then the ITE of the training for Jane is $0.5 million. Her attendance at the training caused a $500,000 increase in her team’s sales.\nBut in the real world, we can never observe both potential outcomes for the same individual at the same time. Jane will either attend the training or she will not.\n\nIf she attends the training (\\(T = 1\\)), we observe \\(Y(1)\\). \\(Y(0)\\) is unobserved; it is a counterfactual. We know what her team’s sales figure is now that she has attended, but we will never know what it would have been had she not attended.\nIf she does not attend the training (\\(T = 0\\)), we observe \\(Y(0)\\). \\(Y(1)\\) is unobserved; it is also a counterfactual. We know what her team’s sales figure is now that she has not attended, but we will never know what it would have been had she attended.\n\nThis real life issue is called the Fundamental Problem of Causal Inference. Causal inference is, in essence, a missing data problem. For every single person in our study, at least half of the crucial information (one of their potential outcomes) is missing. The table below illustrates this. The question marks represent the unobservable, counterfactual outcomes.\n\n\n\n\n\n\n\n\n\n\nEmployee\nAttended Training (T)\nOutcome if Trained (Y(1))\nOutcome if Not Trained (Y(0))\nObserved Outcome (Y)\n\n\n\n\nJane\n1\n$2.1m\n?\n$2.1m\n\n\nDavid\n0\n?\n$3.1m\n$3.1m\n\n\nSuraya\n1\n$1.2m\n?\n$1.2m\n\n\nZubin\n0\n?\n$2.9m\n$2.9m\n\n\n\nNow we know that we can never calculate the individual treatment effect for any single person. But as statisticians, we could instead aim for the next best thing. We could aim to estimate the average treatment (ATE) effect across a population of individuals. This is the mean (or expected) difference in potential outcomes if everyone in the population were to receive the treatment versus if everyone were to receive the control. Formally, it is defined as:\n\\[\n\\mathrm{ATE} = E[Y(1) - Y(0)]\n\\]\nIf we estimate the ATE of our management training to be $0.6 million, it means that, on average, the program increases average team sales by that amount. This is exactly the kind of information a decision-maker needs to evaluate the program’s worth. For example, they can compare the ATE to the cost of running the training to see if the investment has a positive financial return.\nBut, again, in real life it is usually the case that not everyone is treated. Our training program is optional and not everyone will attend. Therefore, it is more insightful for us to determine the average treatment effect for those who attend the training. We want to determine the average treatment effect for the treated, known as the ATT:\n\\[\n\\begin{aligned}\n\\mathrm{ATT} &= E[Y(1) - Y(0) \\mid T=1] \\\\\n&= E[Y(1) \\mid T=1] - E[Y(0) \\mid T=1]\n\\end{aligned}\n\\]\nThe question now becomes: how can we estimate the ATT when we can’t see \\(Y(0)\\) for the people who have been treated?\n\n\n15.2.2 Naive comparisons and counfounding\nThe simplest thing we could do is use the data we have, and calculate the difference in the average observed outcomes between the two groups: treated and not treated. This is called the naive comparison (NC).\n\\[\n\\mathrm{NC} = E[Y(1) \\mid T=1] - E[Y(0) \\mid T=0]\n\\]\nLet’s examine the formal mathematical difference between the naive comparison and the ATT.\n\\[\n\\begin{aligned}\n\\mathrm{NC} - \\mathrm{ATT} &= E[Y(1) \\mid T=1] - E[Y(0) \\mid T=0] - \\\\& \\mathrm{\\ \\ \\ \\ \\ } (E[Y(1) \\mid T=1] - E[Y(0) \\mid T=1]) \\\\\n&= E[Y(0) \\mid T=1] - E[Y(0) \\mid T=0]\n\\end{aligned}\n\\]\nThis is known as the bias of the naive comparison as an estimator for the ATT. If this bias is zero, then the naive comparison is an unbiased estimator of the ATT. If this bias is not zero, then the naive comparison is a biased estimator of the ATT. This bias term can be non-zero for a variety of reasons, but the most common is confounding4. Confounding is the presence of a third variable (or set of variables) that is a common cause of both the treatment and the outcome. This common cause creates a spurious association between the treatment and the outcome that is not causal.\nReturning to the management training example, the managers who volunteer for training (\\(T=1\\)) might be more ambitious, more motivated, and already better managers than those who do not (\\(T=0\\)). Therefore, even if nobody got the training, the treatment group’s teams would likely have higher sales anyway. That is, \\(E[Y(0) \\mid T=1] &gt; E[Y(0) \\mid T=0]\\). This difference is confounding. The ambition and motivation of the managers is a common cause of both their decision to take the training and their teams’ higher sales.\nThe naive comparison mixes the true causal effect of the training (ATT) with this pre-existing bias. Our entire goal in causal inference is to find a way to eliminate this bias and isolate the true ATT. The most effective (but, sadly, usually the least practical) way to do this is through a randomized controlled trial (RCT).\nIn an RCT, the researchers randomly assign individuals to either the treatment or the control group. Because the assignment is random, it cannot be correlated with any pre-existing characteristics of the individuals. Over large numbers, randomization ensures that, on average, the treatment group and the control group are identical in every way before the treatment is administered. Under these conditions, the bias term is zero and the naive comparison is equal to the ATT.\n\n\n15.2.3 Causal inference from observational data\nUnfortunately, in psychology, sociology, human resources and other people-related fields we can’t always run an RCT. It might be inequitable (we can’t randomly give some employees a benefit that we don’t give others), impractical (we can’t randomly assign a new corporate culture to half the company), or impossible (we can’t randomly assign personality traits). We are more often than not working with observational data, where people have already “selected their own treatments”.\nTo estimate a causal effect from such data, we need to try to emulate a randomized trial with statistics. To do this, we must rely on a set of critical, untestable assumptions. Conditional on these assumptions, we can recover unbiased estimates of causal effects. Let’s review these assumptions.\nFirst, the Stable Unit Treatment Value Assumption (SUTVA) has two parts:\n\nNo interference: The potential outcomes for any individual are not affected by the treatment assignments of other individuals. In our management training example, this means that Jane’s team’s sales figure doesn’t change if her colleague David attends the training. This assumption is often violated in social settings. If Jane and David both attend, learn new techniques and subsequently collaborate, their combined effect might be larger than the sum of their individual effects (positive interference). Conversely, if both their teams are competing for the same pool of customers, David attending the training without Jane attending might negatively affect Jane’s outcome (negative interference).\nConsistency: The observed outcome for an individual who received the treatment \\(T\\) is equal to their potential outcome under that specific treatment, \\(Y(T)\\). This sounds trivial, but it implies that the treatment is well-defined. “Management training” must mean the same thing for everyone. If part of Jane’s training is a follow up on how she passes on her learning to her team, but David is not given this follow up, then the treatment is not consistent between them. Their potential outcomes under the treatment are not comparable.\n\nSecond, the Ignorability (or Exchangeability or Unconfoundedness) assumption is our attempt to statistically deal with confounding. Formally, it states that, conditional on a set of pre-treatment variables \\(X\\), the treatment assignment is independent of the potential outcomes. This is formally stated as5:\n\\[(Y(1), Y(0)) \\perp T \\mid X\\]\nThat is, once we account for the variables in \\(X\\), the group that received the treatment and the group that did not are, on average, comparable, as if the treatment had been randomly assigned within each level of \\(X\\).\nIn our training example, we suspected that “motivation” might be a confounder. Managers’ motivation (\\(X\\)) affects both their likelihood of taking the training (\\(T\\)) and their team’s sales (\\(Y\\)). Ignorability assumes that if we look only at a group of managers with the same level of motivation, then within that group, the decision to take the training was essentially random with respect to their potential team sales. By conditioning on (or controlling for) all possible common causes of the treatment and the outcome—the set of confounders \\(X\\)—we hope to make the bias disappear. This is why it is sometimes called “conditional exchangeability”. We don’t believe the entire treatment and control groups are exchangeable, but we are willing to assume they are exchangeable within each level of \\(X\\).\nOf course the challenge here is that we must identify and measure all the important confounders. If there is a confounder that we did not measure or include in our set \\(X\\) (e.g., the managers’ level of prior experience), then this assumption will be violated, and our causal estimate will be biased due to unmeasured confounding. This is the biggest challenge of causal inference on observational people data. Many potential confounders are unknown, unmeasurable or both.\nThird, the Positivity (or Overlap) assumption requires that for every combination of our confounding variables \\(X\\) in our population, there is a non-zero probability of being both treated and untreated. This is formally stated as:\n\\[\n\\begin{aligned}\n0 &lt; P(T=1 \\mid X=x) &lt; 1 \\\\\n0 &lt; P(T=0 \\mid X=x) &lt; 1\n\\end{aligned}\n\\]\nfor all \\(x \\in X\\). This means that there are no subgroups of people defined by our confounding variables who always or never receive the treatment. For example, if one of our confounding variables was ‘years experience’, and our management training was only ever offered to managers with more than 5 years of experience, then for managers with less than 5 years experience, \\(P(T=1 \\mid X) = 0\\). We have no data on what would happen if a junior manager took the training, so we cannot estimate the effect for them. There is no “overlap” in the data for that subgroup.\nIf positivity is violated, we simply cannot estimate the ATT for some groups associated with our confounding variables, and we would have to rethink our approach to the analysis.\n\n\n15.2.4 The ‘boobytraps’ of causal inference\nArmed with RCM and the key assumptions above, we can now articulate the common types of problems we might encounter in attempting to infer causality between a treatment and an outcome. All of these “boobytraps” relate to how we handle other variables outside the treatment variable \\(T\\) and the outcome variable \\(Y\\) in different situations.\n\nConfounding: As we have seen, this is a central problem. A third variable (or set of variables) \\(X\\) is a common cause of both the treatment \\(T\\) and the outcome \\(Y\\). The relates to the assumption of ignorability. To obtain an unbiased estimate of the ATT, we must identify and condition on all confounding variables. In our management training example, if we fail to account for the managers’ motivation levels, which affects both their likelihood of taking the training and their team’s sales, our estimate of the training’s causal effect will be biased.\nSelection Bias: While confounding is a form of selection bias, this term specifically refers to a situation where the sample is selected (or the analysis is conditioned) based on factors related to the outcome. This relates to the assumption of positivity. If certain types of individuals are systematically excluded from the analysis based on factors related to the outcome, it can bias our estimates. For example, if we only analyze data from managers who had high sales, we might over- or underestimate the training’s effectiveness because we are ignoring those who had low sales.\nMediation: In understanding causal mechanisms, it is critical to consider if a causal effect is operating via some intermediate variables or mediators. This relates to the assumption of SUTVA. If we condition on a mediator variable that lies on the causal path from treatment to outcome, we can block part of the causal effect we are trying to estimate. For example, imagine if the training results in managers teaching new sales techniques to their teams, which then impacts sales. If we condition our analysis on the specific sales techniques used by the teams, we will underestimate the total causal effect of the training on sales.\n\nThe RCM gives us a clear language to define the causal effects we want to estimate. The assumptions of SUTVA, Ignorability, and Positivity define the “rules of the game” for when we can legitimately estimate those effects from observational data without bias. The challenge, however, is that the crucial assumption of Ignorability depends on identifying the correct set of confounding variables. How do we do that? Simply throwing every variable we have into a regression model is not the answer and, in fact, it usually makes things worse.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#the-structural-causal-model-scm",
    "href": "causal_inference.html#the-structural-causal-model-scm",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "15.3 The structural causal model (SCM)",
    "text": "15.3 The structural causal model (SCM)\nThe RCM is powerful for formally defining causal effects, but it only takes us so far. It tells us we need to condition on all confounders, but it doesn’t tell us how to identify them. For this, we will need to incorporate our subject-matter knowledge and our theories about how the system we are researching actually works. We can express our causal theories using the structural causal model (SCM). Graphs are the cornerstone of the SCM. They are a visual language for expressing our assumptions about the causal relationships between variables. By representing our assumptions in a graph, we can use a clear set of rules to determine the statistical implications of those assumptions and to estimate causal effects.\n\n15.3.1 Directed Acyclic Graphs (DAGs)\nLike all graphs, DAGs consist of two simple components:\n\nNodes (or Vertices): These represent variables. They can be anything we can measure or imagine: a treatment (e.g., training), an outcome (e.g., sales), and any other variables that are believed to play a role in a problem (e.g., motivation).\nEdges (or Arrows): These are directed arrows that connect two nodes. An arrow drawn from node A to node B (A -&gt; B), means we are assuming that variable A has a direct causal influence on variable B. The arrow represents a hypothetical intervention: if we were to intervene and change the value of A, we would expect to see a change in the value of B. “Direct” here means not mediated by any other variable already in the graph.\n\nDAGs must be acyclic. This means there are no loops in the graph. You cannot start at a node, follow the direction of the arrows, and end up back at the same node. This makes sense in most cross-sectional research studies, where it’s assumed that causation flows in one direction at any single point in time.\nLet’s look at the simple example of our sales manager training program. Let’s say we believe that attendance at the training program has a direct causal effect on the sales of the manager’s team. Let’s also say we believe that a manager’s pre-existing motivation has a direct causal effect on both their decision to sign up for the training and their team’s sales. The DAG for this causal theory is shown in Figure 15.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.1: DAG representing the assumed causal relationships between motivation, training, and sales.\n\n\n\nThis simple graph encodes several causal beliefs: 1. Motivation has a direct causal effect on Training (more motivated managers are more likely to sign up). 2. Motivation has a direct causal effect on Sales (more motivated managers lead their teams to achieve higher sales figures). 3. Training has a direct causal effect on Sales (this is the effect we want to estimate). 4. There are no other common causes of any two variables in the graph. 5. Sales does not have a direct causal effect on Training (the outcome doesn’t affect the treatment).\nThis DAG is our causal model. It is not derived from the data; it is imposed upon it by our expert knowledge and beliefs. The DAG forces us to be transparent about our assumptions, which can then be analyzed, critiqued or improved.\n\n\n15.3.2 DAG paths and the three fundamental structures\nThe reason DAGs are so useful is that they provide rules for how statistical associations flow between variables. An association between two variables \\(X\\) and \\(Y\\) exists if there is an open (or unblocked) path between them in the DAG. A direct causal effect is one source of association, but there are others. By understanding the three fundamental structures that can exist between variables in a DAG, we can create a system through which we can identify open non-causal paths. Each of these structures relate to the “boobytraps” we discussed earlier: mediation, confounding and selection bias. They can be seen in Figure 15.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: The three fundamental structures in DAGs: Chain (Mediation), Fork (Confounding), and Collider (Selection Bias). In the chain, M is the mediator. In the fork, C is the common cause (confounder). In selection bias, D is the collider and E is a descendent collider.\n\n\n\nReferring to Figure 15.2:\n\nMediation is represented by a ‘chain’ structure. This means A has a direct causal effect on M, and M has a direct causal effect on B. M is a mediator variable. In a chain, there is a clear causal path from A to B. Therefore, A and B will be statistically associated. But if we condition on the mediator M, we block the flow of association between A and B. For example, if we believe Motivation -&gt; Job Satisfaction -&gt; Job Performance, then if we try to estimate the causal effect of Motivation on Job Performance we should not include Job Satisfaction in our statistical model. If we do, we will have a biased estimate.\nConfounding is represented by a ‘fork’ structure. This means C is a common cause of both A and B. In this fork, there is no causal path from A to B. However, because they share a common cause, information flows from C to A and from C to B, creating a spurious (non-causal) association between A and B. If we were to naively correlate A and B, we would find an association, not because A has a direct causal effect on B, but because C has a direct causal effect on both. The non-causal path from A to B is called a ‘backdoor’ path, because A has an edge going into it rather than coming out of it. To block this backdoor path, we must condition on the common cause, C. For example, if we believe Training &lt;- Motivation -&gt; Sales, then to estimate the causal effect of Training on Sales, we must include Motivation in our model. If we leave it out, we will have a biased estimate.\nSelection bias is represented by a ‘collider’ structure. This structure means A and B are both independent causes of a third variable, D. The arrows “collide” at D. In a collider structure, if A and B are independent to begin with, there is no open path between them. The collider D naturally blocks the flow of association. However, if we condition on the collider D, we open the path and create a spurious association between A and B where none existed before. For example, if we believe Knowledge -&gt; Being Hired &lt;- Interpersonal Skills, then in the general population, Knowledge and Interpersonal Skills are uncorrelated. But if we look only at only those who were hired, then within that selected group, Knowledge and Interpersonal Skills can become negatively correlated. Conditioning on the common effect (the collider) induces an association. Moreover, if a variable is a descendant of a collider (i.e., it is on a chain emanating from the collider, such as our variable E), conditioning on that descendant also opens the path.\n\nThis leaves us with three critical rules for how associations flow through a DAG:\n\nConditioning on the middle variable in a chain blocks the path.\nConditioning on the common cause in a fork blocks the path.\nConditioning on a collider (or a descendant of a collider) opens a path.\n\n\n\n15.3.3 D-separation and the backdoor criterion\nOur three rules form the basis of a concept called d-separation (for “directional separation”). Two variables in a DAG, X and Y, are said to be d-separated if there is no open path between them. If they are d-separated, the causal structure represented by the DAG implies that they should be statistically independent. If they are not d-separated (they are “d-connected”), they should be statistically associated.\nThe most important application of these rules is in finding a set of variables to control for to estimate a causal effect. Let’s say we want to estimate the causal effect of a treatment, T, on an outcome, Y. That is, we are interested in the direct causal path from T to Y.\nThe statistical association we measure between T and Y in our data can be a mixture of the true causal effect and any spurious associations from non-causal paths. Our goal is to isolate the causal path. To do this, we need to block all the open non-causal paths between T and Y, without blocking the causal path itself. This means we need to identify and block any backdoor path which is open. A backdoor path is any path that starts with an arrow pointing into T. For example, in our motivation fork (Training &lt;- Motivation -&gt; Sales), the path from Training to Motivation to Sales is a backdoor path. It creates a spurious association that we must block.\nThis leads us to the backdoor criterion, a powerful graphical test for choosing a sufficient set of control variables (an “adjustment set”) for testing a causal effect. Under the backdoor criterion, a set of variables Z is a sufficient adjustment set for estimating the causal effect of T on Y if two conditions are met:\n\nNo variable in Z is a descendant of T (we don’t control for mediators or other variables caused by the treatment).\nThe variables in Z block every backdoor path between T and Y.\n\nIf we can find a set of variables Z that satisfies the backdoor criterion, then we can estimate the causal effect of T on Y by conditioning on Z. In practice, this means including Z along with T as input variables in a regression model. Let’s look at a specific example in order to see this criterion in action.\n\n\n15.3.4 Example of applying the backdoor criterion\nA company runs a selection process for hiring new graduates. The process involves a test, a one-on-one interview and a presentation to a panel. We have scoring data on the all of these, as well as data on the graduates’ academic performance (GPA). We are interested in determining the causal effect of the test score (our treatment) on the likelihood of being hired (our outcome). We have the following causal model in mind:\n\nThe test, the interview and the presentation all have direct causal effects on the hiring decision.\nThe presentation has a direct causal effect on the interview (because the interview is conducted by a member of the presentation panel)\nThe academic ability of the graduate has a direct causal effect on their test, interview and presentation.\nThere are no other direct causal effects.\n\nThe DAG for this causal model is shown in Figure 15.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.3: DAG representing beliefs about causal relationships between a hiring decision, test, interview, presentation and GPA.\n\n\n\nLet’s identify all the paths between Test and Hire and use the backdoor criterion to find a sufficient adjustment set.\n\nPath 1 (the causal path): Test -&gt; Hire. We want to keep this path open. It is causal because it flows in the direction of the arrows from treatment variable to outcome variable.\nPath 2 (q backdoor path): Test &lt;- GPA -&gt; Int -&gt; Hire. This is a classic confounding path through GPA. It’s a fork, with a chain from GPA to Hire. To block it, we have two options. We can condition on the common cause GPA, and then we do not need to condition on Int because by conditioning on GPA we have already blocked the backdoor path. Alternatively, we can condition on Int to block the path through the chain.\nPath 3 (another backdoor path): `Test &lt;- GPA -&gt; Pres -&gt; Hire. This is another backdoor path through GPA and Pres. It’s another fork followed by a chain. Conditioning on GPA will also block this path, or alternatively so will conditioning on Pres.\nPath 4 (another backdoor path): Test &lt;- GPA -&gt; Pres -&gt; Int -&gt; Hire. This is yet another backdoor path through GPA, Pres and Int. It’s another fork followed by a longer chain. Conditioning on GPA will also block this path, as will conditioning on Pres or Int.\nPath 5 (a blocked path): Test &lt;- GPA -&gt; Int &lt;- Pres -&gt; Hire. This path is blocked because Int is a collider. We do not need to concern ourselves with this path.\n\nThis provides us with two possible adjustment sets for determining the causal effect of Test on Hire:\n\n{GPA}: By conditioning on GPA alone, we block all open backdoor paths (Paths 2, 3 and 4) while keeping the causal path (Path 1) open.\n{Int, Pres}: By conditioning on both Int and Pres, we also block all open backdoor paths while keeping the causal path open.\n\nThis means that if we want to run a regression model to determine the causal effect of Test on Hire, we can either include GPA as a control variable, or we can include both Int and Pres as control variables. Either approach will give us an unbiased estimate of the causal effect of Test on Hire, conditional on our DAG.\nThis example illustrates the power of DAGs. Without drawing the graph and applying the backdoor criterion, a researcher might be tempted to control for everything they measured. The DAG shows that this “kitchen sink” approach to regression is not just unnecessary, but it can lead to major errors in inferring causal effects. DAGs provide a principled, theory-driven way to construct regression models.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#causal-inference-in-r",
    "href": "causal_inference.html#causal-inference-in-r",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "15.4 Causal inference in R",
    "text": "15.4 Causal inference in R\nIn R, the dagitty package allows you to specify a DAG using a simple text-based syntax. Once specified, you can query the DAG to identify instrumental variables, and find sufficient adjustment sets using the backdoor criterion. It can also list the “testable implications” of your DAG, by listing the conditional independencies that should hold true in your data if your causal model is correct, which you can verify.\nThe ggdag package is built on top of ggplot2 and works seamlessly with dagitty objects. It provides an easy and highly customizable way to create visualizations of your DAGs. It can automatically highlight paths, adjustment sets, and other features, making it a very helpful tool for both analysis and communication.\nLet’s use our selection process example from Section 15.3.3 as a walkthrough example to illustrate how to do causal inference in R using the dagitty and ggdag packages.\n\n15.4.1 Walkthrough example\nThe selection data set contains data related to the hiring process for 70 candidates in a graduate hiring program. The data contains the following variables:\n\nHire: A binary variable indicating whether the candidate was hired (1) or not (0).\nTest: The candidate’s score out of a maximum of 15 on a written test.\nInt: The candidate’s rating from a one-on-one interview, on an integer scale from 1 (Low) to 5 (High).\nPres: The candidate’s average rating from a presentation to a panel, on a scale from 1 (Low) to 5 (High).\nGPA: The candidate’s undergraduate Grade Point Average (GPA), on a scale from 1.0 to 4.0.\n\nLet’s take a quick look at the data.\n\n# load the selection data set\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/selection.csv\"\nselection &lt;- read.csv(url)\n\n# view the first few rows of the data\nhead(selection)\n\n  GPA Test Pres Int Hire\n1 3.3   10  3.6   3    1\n2 2.6    8  3.9   2    1\n3 3.0    9  3.1   4    1\n4 3.6    9  2.5   4    1\n5 3.1    9  3.6   3    1\n6 3.5   13  2.8   3    0\n\n\nBefore we define a causal model to work with, let’s look at what a ‘naive’ analysis would look like. We can run a simple Bayesian logistic regression explaining Hire from Test alone.\n\nlibrary(brms)\n\n# set brms options\noptions(\n  brms.iter = 10000,\n  brms.chains = 4,\n  brms.seed = 123,\n  brms.refresh = 0,\n  brms.save_pars = save_pars('all')\n)\n\n# run simple Bayesian logistic regression\nnaive_model &lt;- brm(\n  formula = Hire ~ Test,\n  data = selection,\n  family = bernoulli()\n)\n\nWe can now take a look at the posterior estimate of our coefficient for Test, as in Figure 15.4.\n\n\n\n\nlibrary(bayesplot)\n\n# view posterior for Test coefficient\nmcmc_areas(\n  as.matrix(naive_model),\n  pars = \"b_Test\",\n  prob = 0.95\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Posterior distribution of the coefficient for Test from a naive Bayesian logistic regression explaining Hire from Test alone.\n\n\n\nThis naive model suggests a high probability (86.8%) that the test has a positive effect on the log odds of being hired, giving us a degree of confidence that the test is providing a meaningful contribution to the hiring decision. However, we have no idea how much of this effect is a direct causal effect. There may be confounding variables biasing this estimate. To determine what variables to control for, we need to define our causal model using a DAG.\nWe have a hypothesized causal model for how these variables relate to each other, as shown earlier in Figure 15.3. We want to estimate the causal effect of Test on Hire. According to our earlier manual backdoor analysis, we should find that we are able to do this by controlling for either GPA alone, or for both Int and Pres.\n\n\n15.4.2 Specifying and visualizing the DAG\nThe syntax for defining a DAG using dagitty is straightforward. We define the nodes and edges in a string format. The treatment (exposure) and outcome variables can be specified using special tags.\n\n# define the selection DAG\nlibrary(dagitty)\n\nselection_dag_string &lt;- \"dag {\n  GPA\n  Hire [Outcone]\n  Pres\n  Int\n  Test [Exposure]\n  Hire &lt;- Pres\n  Hire &lt;- Int\n  Hire &lt;- Test\n  Int &lt;- Pres\n  Pres &lt;- GPA\n  Int &lt;- GPA\n  Test &lt;- GPA\n}\"\n\nAlternatively, we can use the dagify function from ggdag to create the same DAG string using a formula syntax.\n\n# define using dagify\nlibrary(ggdag)\n\n(selection_dag &lt;- dagify(\n  Hire ~ Pres + Int + Test,\n  Int ~ Pres + GPA,\n  Pres ~ GPA,\n  Test ~ GPA,\n  labels = c(Hire = \"Hire\", Pres = \"Pres\", Int = \"Int\", Test = \"Test\", GPA = \"GPA\"),\n  exposure = \"Test\",\n  outcome = \"Hire\"\n))\n\ndag {\nGPA\nHire [outcome]\nInt\nPres\nTest [exposure]\nGPA -&gt; Int\nGPA -&gt; Pres\nGPA -&gt; Test\nInt -&gt; Hire\nPres -&gt; Hire\nPres -&gt; Int\nTest -&gt; Hire\n}\n\n\nThe ggdag package makes plotting this DAG very easy, as in Figure 15.5.\n\n\n\n\n# visualize the selection DAG\nlibrary(ggplot2)\nggdag(selection_dag) +\n  theme_dag() \n\n\n\n\n\n\n\n\n\n\nFigure 15.5: DAG representing beliefs about causal relationships between a Hiring Decision, Test Score, Interview Score, Presentation Score, and GPA.\n\n\n\nTo control the positioning of the nodes, we can specify relative coordinates in the DAG string or use the coords argument in dagify. For example, to replicate the positioning as in Figure 15.3, we can do the following:\n\n(selection_dag &lt;- dagify(\n  Hire ~ Pres + Int + Test,\n  Int ~ Pres + GPA,\n  Pres ~ GPA,\n  Test ~ GPA,\n  labels = c(Hire = \"Hire\", Pres = \"Pres\", Int = \"Int\", Test = \"Test\", GPA = \"GPA\"),\n  exposure = \"Test\",\n  outcome = \"Hire\",\n  coords = list(\n    x = c(Hire = 0, Pres = 2, Int = 1, Test = 0, GPA = 2),\n    y = c(Hire = 0, Pres = 0, Int = 1, Test = 2, GPA = 2)\n  )\n))\n\ndag {\nGPA [pos=\"2.000,2.000\"]\nHire [outcome,pos=\"0.000,0.000\"]\nInt [pos=\"1.000,1.000\"]\nPres [pos=\"2.000,0.000\"]\nTest [exposure,pos=\"0.000,2.000\"]\nGPA -&gt; Int\nGPA -&gt; Pres\nGPA -&gt; Test\nInt -&gt; Hire\nPres -&gt; Hire\nPres -&gt; Int\nTest -&gt; Hire\n}\n\n\nWe can convert our dagitty object into a tidy data format.\n\n(selection_dag_tidy &lt;- ggdag::tidy_dagitty(selection_dag))\n\n# A DAG with 5 nodes and 7 edges\n#\n# Exposure: Test\n# Outcome: Hire\n#\n# A tibble: 8 × 9\n  name      x     y direction to     xend  yend circular label\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt;    &lt;chr&gt;\n1 GPA       2     2 -&gt;        Int       1     1 FALSE    GPA  \n2 GPA       2     2 -&gt;        Pres      2     0 FALSE    GPA  \n3 GPA       2     2 -&gt;        Test      0     2 FALSE    GPA  \n4 Hire      0     0 &lt;NA&gt;      &lt;NA&gt;     NA    NA FALSE    Hire \n5 Int       1     1 -&gt;        Hire      0     0 FALSE    Int  \n6 Pres      2     0 -&gt;        Hire      0     0 FALSE    Pres \n7 Pres      2     0 -&gt;        Int       1     1 FALSE    Pres \n8 Test      0     2 -&gt;        Hire      0     0 FALSE    Test \n\n\nThis can make it easier to add customizations to our graph visualization using ggplot2. For example we can color code the nodes based on their roles (exposure, outcome, confounder) as in Figure 15.6.\n\n\n\n\nlibrary(dplyr)\n\n# add a role column to the tidy dag\nselection_dag_tidy &lt;- selection_dag_tidy |&gt; \n  dplyr::mutate(\n    role = dplyr::case_when(\n      name == \"Test\" ~ \"Exposure\",\n      name == \"Hire\" ~ \"Outcome\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# plot with custom colors in ggdag\nlibrary(ggplot2)\nggdag(selection_dag_tidy, stylized = TRUE) +\n  geom_dag_node(aes(color = role), size = 20) +\n  geom_dag_text(color = \"black\", size = 4) +\n  theme_dag() \n\n\n\n\n\n\n\n\n\n\nFigure 15.6: DAG with custom coloring of nodes based on their roles (exposure, outcome, other).\n\n\n\n\n\n15.4.3 Deriving information from the DAG\nWe can ask dagitty to find our adjustment sets based on our exposure and outcome variable.\n\n# Find the adjustment set for the effect of Test on Hire\nadjustmentSets(selection_dag)\n\n{ Int, Pres }\n{ GPA }\n\n\nThis confirms the adjustments sets which we derived manually earlier in Section 15.3.3. We can also view the adjustment sets using ggdag, as in Figure 15.7.\n\n\n\n\nggdag_adjustment_set(selection_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nFigure 15.7: Adjustment sets for estimating the causal effect of Test on Hire.\n\n\n\nWe can also ask dagitty to show us the paths between the exposure and outcome variables.\n\npaths(selection_dag)\n\n$paths\n[1] \"Test -&gt; Hire\"                       \"Test &lt;- GPA -&gt; Int -&gt; Hire\"        \n[3] \"Test &lt;- GPA -&gt; Int &lt;- Pres -&gt; Hire\" \"Test &lt;- GPA -&gt; Pres -&gt; Hire\"       \n[5] \"Test &lt;- GPA -&gt; Pres -&gt; Int -&gt; Hire\"\n\n$open\n[1]  TRUE  TRUE FALSE  TRUE  TRUE\n\n\nWe can see that 5 paths have been identified, four of which are open paths. One of these is the direct causal path and the other three are backdoor paths. These open paths can be visualized using ggdag, as in Figure 15.8.\n\n\n\n\nggdag_paths_fan(selection_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nFigure 15.8: Open paths between Test and Hire in the DAG.\n\n\n\nAnother powerful feature of dagitty is its ability to list the testable implications of our hypothesized causal model. If our model is true, it often implies that certain variables should be independent of each other (d-separated), conditional on other variables. These are the conditional independencies predicted by the d-separation rules, and we can verify if they are supported by the data. If they are not, it suggests our DAG is incorrect and needs to be revised.\n\nimpliedConditionalIndependencies(selection_dag)\n\nGPA _||_ Hire | Int, Pres, Test\nInt _||_ Test | GPA\nPres _||_ Test | GPA\n\n\nTo understand the output, we can interpret each line as a conditional independence statement. For example, the first line means that GPA is independent of Hire when conditioned on Int, Pres and Test. This makes sense because the three conditioned variables are mediators between GPA and Hire on chain paths, and we know that when we condition on a mediator we block the path, thus making GPA and Hire d-separated.\n\n\n15.4.4 Using DAGs to guide regression analyses\nNow that we have identified our adjustment sets and implied conditional independencies, we can use them to guide our regression analyses in determining the causal effect of the test performance on the hiring decision. Although we can use DAGs alongside classical (frequentist) regression models, it is more common nowadays to use them with Bayesian regression models. Bayesian models provide a more intuitive framework for causal inference, allowing us to directly estimate the posterior distribution of causal effects and incorporate prior knowledge into our analyses.\nFirst, we can test the conditional independencies implied by our DAG using regression models. We can test the first conditional independence statement: GPA is independent of Hire when conditioned on Int, Pres and Test.\n\n# fit a Bayesian binomial regression model\nmodel_ci &lt;- brm(\n  formula = Hire ~ GPA + Int + Pres + Test,\n  data = selection,\n  family = bernoulli()\n)\n\nWe can now examine the posterior distribution of the GPA coefficient.\n\n\n\n\n# visualize the posterior coefficients\nmcmc_areas(\n  as.matrix(model_ci),\n  pars = c(\"b_GPA\"),\n  prob = 0.95\n) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.9: Posterior distribution of the GPA coefficient from the Bayesian regression model testing the conditional independence implied by the DAG.\n\n\n\nFigure 15.9 shows the posterior distribution of the GPA coefficient, with a coefficient of zero comfortably within the range of credible probability. Based on this, we can determine a level of comfort that our DAG is supported by our observed data.\nAssuming our other conditional dependencies are supported by the data (which should be tested), we can now estimate the causal effect of Test on Hire using any of the two adjustment sets we identified earlier. In this case, we will control for GPA alone.\n\n# fit a Bayesian binomial regression model to estimate the causal effect of Test on Hire\nmodel_causal &lt;- brm(\n  formula = Hire ~ Test + GPA,\n  data = selection,\n  family = bernoulli()\n)\n\nWe can visualize our causal effect estimate for Test, as in Figure 15.10.\n\n\n\n\n# visualize the posterior coefficient for Test\nmcmc_areas(\n  as.matrix(model_causal),\n  pars = c(\"b_Test\"),\n  prob = 0.95\n) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.10: Posterior distribution of the causal effect of Test on Hire\n\n\n\nWe can also view the 95% credible interval for the causal effect of Test on Hire.\n\n# get 95% credible interval for the causal effect of Test on Hire\n(ci &lt;- posterior_summary(as.matrix(model_causal), probs = c(0.025, 0.975))[\"b_Test\", c(\"Q2.5\", \"Q97.5\")])\n\n      Q2.5      Q97.5 \n-0.3063534  0.4456487 \n\n\nWe can directly determine the probability that the effect of the test score is positive by calculating the proportion of posterior samples where the coefficient for Test is greater than zero.\n\n# calculate the probability that the effect of Test is positive\nposterior_samples_causal &lt;- as.matrix(model_causal)\n(prob_positive &lt;- mean(posterior_samples_causal[,\"b_Test\"] &gt; 0))\n\n[1] 0.63135\n\n\nThis indicates that there is a 63.1% probability that the test score has a positive causal effect on the log odds of being hired. This is a big difference from our earlier naive estimation, and gives us much lower confidence that the test is making a meaningful contribution to the hiring decision. Controlling for our confounder GPA has had a major impact on our estimate.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#learning-exercises",
    "href": "causal_inference.html#learning-exercises",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "15.6 Learning exercises",
    "text": "15.6 Learning exercises\n\n15.6.1 Discussion questions\n\nWhat are the different types of treatment effects defined in the Rubin causal model (RCM)? How are they defined mathematically?\nWhat is ‘bias’ in the context of the RCM? Give a common reason why the naive estimate of a treatment effect might be biased.\nOutline the three key assumptions (SUTVA, Ignorability, Positivity) required for unbiased causal inference on observational data.\nExplain the three fundamental structures in Directed Acyclic Graphs (DAGs): chain, fork, and collider. How do these structures relate to mediation, confounding, and selection bias?\nWhat is the difference between a causal path and a backdoor path in a Directed Acyclic Graph (DAG)? Why is it important to distinguish between the two when estimating causal effects?\nWhat does it mean for two variables to be d-separated in a DAG? How does d-separation relate to statistical independence?\nWhat is the backdoor criterion, and how does it help in identifying a sufficient adjustment set for estimating causal effects?\nHow can DAGs be used to guide regression analyses for causal inference? What are the advantages of using DAGs in this context?\nWhy is it important to test the conditional independencies implied by a DAG before using it to estimate a causal effect?\nConsider a multivariable problem that you have dealt with in your study or work? How might you apply causal inference techniques to this problem?\n\n\n\n15.6.2 Data exercises\nFor our data exercises in this chapter, we will return to the sociological_data dataset in Section 4.8. Take a random sample of about 20% of the observations and assume that this is the only data you have available. For this exercise, our aim will be to determine the causal effect of education_months on annual_income_ppp. We will assume that we have the following causal model in mind:\n\neducation_months, average_wk_hrs, job_type and gender all have direct causal effects on annual_income_ppp.\neducation_months, languages, gender and region has a direct causal effect on job_type\ngender, region and family_size have direct causal effects on education_months\nwork_distance has a direct causal effect on average_wk_hrs\nThere are no other causal effects between variables.\n\n\nUsing a naive Bayesian regression model, estimate the effect of education_months on annual_income_ppp without controlling for any other variables. Visualize the posterior distribution of the effect estimate and interpret the results.\nUsing the dagitty or ggdag package in R, specify the DAG representing the causal model described above.\nExperiment with adjusting your visualization. Try changing the layout, colors, and sizes to improve clarity.\nIdentify all the paths between education_months and annual_income_ppp. Classify each path as either a causal path or a backdoor path.\nUsing the backdoor criterion, determine a sufficient adjustment set for estimating the causal effect of education_months on annual_income_ppp. Try to do this manually first, then verify your answer using the appropriate function in dagitty or ggdag.\nList the conditional independencies implied by your DAG. Using regression models, test at least two of these conditional independencies using the sociological_data dataset.\nEstimate the causal effect of education_months on annual_income_ppp using a Bayesian regression model that controls for the variables in your adjustment set.\nVisualize the posterior distribution of the causal effect estimate and interpret the results. Give the 95% credible interval for the causal effect estimate. How has your estimate changed compared to the naive model in Question 1?\nExperiment with adjusting the causal model above. Suggest two alternative models that might also be plausible. If you wish, use checks of the conditional independencies of the original model to guide your modifications.\nFor each alternative model, repeat steps 1-7 and compare the causal effect estimates you obtain. How sensitive are your results to changes in the causal model?\n\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference: With r.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer.\n\n\nPearl, Judea, and Dana Mackenzie. 2019. The Book of Why: The New Science of Cause and Effect.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” Journal of Educational Psychology.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#footnotes",
    "href": "causal_inference.html#footnotes",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "",
    "text": "Rubin (1974)↩︎\nHolland (1986)↩︎\nPearl (2000)↩︎\nAnother source of bias is when treatment effects differ between individuals. This is known as the Heterogeneous Treatment Effect (HTE) bias. In our causal models, we make the assumption that there is no HTE bias and that Y(1) - Y(0) is the same across all individuals.↩︎\nThe symbol \\(\\perp\\) is used in mathematics to mean ‘is independent of’.↩︎",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "further.html",
    "href": "further.html",
    "title": "16  Further Exercises for Practice",
    "section": "",
    "text": "16.1 Analyzing graduate salaries\nThis final chapter contains a set of scenarios and exercises that will allow you to put into further practice some of the techniques you have learned in this book, and are supplementary to the exercises provided at the end of each of the earlier chapters. All the exercises are based on data that is available in the peopleanalyticsdata package in R, or alternatively can be downloaded from the internet. While the scenarios are fictitious, they are intended to represent typical questions and situations that arise when doing statistical modeling in people analytics.\nAs you work through these scenarios, I encourage you to document your work using either an R Markdown document or a Jupyter Notebook if you prefer. This will help you keep a record of your method, approach and code in case you need to put it into practice again in the future. It will also make it easy for you to share your work with others (for example by putting it in a Github repository), which will allow you to collaborate, discuss and open your work to critique. If you are starting out on your analytics journey, exposing your work to others is one of the best ways to learn. If you are more experienced, then there are others that will undoubtedly benefit from seeing how you went about solving these problems.\nGraduate salary levels are important economic indicators of the value of tertiary education. They can provide important insight about the value of education to employers or to the economy as a whole. When studied in detail they can highlight which particular disciplines have higher or lower levels of demand for graduates, and they can be important factors in determining what subjects or majors students choose to specialize in.\nGovernment agencies and educational institutions will analyze graduate salaries regularly to help critique or validate policy or strategy. Employers will also regularly study publicly available graduate salary information to help them benchmark their compensation and benefits against the external market for graduates.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#analyzing-graduate-salaries",
    "href": "further.html#analyzing-graduate-salaries",
    "title": "16  Further Exercises for Practice",
    "section": "",
    "text": "16.1.1 The graduates data set\nThe graduates data set contains information on graduates currently in the United States across 173 specific subject majors grouped into 16 disciplines of study. This data set is sourced from the FiveThirtyEight data repository1.\nLoad the graduates data set via the peopleanalyticsdata package or download it from the internet2. The fields in the graduate data set are as follows:\n\nMajor is the specific subject major.\nDiscipline is the broad subject discipline.\nTotal is the number of graduates of working age in the US.\nUnemployment_rate is the proportion of graduates currently unemployed.\nMedian_salary is the current median salary of those employed in US dollars.\n\n\n\n16.1.2 Discussion questions\n\nWhat kind of outcome is the Median_salary column?\nWhich of the variables in the data set would you be interested in using to explain the Median_salary outcome? Why?\nAre there any transformations you would consider on any of the input variables to help with interpreting the model?\nWhat type of model would you use to try to explain the Median_salary outcome using these variables?\nDescribe the data type of each of the input variables. How would you interpret the coefficients of the model for each of these data types?\n\n\n\n16.1.3 Data exercises\n\nPerform an exploratory data analysis on the data set. Do you see any interesting patterns?\nConduct appropriate hypothesis tests on any of the input variables that interest you to determine if they relate to statistically significant differences in the Median_salary outcome.\nIf you wish to, perform transformations on the data to help with the interpretation of model results. For example, are the numerical scales intuitive for interpretation?\nRun an appropriate multivariate model to determine which input variables have a significant effect on median graduate salary.\nArticulate the results of your model, including an estimate of the effect of the input variables and the overall fit and goodness-of-fit of the model.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#analyzing-a-recruiting-process",
    "href": "further.html#analyzing-a-recruiting-process",
    "title": "16  Further Exercises for Practice",
    "section": "16.2 Analyzing a recruiting process",
    "text": "16.2 Analyzing a recruiting process\nOrganizations are often very interested in analyzing data from recruiting processes, usually with a couple of goals in mind. Firstly, there is an interest in whether the process is efficient and effective. Secondly, there is an interest in whether the process is fair to different groups and a foundation for the recruiting of a sufficiently diverse set of employees.\nThe efficiency and effectiveness of a recruiting process can depend heavily on how it is organized, what methods are used and whether those methods are helpful in determining hiring decisions. Statistics from individual elements of the process such as interviews or assessments are often studied. Models can be built to determine which elements influence the decision to hire or not to hire. In an ideal world, some sort of future job performance outcome would be particularly useful in studying the efficiency and effectiveness of a recruiting process, but often this is a very difficult thing to do, especially if the process is very selective. Hiring only a small proportion of applicants usually results in a statistical phenomenon known as range restriction, where the statistics of those hired fall in a very narrow range that makes useful analysis extremely challenging. For this reason, many organizations focus primarily on the final hiring decision as an outcome of interest.\nUnderstanding fairness in a recruiting process usually involves studying how the statistics of that process differ between subgroups of interest and whether any of the differences are significant enough to infer potential bias. Understanding whether these differences are attributable to a particular element of the process, whether it be a test or the rating behaviors of interviewers, is also important in determining whether specific action can be taken to remedy the situation.\n\n16.2.1 The recruiting data set\nThe recruiting data set contains information on 966 applicants who went through the final stages of a recruiting process for graduate positions at a large international financial services company. The recruiting process operates as follows:\n\nApplications are screened according to a number of criteria, including their SAT scores and their undergraduate GPA as well as an online aptitude test they are requested to take and numerous other judgments made by the individuals screening the applications.\nApplicants who pass the screening stage are invited for three interviews, two of them with line managers and a third with a human resources professional. Different line managers or HR professionals conduct the interviews on different interview days. Each interviewer independently gives an applicant a score on a Likert scale of 1–5 indicating increasing positive sentiment.\nInterviewers and human resources professionals gather to discuss each case and make a final decision on whether to hire or not to hire. All the information used in screening and evaluating applicants is made available to decision makers during this discussion.\n\nLoad the recruiting data set via the peopleanalyticsdata package or download it from the internet3. The fields in the recruiting data set are as follows:\n\ngender is the gender of the applicant.\nsat is the SAT score of the applicant.\ngpa is the Undergraduate GPA of the applicant.\napttest is the score on the aptitude test given to the applicant.\nint1 is the rating of the first line manager interviewer.\nint2 is the rating of the second line manager interviewer.\nint3 is the rating of the human resources interviewer.\nhired is a binary indicator of whether a decision was made to hire the applicant.\n\n\n\n16.2.2 Discussion questions\n\nConsidering the way the recruiting process works, what kinds of inferential analysis or modeling would you be interested in applying to help understand its efficiency and effectiveness?\nWhat kind of model is most appropriate for explaining the hired outcome?\nOne of your stakeholders is suggesting that the aptitude test is a waste of time and that the information it provides can already be gleaned from the applicants’ SAT scores and GPA. What kind of statistical analysis or model would help you confirm or reject this?\nDo you think that collinearity might pose a risk in this data? If so, what variables would concern you?\nWhat kind of hypothesis test would you use to determine if the hiring outcome may be different by gender?\nWhat kind of hypothesis test would you use to determine if the aptitude test score may be different by gender?\nHow would you go about determining if any gender difference in the hiring outcome can be attributed to a specific part of the process?\n\n\n\n16.2.3 Data exercises\n\nPerform an exploratory data analysis on the recruiting data set. Be sure to convert data to the best type for your purposes.\nDevelop a model to test how the aptitude test results are explained by an applicant’s SAT and GPA. What can you conclude from this? Have you considered possible collinearity in this model?\nDevelop a model to explain how all the inputs in the hiring decision (interview ratings, aptitude test, SAT and GPA) influence the hiring decision. Reduce this to the most parsimonious model you are comfortable with. What can you conclude from this model about the role of the different elements of the recruiting process in the final hiring decision?\nTest whether there is a statistically significant difference in the hiring outcome for males versus females.\nBy adding gender into your model from Data Exercise 3, determine what element or elements of the recruiting process may be related to any differences in gender in the hiring outcome.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#analyzing-the-drivers-of-performance-ratings",
    "href": "further.html#analyzing-the-drivers-of-performance-ratings",
    "title": "16  Further Exercises for Practice",
    "section": "16.3 Analyzing the drivers of performance ratings",
    "text": "16.3 Analyzing the drivers of performance ratings\nIn many organizations and for many job types, promotion and performance ratings are the primary indicators of the success of employees. However, promotion is not always available to employees and can be very dependent on role and timing. Since performance ratings are usually generated on a regular basis, it is usually these that garner the most attention in the analysis of success.\nHowever, performance ratings are not perfect indicators of reality. They are usually the result of some judgment from one or more evaluators. Part of that judgment will be informed by data and part will be informed by contextual considerations or personal preferences outside of the data. Therefore it is frequently of interest to analyze performance ratings as the outcome of a decision making process. Such analysis can inform us as to what parts of the evaluation process are operating as intended and what parts are not. Multivariate models around performance can help us understand the degree to which the evaluation process is data driven, the degree to which unfairness might exist in some evaluation decisions and what might be the source of that unfairness.\n\n16.3.1 The employee_performance data set\nThe employee_performance data set contains data on the most recent performance evaluations of 366 salespeople in a technology company. Each employee is evaluated by their manager, who considers certain performance indicators together with their own judgment and awards a performance rating from 1 to 3, where 1 means ‘Needs improvement’, 2 means ‘Performing well’ and 3 means ‘Outstanding’.\nLoad the employee_performance data set via the peopleanalyticsdata package or download it from the internet4. The fields in the employee_performance data set are as follows:\n\nsales: The sales in millions of dollars made by the salesperson during the evaluation period\nnew_customers: The number of new customers acquired by the salesperson during the evaluation period\nregion: The region that the salesperson operates in: North, South, East or West\ngender: The gender of the salesperson\nrating: The performance rating awarded by the manager\n\n\n\n16.3.2 Discussion questions\n\nWhat hypothesis test should be used to determine if there is a significant relationship between sales and performance rating?\nWhat hypothesis test should be used to determine if there is a similar distribution of performance ratings between the four regions?\nWhat type of an outcome is the performance rating? What kind of model is appropriate for explaining what influences performance ratings?\nWhich input variables would you want to be significant and which would you want to be insignificant in your model in order to support an argument that the evaluation process is fair and relevant to the job?\nWhat assumptions would you need to check after you have run your model to have confidence that you can trust your inferences?\n\n\n\n16.3.3 Data exercises\n\nRun separate and appropriate hypothesis tests on each of the input variables to determine if they have a significant effect on the performance outcome.\nPrepare your data for running an appropriate multivariate model to explain the performance outcome. Be sure to convert to appropriate data types.\nRun an appropriate multivariate model to explain the performance outcome. Report on which variables are significant and estimate the effects of the significant variables.\nDetermine and comment on the overall fit and goodness-of-fit of your model. Use this to make a comment on how ‘data driven’ you believe the evaluation process to be.\nPerform appropriate checks on the assumptions of your model. What approach might you take if any of these tests fail?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#analyzing-promotion-differences-between-groups",
    "href": "further.html#analyzing-promotion-differences-between-groups",
    "title": "16  Further Exercises for Practice",
    "section": "16.4 Analyzing promotion differences between groups",
    "text": "16.4 Analyzing promotion differences between groups\nAs mentioned in the previous section, promotion is a more challenging outcome to analyze because it can happen at different times for different people or groups. Nevertheless, over certain time periods—usually several years—organizations will be interested in understanding what affects the likelihood of promotion among their employees.\nIt is particularly interesting to compare subgroups of employees that have common characteristics to see if there is a difference in their likelihood of promotion over a specified time period. This is highly analogous to the study of retention or attrition in that promotion can be considered a singular event that can happen to different individuals at different points in time. Like retention or attrition, we are not only interested in whether this event occurred as at the end of a period of time, we are also interested in when it occurred throughout the period.\n\n16.4.1 The promotion data set\nThe promotion data set contains data on 1134 individuals who joined a retailer in an entry-level job, and tracks them for up to eight subsequent years post joining. For each individual the data records whether or not they were promoted, and if so in which year the promotion occurred, where the date of their joining is Year 0. Once promotion occurs, an individual is no longer tracked. If an individual was not promoted, then the year in which the last record occurred is captured.\nLoad the promotion data set via the peopleanalyticsdata package or download it from the internet5. The fields in the data are as follows:\n\ndiverse indicates whether or not the individual is a member of one of the organization’s diversity programs.\nflexible indicates whether the individual worked on a part-time program for at least 6 months.\nstore indicates if the individual joined the company in a retail store position.\npromoted indicates whether the individual was promoted.\nyear indicates the year in which the last record was made of the individual.\n\n\n\n16.4.2 Discussion questions\n\nWhat type of analysis is most appropriate to understand if there is a difference in promotion likelihood for employees who are on flexible hours, in diversity programs or who work in-store?\nWhat type of illustration would you use to show whether each of these three factors individually have an effect on promotion likelihood?\nWhat type of model would be most appropriate to determine the combined effects of all of these factors on promotion likelihood?\nHow would you go about determining whether any differences in promotion likelihood in a given group (such as the flexible working group) is mediated by membership of another group?\nWhat assumptions would you need to check to validate that your analysis is trustworthy?\n\n\n\n16.4.3 Data exercises\n\nRun an exploratory data analysis to understand any general patterns of interest in the data.\nPerform an analysis and generate an appropriate graph to illustrate the impact of flexible working on the likelihood of promotion. Determine if there is a statistically significant effect.\nRepeat this analysis to determine the impact of diversity program membership.\nRun an appropriate multivariate model to determine how all three variables of flexible working, diversity and in-store working affect the likelihood of promotion. Remember to check the assumptions of your model.\nHow would you explain your conclusions? Are there any corrective actions that this analysis might point to?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#analyzing-feedback-on-learning-programs",
    "href": "further.html#analyzing-feedback-on-learning-programs",
    "title": "16  Further Exercises for Practice",
    "section": "16.5 Analyzing feedback on learning programs",
    "text": "16.5 Analyzing feedback on learning programs\nAssessing the effectiveness of learning programs remains one of the most challenging problems in people analytics. As with many challenging analytics problems, measurement is the key issue. It is exceptionally difficult to track and measure the impact of learning on the future day-to-day success of the individuals who have had access to it. While it makes sense to try to understand the influence of learning on important outcomes like employment, promotion or attrition, these outcomes can often be too distant, too generic in nature and too influenced by context and other factors to expect specific learning participation to show any meaningful influence on them.\nBecause of the challenges in objectively measuring the impact of learning, analysts often rely on the reaction and feedback of learning participants as an important measure of the success of learning programs. If the content of the program is known to be important to future work and related to future success, and if the participants report that the program was effective for them, then this can create a compelling argument for the success of the program.\n\n16.5.1 The learning data set\nThe learning data set contains 4974 instances of feedback from 326 different participants in a range of learning programs offered by an executive education provider. Each row of data represents the feedback of a specific participant on a specific program that they participated in. Participants were not required to respond to all feedback questions and any question where no response was given is indicated as NA.\nLoad the learning data set via the peopleanalyticsdata package or download it from the internet6. The fields in the learning data set are as follows:\n\nidcode is the unique ID of the participant.\nrec is a binary value indicating whether the participant would recommend the program to others.\nrel is a rating from the participant on the relevance of the program to their work, where 1 is Very Low and 5 is Very High.\nfun is a rating on how enjoyable and fun the participant found the program, where 1 is Very Low and 5 is Very High\nclar is a rating from the participant on the clarity of the content and teaching in the program, where 1 is Very Low and 5 is Very High.\nhome is a rating from the participant on the quality of the homework or project work in the program, where 1 is Very Low and 5 is Very High.\nclass is a rating from the participant on the quality of the overall class who attended the program, where 1 is Very Low and 5 is Very High.\nfac is a rating from the participant on the quality of the program faculty and instructors, where 1 is Very Low and 5 is Very High.\n\n\n\n16.5.2 Discussion questions\n\nWhat kind of outcome is the rec column and which kind of model best suits this outcome type?\nDescribe the nature of the hierarchy in this data set.\nDescribe what question we would be answering if we ignored the hierarchy in modeling what influences rec.\nDescribe what question we would be answering if we considered the hierarchy in modeling what influences rec.\nWhat kind of model would you use to explicitly consider this hierarchy in modeling what influences rec? What kinds of parameters would you experiment with in running this model?\nDescribe what you would expect to see in the output of a model that considered the hierarchy in modeling what influences rec.\n\n\n\n16.5.3 Data exercises\n\nPrepare and run a model to determine which elements of feedback influence whether or not the program will be recommended to others.\nPrepare and run a separate model to determine which elements of feedback influence a participant in deciding if they would recommend a program to others.\nExperiment with the model from Data Exercise 2 by adjusting which parameters you model at the participant level.\nDescribe the different outputs of your models from Data Exercises 2 and 3 and how to interpret them.\nCompare the outputs of your models from Data Exercises 2 and 3 to those from Data Exercise 1. How might your conclusions differ between the two modeling approaches?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "further.html#footnotes",
    "href": "further.html#footnotes",
    "title": "16  Further Exercises for Practice",
    "section": "",
    "text": "https://github.com/fivethirtyeight/data↩︎\nhttp://peopleanalytics-regression-book.org/data/graduates.csv↩︎\nhttp://peopleanalytics-regression-book.org/data/recruiting.csv↩︎\nhttp://peopleanalytics-regression-book.org/data/employee_performance.csv↩︎\nhttp://peopleanalytics-regression-book.org/data/promotion.csv↩︎\nhttp://peopleanalytics-regression-book.org/data/learning.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Further Exercises for Practice</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 2007. An Introduction to Categorical Data\nAnalysis.\n\n\n———. 2010. Analysis of Ordinal Categorical Data.\n\n\nBartholomew, David J., Martin Knott, and Irini Moustaki. 2011.\nLatent Variable Models and Factor Analysis: A Unified Approach.\n\n\nBhattacharya, P. K., and Prabir Burman. 2016. Theory and Methods of\nStatistics.\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference: With\nr.\n\n\nCollett, David. 2015. Modelling Survival Data in Medical\nResearch.\n\n\nDemidenko, Eugene. 2007. “Sample Size Determination for Logistic\nRegression Revisited.” Statistics in Medicine.\n\n\nFagerland, Morten W., and David W. Hosmer. 2017. “How to Test for\nGoodness of Fit in Ordinal Logistic Regression Models.” The\nStata Journal.\n\n\nFagerland, Morten W., David W. Hosmer, and Anna M. Bofin. 2008.\n“Multinomial Goodness‐of‐fit Tests for Logistic Regression\nModels.” Statistics in Medicine.\n\n\nFriendly, Michael, and David Meyer. 2016. Discrete Data Analysis\nwith R: Visualization and Modeling Techniques for\nCategorical and Count Data.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association.\n\n\nHosmer, David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013.\nApplied Logistic Regression.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for\nStatistics, Social, and Biomedical Sciences: An Introduction.\n\n\nJeffreys, Harold. 1961. The Theory of Probability (3rd\nEdition).\n\n\nJiang, Jiming. 2007. Linear and Generalized Linear Mixed Models and\nTheir Applications.\n\n\nKruschke, John K. 2018. “Rejecting or Accepting Parameter Values\nin Bayesian Estimation.” Advances in Methods and Practices in\nPsychological Science.\n\n\nLindley, Dennis V. 1972. Bayesian Statistics: A Review.\n\n\nLumley, T., P. Diehr, S. Emerson, and L. Chen. 2002. “The\nImportance of the Normality Assumption in Large Public Health Data\nSets.” Annu Rev Public Health.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan.\n\n\nMenard, Scott. 2010. Logistic Regression: From Introductory to\nAdvanced Concepts and Applications.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2012.\nIntroduction to Linear Regression Analysis.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and\nInference.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer.\n\n\nPearl, Judea, and Dana Mackenzie. 2019. The Book of Why: The New\nScience of Cause and Effect.\n\n\nRao, C. Radhakrishna, Shalabh, Helge Toutenburg, and Christian Heumann.\n2008. The Multiple Linear Regression Model and Its Extensions.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in\nRandomized and Nonrandomized Studies.” Journal of Educational\nPsychology.\n\n\nSenn, Stephen. 2011. “Francis Galton and Regression to the\nMean.” Significance.\n\n\nSkrondal, Anders, and Sophia Rabe-Hesketh. 2004. Generalized Latent\nVariable Modeling: Multilevel, Longitudinal, and Structural Equation\nModels.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics\nwith s.\n\n\nWickham, Hadley, Garrett Grolemund, and Mine Çetinkaya-Rundel. 2023.\nR for Data Science. https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2023.\nggplot2: Elegant Graphics for Data\nAnalysis. https://ggplot2-book.org/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#sec-bayes-bin-log-reg",
    "href": "other_bayesian_regression.html#sec-bayes-bin-log-reg",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "",
    "text": "library(bayesplot)\nlibrary(ggplot2)\n\nmcmc_areas(\n  exp(as.matrix(uninf_binomial_model, pars = c(\"b_sales\"))),\n  prob = 0.66, \n  prob_outer = 0.95\n) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.1: Posterior distribution of the odds ratio for the sales coefficient in the Bayesian Binomial Logistic Regression model.\n\n\n\n\n\n\n\n\npp_check(uninf_binomial_model, type = \"stat\", ndraws = 1000, binwidth = 0.01) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 14.2: Posterior predictive check comparing the proportion of successes in the observed data (black line) to that in the posterior predictive simulations for the Bayesian Binomial Logistic Regression model (histogram).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#fitting-other-regression-models-using-python",
    "href": "other_bayesian_regression.html#fitting-other-regression-models-using-python",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.6 Fitting other regression models using Python",
    "text": "14.6 Fitting other regression models using Python\nTo fit similar models using Python, we can use bambi for model specification and fitting with arviz for summarizing and visualizing posteriors. The workflow is very similar to that which we described in Section 13.8, but specifying the appropriate model type in bambi using the family argument in the Model() constructor. The table below gives the corresponding family names in bambi for various regression models we have discussed in this chapter:\n\n\n\nRegression Model\nbambi Family Name\n\n\n\n\nLinear Regression\ngaussian\n\n\nBinomial Logistic Regression\nbernoulli\n\n\nMultinomial Logistic Regression\ncategorical\n\n\nOrdinal Logistic Regression\ncumulative\n\n\nPoisson Regression\npoisson\n\n\nNegative Binomial Regression\nnegativebinomial\n\n\n\nAs an example, to fit our Bayesian binomial logistic regression model from Section 14.1 in this chapter using bambi in Python, first we specify and build the model.\n\nimport pandas as pd\nimport bambi as bmb\nimport arviz as az\n\n# load the salespeople dataset\nurl = \"https://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# remove rows with NAs\nsalespeople = salespeople.dropna()\n\n# take a random sample of 100 observations\nsalespeople_bayes = salespeople.sample(n=100, random_state=123)\n\n# spwecify uninformed Bayesian binomial logistic regression model\nuninf_binomial_model = bmb.Model(\n    formula=\"promoted ~ sales + customer_rate + performance\",\n    data=salespeople_bayes,\n    family=\"bernoulli\"  # for binary outcome variable\n)\n\n# build and view model\nuninf_binomial_model.build()\nprint(uninf_binomial_model)\n\n       Formula: promoted ~ sales + customer_rate + performance\n        Family: bernoulli\n          Link: p = logit\n  Observations: 100\n        Priors: \n    target = p\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 1.5)\n            sales ~ Normal(mu: 0.0, sigma: 0.0053)\n            customer_rate ~ Normal(mu: 0.0, sigma: 1.064)\n            performance ~ Normal(mu: 0.0, sigma: 1.0743)\n\n\nIf we are comfortable with our priors, we proceed to fit the model:\n\n# fit the model\nfitted = uninf_binomial_model.fit(draws=10000, chains=4, random_seed=123)\n\nWe can then summarize the posterior distributions of the coefficients using arviz as follows:\n\n# summarize the posterior distributions\naz.summary(fitted)\n\n                mean     sd  hdi_3%  ...  ess_bulk  ess_tail  r_hat\nIntercept     -9.943  2.238 -14.074  ...   53135.0   31337.0    1.0\nsales          0.018  0.003   0.013  ...   41696.0   31359.0    1.0\ncustomer_rate -0.444  0.406  -1.202  ...   46972.0   31904.0    1.0\nperformance    0.197  0.346  -0.448  ...   52186.0   33108.0    1.0\n\n[4 rows x 9 columns]\n\n\nWe can plot the posterior distributions of the log-odds coefficients as in Figure 14.6.\n\n\n\n\n# Plot posterior distributions\naz.plot_posterior(fitted)\n\n\n\n\n\n\n\n\n\n\nFigure 14.6: Posterior distributions of the coefficients for the Bayesian binomial logistic regression model fitted using bambi in Python.\n\n\n\nAnd the posterior predictive check can be performed as in Figure 14.7.\n\n\n\n\n# perform posterior predictive check\nuninf_binomial_model.predict(fitted, kind=\"response\", random_seed=123)\naz.plot_ppc(fitted, num_pp_samples=1000, random_seed=123)\n\n\n\n\n\n\n\n\n\n\nFigure 14.7: Posterior predictive check comparing the proportion of successes in the observed data to that in the posterior predictive simulations for the Bayesian binomial logistic regression model fitted using bambi in Python.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian_linear_regression.html#sec-py-linear-bayes",
    "href": "bayesian_linear_regression.html#sec-py-linear-bayes",
    "title": "13  Linear Regression Using Bayesian Inference",
    "section": "13.8 Bayesian linear regression using Python",
    "text": "13.8 Bayesian linear regression using Python\nThe bambi package can be used to fit Bayesian regression models in Python, providing a high-level interface similar to brms in R, and using the PyMC package for the MCMC backend. The arviz package can be used for visualization and diagnostics of Bayesian models.\nTo fit our uninformed Bayesian linear regression model from earlier in this chapter in Python, we first specify the model and then view it to see the default priors:\n\nimport pandas as pd\nimport bambi as bmb\nimport arviz as az\n\n# load the data\nurl = \"https://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\n\n# take a random sample of 100 rows for faster computation\nugtests_bayes = ugtests.sample(n=100, random_state=123)\n\n# specify a Bayesian linear regression model with default priors\nmodel = bmb.Model(\"Final ~ Yr1 + Yr2 + Yr3\", data=ugtests_bayes, family = \"gaussian\")\nmodel.build()\nprint(model)\n\n       Formula: Final ~ Yr1 + Yr2 + Yr3\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 154.63, sigma: 626.7374)\n            Yr1 ~ Normal(mu: 0.0, sigma: 6.9113)\n            Yr2 ~ Normal(mu: 0.0, sigma: 3.6767)\n            Yr3 ~ Normal(mu: 0.0, sigma: 3.3494)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 44.2951)\n\n\nIf we wish, we can specify informative priors in the bmb.Model() constructor:\n\n# define informative priors\npriors = {\n    \"Yr1\": bmb.Prior(\"Normal\", mu=0, sigma=0.05)\n}\n\n# fit a Bayesian linear regression model with informative priors\nmodel_inf = bmb.Model(\"Final ~ Yr1 + Yr2 + Yr3\", data=ugtests_bayes, priors=priors)\nprint(model_inf)\n\n       Formula: Final ~ Yr1 + Yr2 + Yr3\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 154.63, sigma: 511.4135)\n            Yr1 ~ Normal(mu: 0.0, sigma: 0.05)\n            Yr2 ~ Normal(mu: 0.0, sigma: 3.6767)\n            Yr3 ~ Normal(mu: 0.0, sigma: 3.3494)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 44.2951)\n\n\nOnce we have specified our model, we can fit it using MCMC sampling:\n\n# fit the model using MCMC sampling\nfitted = model_inf.fit(draws=10000, chains=4, random_seed=123)\n\nAnd then we can summarize the posteriors using a handy function from arviz:\n\n# summarize the posterior distributions\naz.summary(fitted)\n\n             mean      sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat\nsigma      29.353   2.131  25.422   33.323  ...    0.011   52652.0   31445.0    1.0\nIntercept  27.338  12.527   3.741   50.868  ...    0.063   70129.0   34453.0    1.0\nYr1         0.005   0.048  -0.087    0.094  ...    0.000   57849.0   33596.0    1.0\nYr2         0.284   0.100   0.093    0.469  ...    0.000   51064.0   33476.0    1.0\nYr3         0.931   0.091   0.761    1.103  ...    0.000   51219.0   34582.0    1.0\n\n[5 rows x 9 columns]\n\n\nWe can also use arviz functions to visualize the posterior distributions and trace plots as in Figure 13.8 and Figure 13.9:\n\n\n\n\n# plot posterior distributions\naz.plot_posterior(fitted)\n\n\n\n\n\n\n\n\n\n\nFigure 13.8: Posterior distributions for the coefficients of our informed Bayesian linear regression model in Python.\n\n\n\n\n\n\n\n# plot trace plots\naz.plot_trace(fitted)\n\n\n\n\n\n\n\n\n\n\nFigure 13.9: Trace plots of the MCMC simulation for our informed Bayesian linear regression model in Python.\n\n\n\nA posterior predictive check can also be performed using arviz, as in Figure 13.10. The predict() method generates posterior predictive samples which are added to the fitted model object, and the plot_ppc() function visualizes them against the observed data.\n\n\n\n\n# perform posterior predictive check\nmodel_inf.predict(fitted, kind=\"response\", random_seed=123)\naz.plot_ppc(fitted, num_pp_samples=1000, random_seed=123)\n\n\n\n\n\n\n\n\n\n\nFigure 13.10: Posterior predictive check for our informed Bayesian linear regression model in Python.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression Using Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "causal_inference.html#causal-inference-using-dags-in-python",
    "href": "causal_inference.html#causal-inference-using-dags-in-python",
    "title": "15  Causal Inference - Moving From Association to Causation",
    "section": "15.5 Causal inference using DAGs in Python",
    "text": "15.5 Causal inference using DAGs in Python\nThe DoWhy package in Python provides a comprehensive framework for causal inference. It allows users to define causal models, identify adjustment sets and estimate causal effects using various methods, including linear regression and propensity score matching. DoWhy also integrates with other libraries like NetworkX for graph representation and visualization.\nDoWhy can be used to create and analyze DAGs, identify confounders and guide regression analyses to estimate causal effects, similar to dagitty in R. As an example, the following creates our selection process DAG as in Figure 15.3:\n\nimport pandas as pd\nfrom dowhy import CausalModel\n\n# get selection data\nurl = \"https://peopleanalytics-regression-book.org/data/selection.csv\"\nselection = pd.read_csv(url)\n\n# define the causal graph\ngraph_str = \"\"\"graph[\n  directed 1 node[id \"Hire\" label \"Hire\"]\n  node[id \"Test\" label \"Test\"]\n  node[id \"Int\" label \"Int\"]\n  node[id \"Pres\" label \"Pres\"]\n  node[id \"GPA\" label \"GPA\"]\n  edge[source \"Test\" target \"Hire\"]\n  edge[source \"Int\" target \"Hire\"]\n  edge[source \"Pres\" target \"Hire\"]\n  edge[source \"Pres\" target \"Int\"]\n  edge[source \"GPA\" target \"Pres\"]\n  edge[source \"GPA\" target \"Int\"]\n  edge[source \"GPA\" target \"Test\"]\n]\"\"\"\n\n# define treatment and outcome\ntreatment_name=\"Test\"\noutcome_name=\"Hire\"\n\n# create causal model\nselection_dag = CausalModel(\n    data = selection,\n    graph=graph_str, \n    treatment=treatment_name, \n    outcome=outcome_name\n)\n\nWe can view thw causal graph using the view_graph() method as in Figure 15.11:\n\n\n\n\n# view DAG\nselection_dag.view_model()\n\n\n\n\n\n\n\n\n\n\nFigure 15.11: Causal graph representing beliefs about causal relationships between a Hiring Decision, Test Score, Interview Score, Presentation Score, and GPA.\n\n\n\nWe can then identify the minimal adjustment set using the identify_effect() method, which confirms that GPA needs to be added to our model to estimate the causal effect of Test on Hire.\n\n# identify minimal adjustment set\nadjustments = selection_dag.identify_effect()\nprint(adjustments)\n\nEstimand type: EstimandType.NONPARAMETRIC_ATE\n\n### Estimand : 1\nEstimand name: backdoor\nEstimand expression:\n   d                \n───────(E[Hire|GPA])\nd[Test]             \nEstimand assumption 1, Unconfoundedness: If U→{Test} and U→Hire then P(Hire|Test,GPA,U) = P(Hire|Test,GPA)\n\n### Estimand : 2\nEstimand name: iv\nNo such variable(s) found!\n\n### Estimand : 3\nEstimand name: frontdoor\nNo such variable(s) found!\n\n### Estimand : 4\nEstimand name: general_adjustment\nEstimand expression:\n   d                \n───────(E[Hire|GPA])\nd[Test]             \nEstimand assumption 1, Unconfoundedness: If U→{Test} and U→Hire then P(Hire|Test,GPA,U) = P(Hire|Test,GPA)\n\n\nThe DoWhy package contains a very wide range of methods for estimating causal effects, including linear regression, propensity score matching, inverse probability weighting, and more. Its estimation capabilities do not currently extend to generalized linear models or Bayesian regression. In the context of explanatory modeling, it can be used to construct causal models which can then inform the structure of regression models to be implemented in statsmodels or bambi.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal Inference - Moving From Association to Causation</span>"
    ]
  },
  {
    "objectID": "other_bayesian_regression.html#bayesian-nixed-effects-models",
    "href": "other_bayesian_regression.html#bayesian-nixed-effects-models",
    "title": "14  Fitting Other Regression Models Using Bayesian Inference",
    "section": "14.4 Bayesian nixed effects models",
    "text": "14.4 Bayesian nixed effects models\nIn Chapter 9, we learned that mixed effects models (also known as multilevel or hierarchical models) are used when our data has a hierarchical structure, such as students nested within schools or employees nested within departments. These models allow us to account for the non-independence of observations within groups by including random effects that capture the variability between groups. In a Bayesian framework, we can fit mixed effects models using the brms package by specifying random effects in the model formula using the (1 | group_variable) syntax which we learned in Chapter 9. There is no need to specify a different family for mixed effects models in brms; we simply include the random effects in the formula and apply the appropriate model family for the outcome variable type.\nTo illustrate, let’s use a random sample from our speed_dating dataset from Chapter 9 and run a Bayesian binomial mixed effects model to understand how the various factors considered by individuals over multiple speed dates relate to the overall binary decision they made about their dates.\n\n# load the speed dating dataset\nurl &lt;- \"https://peopleanalytics-regression-book.org/data/speed_dating.csv\"\nspeed_dating &lt;- read.csv(url)\n\n# remove rows with NAs\nspeed_dating &lt;- speed_dating[complete.cases(speed_dating), ]\n\n# take a sample of 1000 observations\nset.seed(123)\nspeed_dating_bayes &lt;- speed_dating[sample(nrow(speed_dating), 1000), ]\n\n# fit Bayesian Binomial Logistic Mixed Effects model with flat priors\nuninf_mixed_binomial_model &lt;- brm(\n  formula = dec ~ agediff + samerace + attr + intel + prob + (1 | iid),\n  data = speed_dating_bayes,\n  family = bernoulli(), # for binary outcome variable\n  save_pars = save_pars('all', group = TRUE) # save group-level parameters\n)\n\nNow we can view our summary.\n\nsummary(uninf_mixed_binomial_model)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: dec ~ agediff + samerace + attr + intel + prob + (1 | iid) \n   Data: speed_dating_bayes (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nMultilevel Hyperparameters:\n~iid (Number of levels: 449) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     2.95      0.44     2.17     3.91 1.00     3157     6004\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -14.32      1.92   -18.50   -10.93 1.00     3973     6472\nagediff      -0.03      0.05    -0.13     0.07 1.00    12761    14227\nsamerace     -0.01      0.30    -0.59     0.59 1.00    12586    13611\nattr          1.36      0.16     1.08     1.69 1.00     5088     8137\nintel         0.16      0.13    -0.07     0.42 1.00     8002     9609\nprob          0.78      0.12     0.57     1.02 1.00     4607     7221\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we can see the posterior coefficient distributions for each fixed effect, representing the effect of a unit change in that variable on the log-odds of a positive decision about a date. We can also see the estimated standard deviation of the random intercepts for each individual under the Multilevel Hyperparameters, which captures the variability in baseline log-odds of a positive decision across individuals. A posterior predictive check can be done in the same way as for a standard Bayesian binomial logistic regression model.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fitting Other Regression Models Using Bayesian Inference</span>"
    ]
  }
]