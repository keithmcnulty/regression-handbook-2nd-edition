<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Keith McNulty">

<title>5&nbsp; Binomial Logistic Regression for Binary Outcomes – Handbook of Regression Modeling in People Analytics (2nd edition)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./multinomial_regression.html" rel="next">
<link href="./linear_regression.html" rel="prev">
<link href="./www/cover/coverpage-og.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-108e51fa0b95596792f9f0eaa63a27ea.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.9/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<script src="site_libs/jquery-3.5.1/jquery.min.js"></script>
<link href="site_libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./binomial_logistic_regression.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Binomial Logistic Regression for Binary Outcomes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Handbook of Regression Modeling in People Analytics (2nd edition)</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/keithmcnulty/regression-handbook-2nd-edition" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword by Alexis Fink</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./importance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">The Importance of Regression in People Analytics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basic_r.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Basics of the R Programming Language</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./primer_stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistics Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression for Continuous Outcomes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binomial_logistic_regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Binomial Logistic Regression for Binary Outcomes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multinomial_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multinomial Logistic Regression for Nominal Category Outcomes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ordinal_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Proportional Odds Logistic Regression for Ordered Category Outcomes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./poisson_nb.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Poisson, Quasi-Poisson and Negative Binomial Regression for Count Outcomes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hierarchical_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Modeling Explicit and Latent Hierarchy in Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Survival Analysis for Modeling Singular Events Over Time</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./power_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Power Analysis for Estimating Required Sample Sizes for Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Inference - A Modern Alternative to Classical Statistical Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear Regression Using Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./other_bayesian_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Fitting Other Regression Models Using Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Causal Inference - Moving From Association to Causation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./further.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Further Exercises for Practice</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliography.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#when-to-use-it" id="toc-when-to-use-it" class="nav-link active" data-scroll-target="#when-to-use-it"><span class="header-section-number">5.1</span> When to use it</a>
  <ul class="collapse">
  <li><a href="#sec-logistic-origins" id="toc-sec-logistic-origins" class="nav-link" data-scroll-target="#sec-logistic-origins"><span class="header-section-number">5.1.1</span> Origins and intuition of binomial logistic regression</a></li>
  <li><a href="#use-cases-for-binomial-logistic-regression" id="toc-use-cases-for-binomial-logistic-regression" class="nav-link" data-scroll-target="#use-cases-for-binomial-logistic-regression"><span class="header-section-number">5.1.2</span> Use cases for binomial logistic regression</a></li>
  <li><a href="#sec-walkthrough-logit" id="toc-sec-walkthrough-logit" class="nav-link" data-scroll-target="#sec-walkthrough-logit"><span class="header-section-number">5.1.3</span> Walkthrough example</a></li>
  </ul></li>
  <li><a href="#sec-mod-prob" id="toc-sec-mod-prob" class="nav-link" data-scroll-target="#sec-mod-prob"><span class="header-section-number">5.2</span> Modeling probabilistic outcomes using a logistic function</a>
  <ul class="collapse">
  <li><a href="#deriving-the-concept-of-log-odds" id="toc-deriving-the-concept-of-log-odds" class="nav-link" data-scroll-target="#deriving-the-concept-of-log-odds"><span class="header-section-number">5.2.1</span> Deriving the concept of log odds</a></li>
  <li><a href="#modeling-the-log-odds-and-interpreting-the-coefficients" id="toc-modeling-the-log-odds-and-interpreting-the-coefficients" class="nav-link" data-scroll-target="#modeling-the-log-odds-and-interpreting-the-coefficients"><span class="header-section-number">5.2.2</span> Modeling the log odds and interpreting the coefficients</a></li>
  <li><a href="#odds-versus-probability" id="toc-odds-versus-probability" class="nav-link" data-scroll-target="#odds-versus-probability"><span class="header-section-number">5.2.3</span> Odds versus probability</a></li>
  </ul></li>
  <li><a href="#running-a-multiple-binomial-logistic-regression-model" id="toc-running-a-multiple-binomial-logistic-regression-model" class="nav-link" data-scroll-target="#running-a-multiple-binomial-logistic-regression-model"><span class="header-section-number">5.3</span> Running a multiple binomial logistic regression model</a>
  <ul class="collapse">
  <li><a href="#running-and-interpreting-a-multiple-binomial-logistic-regression-model" id="toc-running-and-interpreting-a-multiple-binomial-logistic-regression-model" class="nav-link" data-scroll-target="#running-and-interpreting-a-multiple-binomial-logistic-regression-model"><span class="header-section-number">5.3.1</span> Running and interpreting a multiple binomial logistic regression model</a></li>
  <li><a href="#sec-logistic-gof" id="toc-sec-logistic-gof" class="nav-link" data-scroll-target="#sec-logistic-gof"><span class="header-section-number">5.3.2</span> Understanding the fit and goodness-of-fit of a binomial logistic regression model</a></li>
  <li><a href="#model-parsimony" id="toc-model-parsimony" class="nav-link" data-scroll-target="#model-parsimony"><span class="header-section-number">5.3.3</span> Model parsimony</a></li>
  </ul></li>
  <li><a href="#other-considerations-in-binomial-logistic-regression" id="toc-other-considerations-in-binomial-logistic-regression" class="nav-link" data-scroll-target="#other-considerations-in-binomial-logistic-regression"><span class="header-section-number">5.4</span> Other considerations in binomial logistic regression</a></li>
  <li><a href="#binomial-logistic-regression-using-python" id="toc-binomial-logistic-regression-using-python" class="nav-link" data-scroll-target="#binomial-logistic-regression-using-python"><span class="header-section-number">5.5</span> Binomial logistic regression using Python</a></li>
  <li><a href="#learning-exercises" id="toc-learning-exercises" class="nav-link" data-scroll-target="#learning-exercises"><span class="header-section-number">5.6</span> Learning exercises</a>
  <ul class="collapse">
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions"><span class="header-section-number">5.6.1</span> Discussion questions</a></li>
  <li><a href="#data-exercises" id="toc-data-exercises" class="nav-link" data-scroll-target="#data-exercises"><span class="header-section-number">5.6.2</span> Data exercises</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/keithmcnulty/regression-handbook-2nd-edition/edit/main/binomial_logistic_regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/keithmcnulty/regression-handbook-2nd-edition/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-bin-log-reg" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Binomial Logistic Regression for Binary Outcomes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Binary (two class) problems are very common. Hiring, promotion and attrition are often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Multi-class outcomes like performance ratings on an ordinal scale, or survey responses on a Likert scale are often converted to binary outcomes by dividing the ratings into two groups, for example ‘High’ and ‘Not High’.</p>
<p>In any situation where our outcome is binary, we are effectively working with likelihoods. These are not generally linear in nature, and so we no longer have the comfort of our inputs being <em>directly</em> linearly related to our outcome. Therefore direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. Instead, linear relationships can be inferred on <em>transformations</em> of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of <em>generalized linear models</em> or <em>GLMs</em>. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved but worth the time investment in order to build a competent understanding of how to interpret these types of models.</p>
<section id="when-to-use-it" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="when-to-use-it"><span class="header-section-number">5.1</span> When to use it</h2>
<section id="sec-logistic-origins" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="sec-logistic-origins"><span class="header-section-number">5.1.1</span> Origins and intuition of binomial logistic regression</h3>
<p>Imagine that we have a set of observations of a random variable that is binary or dichotomous in nature. For convenience, let’s call the two values that any observation can take ‘success’ and ‘failure’. Let’s say that the probability of an observation being a success is <span class="math inline">\(p\)</span>, and therefore the probability of an observation being a failure is <span class="math inline">\(1 - p\)</span>. Such a random variable is known as a <em>Bernoulli random variable</em>.</p>
<p>Let’s also imagine we take a sample of <span class="math inline">\(n\)</span> observations of a Bernoulli random variable. Then the number of successes in the sample, which we will call <span class="math inline">\(Y\)</span>, will follow a <em>binomial distribution</em> with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. The probability of observing exactly <span class="math inline">\(k\)</span> successes in our sample is given by the formula:</p>
<p><span class="math display">\[
P(Y = k) = \binom{n}{k} p^k (1 - p)^{n - k}
\]</span></p>
<p>where <span class="math inline">\(\binom{n}{k} = \frac{n!}{k!(n-k)!}\)</span> is the binomial coefficient, which gives the number of ways of choosing a subset of size <span class="math inline">\(k\)</span> from a total of <span class="math inline">\(n\)</span> observations. Here is the distribution of <span class="math inline">\(Y\)</span> for <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 0.3\)</span>, shown in <a href="#fig-binomial-dist" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>:</p>
<div id="fig-binomial-dist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-binomial-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binomial-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Binomial distribution showing the probability of <span class="math inline">\(k\)</span> successes in 100 observations with a success probability of 0.3
</figcaption>
</figure>
</div>
<p>Now, although the variable <span class="math inline">\(Y\)</span> here is discrete (can only take non-negative integer values), we can see that the shape of <span class="math inline">\(Y\)</span> resembles a normal distribution. In fact, as long as <span class="math inline">\(n\)</span> is not very small and <span class="math inline">\(p\)</span> is not very biased<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, the binomial distribution can be well approximated by a normal distribution with mean <span class="math inline">\(np\)</span> and variance <span class="math inline">\(np(1 - p)\)</span> (standard deviation <span class="math inline">\(\sqrt{np(1-p)}\)</span>) as shown in <a href="#fig-binomial-normal-approx" class="quarto-xref">Figure&nbsp;<span>5.2</span></a> for <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 0.3\)</span>.</p>
<div id="fig-binomial-normal-approx" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-binomial-normal-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binomial-normal-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Binomial distribution with <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 0.3\)</span> (blue bars) and normal approximation with <span class="math inline">\(\mu = 30\)</span>, <span class="math inline">\(\sigma = \sqrt{21}\)</span> (red line)
</figcaption>
</figure>
</div>
<p>Now if we consider tha <em>cumulative</em> probability of <span class="math inline">\(Y\)</span>, that is, the probability of observing <em>up to and including <span class="math inline">\(k\)</span> successes</em>, we can see that this cumulative probability takes on an S-shape, which is also well approximated by the cumulative normal distribution, as shown in <a href="#fig-binomial-cum-prob" class="quarto-xref">Figure&nbsp;<span>5.3</span></a> for <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 0.3\)</span>.</p>
<div id="fig-binomial-cum-prob" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-binomial-cum-prob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binomial-cum-prob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Cumulative binomial distribution with <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 0.3\)</span> (blue bars) and normal approximation with <span class="math inline">\(\mu = 30\)</span>, <span class="math inline">\(\sigma = \sqrt{21}\)</span> (red line)
</figcaption>
</figure>
</div>
<p>We will be interested in modeling this cumulative probability of success, in order to understand how various input variables might influence it. Because we know that it is well approximated by the normal distribution, we can utilize a function that has very similar characteristics to the normal distribution, but is easier to work with mathematically.</p>
<p>The <em>logistic function</em> was first introduced by the Belgian mathematician Pierre François Verhulst in the early 1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population. The formula for Verhulst’s function was:</p>
<p><span class="math display">\[
y = \frac{L}{1 + e^{-k(x - x_0)}}
\]</span> where <span class="math inline">\(e\)</span> is the exponential constant, <span class="math inline">\(x_0\)</span> is the value of <span class="math inline">\(x\)</span> at the midpoint, <span class="math inline">\(L\)</span> is the maximum value of <span class="math inline">\(y\)</span> (known as the ‘carrying capacity’) and <span class="math inline">\(k\)</span> is the maximum gradient of the curve.</p>
<p>The logistic function, as shown in <a href="#fig-logistic-function-verhulst" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>, was felt to accurately capture the theorized stages of population growth, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity.</p>
<div id="fig-logistic-function-verhulst" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-function-verhulst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="www/05/logistic-curve.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-function-verhulst-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Verhulst’s logistic function modeled both the exponential nature and the natural limit of population growth
</figcaption>
</figure>
</div>
<p>In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was observed that the logistic function has a similar S-shape (or <em>sigmoid</em>) to a cumulative normal distribution of probability, as depicted in <a href="#fig-norm-log-curves" class="quarto-xref">Figure&nbsp;<span>5.5</span></a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, where the <span class="math inline">\(x\)</span> scale for the normal distribution represents standard deviations around the mean. As we will learn, the logistic function gives rise to a mathematical model where the coefficients are easily interpreted in terms of likelihood of the outcome. Unsurprisingly, therefore, the logistic model soon became a common approach to modeling probabilistic phenomena.</p>
<div id="fig-norm-log-curves" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-norm-log-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-norm-log-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret
</figcaption>
</figure>
</div>
</section>
<section id="use-cases-for-binomial-logistic-regression" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="use-cases-for-binomial-logistic-regression"><span class="header-section-number">5.1.2</span> Use cases for binomial logistic regression</h3>
<p>Binomial logistic regression can be used when the outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is an underlying assumption that the cumulative probability of the outcome takes a shape similar to a cumulative normal distribution.</p>
<p>Here are some example questions that could be approached using binomial logistic regression:</p>
<ul>
<li>Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what influence do these factors have on the likelihood of the individual receiving a high performance rating?</li>
<li>Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election?</li>
<li>Given a set of statistics about the in-game activity of soccer players, what relationship does each statistic have with the likelihood of a player scoring a goal?</li>
</ul>
</section>
<section id="sec-walkthrough-logit" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="sec-walkthrough-logit"><span class="header-section-number">5.1.3</span> Walkthrough example</h3>
<p>You are an analyst for a large company consisting of regional sales teams across the country. Twice every year, this company promotes some of its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment.</p>
<p>You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a data set containing data for the last three years of salespeople considered for promotion. The <code>salespeople</code> data set contains the following fields:</p>
<ul>
<li><code>promoted</code>: A binary value indicating 1 if the individual was promoted and 0 if not</li>
<li><code>sales</code>: the sales (in thousands of dollars) attributed to the individual in the period of the promotion</li>
<li><code>customer_rate</code>: the average satisfaction rating from a survey of the individual’s customers during the promotion period</li>
<li><code>performance</code>: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest)</li>
</ul>
<p>Let’s take a quick look at the data.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if needed, download salespeople data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">"https://peopleanalytics-regression-book.org/data/salespeople.csv"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>salespeople <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(url)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># look at the first few rows of data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(salespeople)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  promoted sales customer_rate performance
1        0   594          3.94           2
2        0   446          4.06           3
3        1   674          3.83           4
4        0   525          3.62           2
5        1   657          4.40           3
6        1   918          4.54           2</code></pre>
</div>
</div>
<p>The data looks as expected. Let’s get a summary of the data.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(salespeople)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>    promoted          sales       customer_rate    performance 
 Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  
 1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  
 Median :0.0000   Median :475.0   Median :3.620   Median :3.0  
 Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  
 3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  
 Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  
                  NA's   :1       NA's   :1       NA's   :1    </code></pre>
</div>
</div>
<p>First we see a small number of missing values, and we should remove those observations. We see that about a third of individuals were promoted, that sales ranged from $151k to $945k, that as expected the average satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and <code>promoted</code> is numeric when it should be categorical. Let’s convert these, and then let’s do a pairplot to get a quick view on some possible underlying relationships, as in <a href="#fig-log-pairplot" class="quarto-xref">Figure&nbsp;<span>5.6</span></a>.</p>
<div id="fig-log-pairplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-log-pairplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># remove NAs</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>salespeople <span class="ot">&lt;-</span> salespeople[<span class="fu">complete.cases</span>(salespeople), ]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert performance to ordered factor and promoted to categorical</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>salespeople<span class="sc">$</span>performance <span class="ot">&lt;-</span> <span class="fu">ordered</span>(salespeople<span class="sc">$</span>performance, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>salespeople<span class="sc">$</span>promoted <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(salespeople<span class="sc">$</span>promoted)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># generate pairplot</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(salespeople)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-log-pairplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: Pairplot for the <code>salespeople</code> data set
</figcaption>
</figure>
</div>
<p>We can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t think much of you, sales wouldn’t likely be very high).</p>
<p>So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this.</p>
</section>
</section>
<section id="sec-mod-prob" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-mod-prob"><span class="header-section-number">5.2</span> Modeling probabilistic outcomes using a logistic function</h2>
<p>Imagine that you have an outcome event <span class="math inline">\(y\)</span> which either occurs or does not occur. The probability of <span class="math inline">\(y\)</span> occurring, or <span class="math inline">\(P(y = 1)\)</span>, obviously takes a value between 0 and 1. Now imagine that some input variable <span class="math inline">\(x\)</span> has a positive effect on the probability of the event occurring. Then you would naturally expect <span class="math inline">\(P(y = 1)\)</span> to increase as <span class="math inline">\(x\)</span> increases.</p>
<p>In our <code>salespeople</code> data set, let’s plot our <code>promotion</code> outcome against the <code>sales</code> input. This can be seen in <a href="#fig-prom-sales-plot" class="quarto-xref">Figure&nbsp;<span>5.7</span></a>.</p>
<div id="fig-prom-sales-plot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prom-sales-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prom-sales-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: Plot of promotion against sales in the <code>salespeople</code> data set
</figcaption>
</figure>
</div>
<p>It’s clear that promotion is more likely with higher sales levels. As we move along the <span class="math inline">\(x\)</span> axis from left to right and gradually include more and more individuals with higher sales, we know that the probability of promotion is gradually increasing overall. We could try to model this probability using our logistic function, which we learned about in <a href="#sec-logistic-origins" class="quarto-xref"><span>Section 5.1.1</span></a>. For example, let’s plot the logistic function <span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-k(x - x_{0})}}
\]</span></p>
<p>on this data, where we set <span class="math inline">\(x_0\)</span> to the mean of <code>sales</code> and <span class="math inline">\(k\)</span> to be some maximum gradient value. In <a href="#fig-prom-with-logistic" class="quarto-xref">Figure&nbsp;<span>5.8</span></a> we can see these logistic functions for different values of <span class="math inline">\(k\)</span>. All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best-fitting logistic function?</p>
<div id="fig-prom-with-logistic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prom-with-logistic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prom-with-logistic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.8: Overlaying logistic functions with various gradients onto previous plot
</figcaption>
</figure>
</div>
<section id="deriving-the-concept-of-log-odds" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="deriving-the-concept-of-log-odds"><span class="header-section-number">5.2.1</span> Deriving the concept of log odds</h3>
<p>Let’s look more carefully at the index of the exponential constant <span class="math inline">\(e\)</span> in the denominator of our logistic function. Note that, because <span class="math inline">\(x_{0}\)</span> is a constant, we have:</p>
<p><span class="math display">\[
-k(x - x_{0}) = -(-kx_{0} + kx) = -(\beta_{0} + \beta_1x)
\]</span> where <span class="math inline">\(\beta_0 = -kx_0\)</span> and <span class="math inline">\(\beta_{1} = k\)</span>. Therefore,</p>
<p><span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
\]</span></p>
<p>This equation makes intuitive sense. As the value of <span class="math inline">\(x\)</span> increases, the value <span class="math inline">\(e^{-(\beta_0 + \beta_1x)}\)</span> gets smaller and smaller towards zero, and thus <span class="math inline">\(P(y = 1)\)</span> approaches its theoretical maximum value of 1. As the value of <span class="math inline">\(x\)</span> decreases towards zero, we see that the value of <span class="math inline">\(P(y = 1)\)</span> approaches a minimum value of <span class="math inline">\(\frac{1}{1 + e^{-\beta_0}}\)</span>. Referring back to our salespeople example, we can thus see that <span class="math inline">\(\beta_0\)</span> helps determine the baseline probability of promotion assuming no sales at all. If <span class="math inline">\(\beta_0\)</span> has an extremely negative value, this baseline probability will approach its theoretical minimum of zero.</p>
<p>Let’s formalize the role of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the likelihood of a positive outcome. We know that for any binary event <span class="math inline">\(y\)</span>, <span class="math inline">\(P(y = 0)\)</span> is equal to <span class="math inline">\(1 - P(y = 1)\)</span>, so</p>
<p><span class="math display">\[
\begin{aligned}
P(y = 0) &amp;= 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{1 + e^{-(\beta_0 + \beta_1x)} - 1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}
\end{aligned}
\]</span></p>
<p>Putting these together, we find that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= \frac{\frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}}{\frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}} \\
&amp;= \frac{1}{e^{-(\beta_0 + \beta_1x)}} \\
&amp;= e^{\beta_0 + \beta_1x}
\end{aligned}
\]</span></p>
<p>or alternatively, if we apply the natural logarithm to both sides</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x
\]</span></p>
<p>The right-hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left-hand side?</p>
<p><span class="math inline">\(P(y = 1)\)</span> is the probability that the event will occur, while <span class="math inline">\(P(y = 0)\)</span> is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the <em>odds</em> of an event. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will not<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Therefore we can conclude that the natural logarithm of the odds of <span class="math inline">\(y\)</span>—usually termed the <em>log odds</em> of <span class="math inline">\(y\)</span>—is linear in <span class="math inline">\(x\)</span>, and therefore we can model the log odds of <span class="math inline">\(y\)</span> using similar linear regression methods to those studied in <a href="linear_regression.html" class="quarto-xref"><span>Chapter 4</span></a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="modeling-the-log-odds-and-interpreting-the-coefficients" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="modeling-the-log-odds-and-interpreting-the-coefficients"><span class="header-section-number">5.2.2</span> Modeling the log odds and interpreting the coefficients</h3>
<p>Let’s take our simple case of regressing the <code>promoted</code> outcome against <code>sales</code>. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run a binomial model </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sales_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> promoted <span class="sc">~</span> sales, </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> salespeople, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># view the coefficients</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>sales_model<span class="sc">$</span>coefficients</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)        sales 
-21.77642020   0.03675848 </code></pre>
</div>
</div>
<p>We can interpret the coefficients as follows:</p>
<ol type="1">
<li><p>The <code>(Intercept)</code> coefficient is the value of the log odds with zero input value of <span class="math inline">\(x\)</span>—it is the log odds of promotion if you made no sales.</p></li>
<li><p>The <code>sales</code> coefficient represents the increase in the log odds of promotion associated with each unit increase in sales.</p></li>
</ol>
<p>We can convert these coefficients from log odds to odds by applying the exponent function, to return to the identity we had previously</p>
<p><span class="math display">\[
\frac{P(y = 1)}{P(y = 0)} = e^{\beta_0 + \beta_1x} = e^{\beta_0}(e^{\beta_1})^x
\]</span></p>
<p>From this, we can interpret that <span class="math inline">\(e^{\beta_0}\)</span> represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by <span class="math inline">\(e^{\beta_1}\)</span>. Given this multiplicative effect that <span class="math inline">\(e^{\beta_1}\)</span> has on the odds, it is known as an <em>odds ratio</em>.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert log odds to base odds and odds ratio</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(sales_model<span class="sc">$</span>coefficients)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> (Intercept)        sales 
3.488357e-10 1.037442e+00 </code></pre>
</div>
</div>
<p>So we can see that the base odds of promotion with zero sales is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (that is, nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04—in other words, it increases the odds of promotion by 4%.</p>
</section>
<section id="odds-versus-probability" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="odds-versus-probability"><span class="header-section-number">5.2.3</span> Odds versus probability</h3>
<p>It is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model.</p>
<p>If a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a probability of 1, the odds become exponentially large, as illustrated in <a href="#fig-odds-prob" class="quarto-xref">Figure&nbsp;<span>5.9</span></a>):</p>
<div id="fig-odds-prob" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odds-prob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odds-prob-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.9: Odds plotted against probability
</figcaption>
</figure>
</div>
<p>The consequence of this is that a given increase in odds can have a different effect on probability depending on what the original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116, which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. <a href="#fig-pcoddsplot" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> shows the impact of a 4% increase in odds according to the original probability of the event.</p>
<div id="fig-pcoddsplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pcoddsplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcoddsplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.10: Effect of 4% increase in odds plotted against original probability
</figcaption>
</figure>
</div>
<p>We can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for converting odds to probability and vice versa. If <span class="math inline">\(O\)</span> represents odds and <span class="math inline">\(P\)</span> represents probability then we have:</p>
<p><span class="math display">\[
\begin{aligned}
O &amp;= \frac{P}{1 - P} \\
P &amp;= \frac{O}{1 + O}
\end{aligned}
\]</span></p>
</section>
</section>
<section id="running-a-multiple-binomial-logistic-regression-model" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="running-a-multiple-binomial-logistic-regression-model"><span class="header-section-number">5.3</span> Running a multiple binomial logistic regression model</h2>
<p>The derivations in the previous section extend to multivariable data. Let <span class="math inline">\(y\)</span> be a dichotomous outcome, and let <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> be our input variables. Then</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p
\]</span> for coefficients <span class="math inline">\(\beta_0, \beta_1,\dots, \beta_p\)</span>. As before:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> represents the log odds of our outcome when all inputs are zero</li>
<li>Each <span class="math inline">\(\beta_i\)</span> represents the increase in the log odds of our outcome associated with a unit change in <span class="math inline">\(x_i\)</span>, assuming no change in other inputs.</li>
</ul>
<p>Applying an exponent as before, we have</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p} \\
&amp;= e^{\beta_0}(e^{\beta_1})^{x_1}(e^{\beta_2})^{x_2}\dots(e^{\beta_p})^{x_p}
\end{aligned}
\]</span></p>
<p>Therefore we can conclude that:</p>
<ul>
<li><span class="math inline">\(e^{\beta_0}\)</span> represents the odds of the outcome when all inputs are zero.</li>
<li>Each <span class="math inline">\(e^{\beta_i}\)</span> represents the <em>odds ratio</em> associated with a unit increase in <span class="math inline">\(x_i\)</span> assuming no change in the other inputs (that is, a unit increase in <span class="math inline">\(x_i\)</span> multiplies the odds of our outcome by <span class="math inline">\(e^{\beta_i}\)</span>).</li>
</ul>
<p>Let’s put this into practice.</p>
<section id="running-and-interpreting-a-multiple-binomial-logistic-regression-model" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="running-and-interpreting-a-multiple-binomial-logistic-regression-model"><span class="header-section-number">5.3.1</span> Running and interpreting a multiple binomial logistic regression model</h3>
<p>Let’s use a binomial logistic regression model to understand how each of the three inputs in our <code>salespeople</code> data set influence the likelihood of promotion.</p>
<p>First, as we learned previously, it is good practice to convert the categorical <code>performance</code> variable to a dummy variable<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(makedummies)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># convert performance to dummy</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>salespeople_dummies <span class="ot">&lt;-</span> makedummies<span class="sc">::</span><span class="fu">makedummies</span>(salespeople)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># check it worked</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(salespeople_dummies)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  promoted sales customer_rate performance_2 performance_3 performance_4
1        0   594          3.94             1             0             0
2        0   446          4.06             0             1             0
3        1   674          3.83             0             0             1
4        0   525          3.62             1             0             0
5        1   657          4.40             0             1             0
6        1   918          4.54             1             0             0</code></pre>
</div>
</div>
<p>Now we can run our model (using the formula <code>promoted ~ .</code> to mean regressing <code>promoted</code> against everything else) and view our coefficients.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run binomial glm</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>full_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> <span class="st">"promoted ~ ."</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">family =</span> <span class="st">"binomial"</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> salespeople_dummies)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># get coefficient summary </span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>(coefs <span class="ot">&lt;-</span> <span class="fu">summary</span>(full_model)<span class="sc">$</span>coefficients)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  Estimate  Std. Error    z value     Pr(&gt;|z|)
(Intercept)   -19.85893195 3.444078811 -5.7661085 8.112287e-09
sales           0.04012425 0.006576429  6.1012212 1.052611e-09
customer_rate  -1.11213130 0.482681585 -2.3040682 2.121881e-02
performance_2   0.26299953 1.021980179  0.2573431 7.969139e-01
performance_3   0.68495453 0.982166998  0.6973911 4.855581e-01
performance_4   0.73449340 1.071963758  0.6851849 4.932272e-01</code></pre>
</div>
</div>
<p>Note how only three of the <code>performance</code> dummies have displayed. This is because everyone is in one of the four performance categories, so the model is using <code>performance_1</code> as the reference case. We can interpret each performance coefficient as the effect of a move to that performance category from <code>performance_1</code>.</p>
<p>We can already see from the last column of our coefficient summary—the coefficient p-values—that only <code>sales</code> and <code>customer_rate</code> meet the significance threshold of less than 0.05. Interestingly, it appears from the <code>Estimate</code> column that <code>customer_rate</code> has a negative effect on the log odds of promotion. For convenience, we can add an extra column to our coefficient summary to create the exponents of our estimated coefficients so that we can see the odds ratios. We can also remove columns that are less useful to us if we wish.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create coefficient table with estimates, p-values and odds ratios</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>(full_coefs <span class="ot">&lt;-</span> <span class="fu">cbind</span>(coefs[ ,<span class="fu">c</span>(<span class="st">"Estimate"</span>, <span class="st">"Pr(&gt;|z|)"</span>)], </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">odds_ratio =</span> <span class="fu">exp</span>(full_model<span class="sc">$</span>coefficients))) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  Estimate     Pr(&gt;|z|)   odds_ratio
(Intercept)   -19.85893195 8.112287e-09 2.373425e-09
sales           0.04012425 1.052611e-09 1.040940e+00
customer_rate  -1.11213130 2.121881e-02 3.288573e-01
performance_2   0.26299953 7.969139e-01 1.300826e+00
performance_3   0.68495453 4.855581e-01 1.983682e+00
performance_4   0.73449340 4.932272e-01 2.084426e+00</code></pre>
</div>
</div>
<p>Now we can interpret our model as follows:</p>
<ul>
<li>All else being equal, sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%</li>
<li>All else being equal, customer ratings have a significant negative effect on the likelihood of promotion, with one full rating higher associated with 67% lower odds of promotion</li>
<li>All else being equal, performance ratings have no significant effect on the likelihood of promotion</li>
</ul>
<p>The second conclusion may appear counter-intuitive, but remember from our pairplot in <a href="#sec-walkthrough-logit" class="quarto-xref"><span>Section 5.1.3</span></a> that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act <em>assuming all other variables are the same</em>. Therefore, if two individuals have the same sales and performance ratings, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion.</p>
<p>Many analysts will feel uncomfortable with stating these conclusions with too much precision, and therefore exponent confidence intervals can be calculated to provide a range for the odds ratios.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(full_model))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                     2.5 %       97.5 %
(Intercept)   7.879943e-13 7.385387e-07
sales         1.029762e+00 1.057214e+00
customer_rate 1.141645e-01 7.793018e-01
performance_2 1.800447e-01 1.061602e+01
performance_3 3.060299e-01 1.547188e+01
performance_4 2.614852e-01 1.870827e+01</code></pre>
</div>
</div>
<p>Therefore we can say that—all else being equal—every additional unit of sales increases the odds of promotion by between 3.0% and 5.7%, and every additional point in customer rating decreases the odds of promotion by between 22% and 89%.</p>
<p>Similar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of up to 89% in odds seems a lot more important than an increase of up to 5.7% in odds. However, the increase of up to 5.7% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of up to 89% is on a full customer rating point on a scale of only 4 full points.</p>
</section>
<section id="sec-logistic-gof" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="sec-logistic-gof"><span class="header-section-number">5.3.2</span> Understanding the fit and goodness-of-fit of a binomial logistic regression model</h3>
<p>Understanding the fit of a binomial logistic regression model is not straightforward and is sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simplify model</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>simpler_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> promoted <span class="sc">~</span> sales <span class="sc">+</span> customer_rate,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">"binomial"</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> salespeople)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>As in the previous chapter, again we have the luxury of a three-dimensional model, so we can visualize it in <a href="#fig-log-reg-3d" class="quarto-xref">Figure&nbsp;<span>5.12</span></a>, revealing a 3D sigmoid curve which ‘twists’ to reflect the relative influence of <code>sales</code> and <code>customer_rate</code> on the outcome.</p>
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-log-reg-3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-0c5a007adf130e5fa4b4" style="width:100%;height:464px;"></div>
<script type="application/json" data-for="htmlwidget-0c5a007adf130e5fa4b4">{"x":{"visdat":{"922b19e7d5ec":["function () ","plotlyVisDat"]},"cur_data":"922b19e7d5ec","attrs":{"922b19e7d5ec":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"mode":"markers","type":"scatter3d","marker":{"size":5,"color":"blue","symbol":104},"name":"Observations","inherit":true},"922b19e7d5ec.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[0.51304378747036605,0.0023290366332891065,0.96791325675030548,0.085055170809734876,0.88899508542332994,0.99999614001934445,3.9415846469361267e-05,3.3526370619405734e-05,5.0106017726651335e-05,0.0007071882478328032,0.27697229377769361,0.9971420892282028,0.27693778229032362,0.0070709812140732431,0.00012184293115255033,0.00014545030098594099,6.8491527318655281e-06,0.00014321157953073557,0.0015289282925672232,0.99999699754650007,0.99241673066241354,0.0048063312419649954,0.050910564453653703,5.8628538706006359e-05,0.99980982241345651,0.99694331729095742,0.00011861546537628204,2.2450005899478837e-06,2.1214280004874962e-05,0.99775580694991239,0.016036248583680499,0.017208780600967997,3.251729612447346e-06,0.99995799273909569,0.00065964499544458085,0.99899372927826513,0.99800632442776238,0.0016350908703035426,0.99999879815048631,0.14282719379847175,0.00052361457648374002,1.0280406943761884e-05,0.9999989978988828,0.055555992634510949,7.955724398476975e-05,0.00078402305155006707,0.015720918156021738,0.0012331624585504774,0.0091551843473662662,0.96463508618113492,0.29899149817867704,0.0017216911187149686,0.007311494155589922,0.0029926706262928251,8.1348627735226153e-05,0.00099196858908720781,0.99984904633485516,0.0001694047115080356,0.99881729861370983,0.00087874561177735556,0.99940564349091898,0.0019427004013640537,0.99495641215681219,0.96940927140424382,0.00098304502670207902,0.044122862433742437,0.036672143743223105,0.011895056870476307,0.09730877461781412,5.9320821088611448e-06,5.1821303396215897e-05,0.0001226828026446934,0.1303804896447664,0.0012194896705953113,0.086098936085861319,0.6542575318601207,0.99981109171099136,0.0020825258785462184,0.00057400940535780245,0.033912541626999776,0.99974082925032526,0.18841996879762404,0.0055577353214504178,0.98533826894573773,0.91573089402718411,0.00085938512230808624,0.0039938342606074045,0.99911826782934277,3.917346746439537e-06,0.0010972604175767157,0.9994829412829439,0.63263064701271443,0.99999875165777696,0.1902037381917544,0.00022284075695328494,0.0018085228949677428,0.032682631919285936,0.0015050822414446679,0.87598158130774106,0.00066712164962269441,0.00038420744661276995,0.00086723691551421366,0.00026654467279782994,0.0044752567006351406,0.023412393006146358,0.900628860957564,0.00082007828253893638,0.0046996361138977889,5.252917843669373e-05,0.00045658288387549756,0.99830783831499526,0.76223049790284414,0.00058311244522132855,0.014266535715782272,0.00051651721979582702,0.00042785910127370784,0.00036655278360342555,0.0096102989524477697,0.99999773695658756,0.99733307281658834,0.23508414554955109,0.99648077593570106,0.22953554089936407,0.0017602922467597644,0.01000385048558659,0.0014389537746719518,0.99895925582698197,0.99998765784876553,0.0057462539301270758,0.99999932639894273,6.3450182926456959e-06,0.0015423128093645928,0.00032619079841053573,0.0062146454450793479,3.9059811993662433e-05,0.99999674495314139,5.7389157505198448e-07,1.5528951992312294e-05,0.99967637866056114,0.0028810388182021127,0.00018369090980264739,0.93634930744356992,0.0061865986718418024,0.99999902695747767,0.0011019758107580072,0.00015877189117223891,0.00029161240487343736,0.023774400373556635,0.0044547698399391676,0.0013726235398632688,0.0090904836788513119,6.3601135172150033e-06,0.99973845989494048,2.455282180494587e-05,0.02173265115569983,0.99982182024230593,0.99984354833540134,0.99744436066869857,8.9989444663976005e-05,3.77781855569173e-06,0.99997553514464321,0.00067783650979910584,0.00041191882277807359,0.00504967855302502,0.99999933692064391,0.01254623269098374,0.014103604670684779,0.00043470779617807903,0.99999932959264359,0.99835930918194693,0.0025582934759559128,0.12709933152061911,0.99676094350262989,0.9833337325224093,6.9235778881309706e-05,0.93278095717935705,0.048899492572319819,0.91572646085629072,0.26231270573387228,0.012079812579912514,0.00033142215555053784,0.40915925473354198,0.87256176682338482,0.99934408759309046,0.0011974120550461587,0.00079629398434484457,0.89118761320262929,0.94415399972261016,0.99609126241652035,6.693257702375403e-05,0.0008291100475419273,0.0028813688978400471,0.0017846030462786576,0.0007650392307984548,2.8734167952632146e-06,3.8965990441351794e-05,0.9582346238623578,0.50406583559299933,0.99973967717072032,0.9999722038690072,0.0010492121203519965,0.97159380299331299,0.9480637808410376,0.0082988346485234257,0.0042790652228158789,0.34092411529973055,0.00012625904134048321,0.00028440922633490384,0.99997909217582026,0.00047980611565371435,0.0018004798476501578,0.99999582677410637,0.74093313103188507,0.030953852350014122,0.014746076810555789,0.0003335720671835084,0.0096545400760163751,0.99535644062420714,0.99251922871018627,0.96126841290298504,0.013111641004797512,9.9223647506070375e-07,0.99999727434573704,0.0012331270819546582,0.0010901312531541433,0.022118752212858823,3.5870396105639534e-05,0.99947358263883412,0.0013248996220309875,0.99999976957577241,0.99915314059995619,0.0074253806275747496,0.0004950469066561465,0.039974192039368167,0.028009469676191438,0.045849207277917504,0.090815582137632245,0.99713530950094942,0.0087740321902467239,0.99999968380429438,0.99689496461488714,0.00028836783398084888,0.0017723552205261295,0.45706390398533314,0.57859012934016429,0.0014554592433055347,0.0026457962736067321,0.99553943054857741,0.0018202893948287753,0.00059112433197931475,0.00036250592960833958,3.0790540845672001e-05,0.99999806228423704,8.9640057315764252e-06,0.99999787981238353,0.9996412130899206,0.99991692048426917,0.00081626354708175936,0.013552896352098231,6.4886446899741766e-06,0.00026369249809228285,1.1130584183612454e-07,0.0042608129416333747,0.0011502334142987174,1.8262585398574769e-06,0.98878648267574698,0.86614512691256074,7.0155318164974044e-05,0.0012606073776191724,3.0315699065442381e-05,0.27242678089870687,0.17739435943512827,0.99998355251200965,0.00028440922633490384,0.99303164717284576,5.0924160768164551e-06,0.00019389483785481934,0.02900019442193261,0.99985502079134347,0.00051669519286774376,0.99995516374662963,3.7181286730171841e-05,0.1278667108490763,0.0027001263816906875,0.00061540202446684774,0.10914417668720119,0.0001982545475683514,0.026754892253536024,1.318973140819558e-05,0.99524180883524693,0.8842861661854563,0.99995683477809993,0.29803239897586514,0.026984649886865969,0.001900379281822701,0.0024851474680909215,0.0041385990217831306,6.1328525300673422e-05,0.99994622856928528,2.8922196085978874e-05,0.045077852937050078,0.00012265814096524061,0.99998359201971809,8.0105660752544523e-05,0.011034808635005819,0.99828481113350198,0.99939895815811008,2.9921888791460203e-06,0.42719484557894283,0.0067477527548034039,0.26101869185273024,0.0036028483025855903,0.99999833979515551,0.0005650321730378741,5.8394230014411486e-06,0.013174026557107621,0.19017719453631229,0.00095505938651264466,0.18983358399887817,0.085941470536857412,0.99762640497296862,0.018464395284543848,0.0011869830472102326,0.97502954110509676,0.0079743174081626251,2.0341874859758533e-06,0.0024304808904848331,6.7529079488795276e-05,0.99972156085602804,0.99957164311569224,0.015787484366577485,0.99770963899072662,0.20326334646761521,7.1302115311237519e-06,0.0024201638965908293,0.9998407430005416,1.8389014027178512e-06,0.023407796251841164,0.00020589866357522137,2.4214986433855472e-05,0.0053733720102237077,0.99999734016410868,0.37231494492211686,0.99910812264828941,0.77655885885669473,0.015618494795571179,0.58024336221972106,0.0059572199248791452,0.017943635993395747,0.00067319868260558497],"x":{},"y":{},"type":"mesh3d","name":"Fitted values","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"sales"},"yaxis":{"title":"customer_rate","nticks":5},"camera":{"eye":{"x":-0.5,"y":2,"z":0}},"zaxis":{"title":"promoted"},"aspectmode":"cube"},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.9399999999999999,4.0599999999999996,3.8300000000000001,3.6200000000000001,4.4000000000000004,4.54,3.0899999999999999,4.8899999999999997,3.7400000000000002,3,2.4300000000000002,3.1600000000000001,3.5099999999999998,3.21,3.02,3.8700000000000001,2.4900000000000002,2.6600000000000001,3.1400000000000001,5,3.5299999999999998,4.2400000000000002,4.4699999999999998,3.6000000000000001,4.4500000000000002,3.9399999999999999,2.54,4.0599999999999996,4.4699999999999998,2.98,3.48,3.7400000000000002,2.4700000000000002,3.3199999999999998,3.5299999999999998,2.6600000000000001,4.8899999999999997,3.6200000000000001,4.4000000000000004,2.5600000000000001,3.3399999999999999,2.5600000000000001,4.3099999999999996,3.02,2.8599999999999999,2.98,3.3900000000000001,2.3599999999999999,2.3300000000000001,1.9399999999999999,4.1699999999999999,3.0699999999999998,3,3.6200000000000001,3.9199999999999999,3.8500000000000001,5,4.4900000000000002,3.7400000000000002,4.75,4.8899999999999997,4.1500000000000004,5,4.29,4.29,3.7400000000000002,2.2200000000000002,3.5699999999999998,3.7400000000000002,3.4100000000000001,3.71,2.1499999999999999,3.4100000000000001,2.0099999999999998,4.4000000000000004,4.0300000000000002,4.6600000000000001,3.6200000000000001,3.6899999999999999,4.2000000000000002,4.1500000000000004,5,3.21,3.7999999999999998,4.2000000000000002,3.8700000000000001,2.75,3.5499999999999998,2.52,3.7599999999999998,3.1099999999999999,4.3300000000000001,3.21,2.4700000000000002,1.51,3.5299999999999998,4.6299999999999999,3.3700000000000001,4.0800000000000001,3.1600000000000001,3.7599999999999998,3.0699999999999998,3.8700000000000001,3.6200000000000001,3.46,2.4900000000000002,2.2200000000000002,4.9800000000000004,3.0499999999999998,4.4699999999999998,1.8999999999999999,5,3.46,2.29,4.54,4.0599999999999996,3.3700000000000001,4.7699999999999996,5,4.4299999999999997,4.9299999999999997,4.0300000000000002,3.0499999999999998,4.4900000000000002,3.8700000000000001,4.1299999999999999,3.0499999999999998,5,3.8999999999999999,3.9199999999999999,3.5299999999999998,4.6799999999999997,3.5099999999999998,2.0299999999999998,3.71,5,2.7200000000000002,5,4.2400000000000002,3.5099999999999998,3.23,4.4699999999999998,2.4300000000000002,2.7000000000000002,4.9800000000000004,3,2.8900000000000001,3.4100000000000001,4.3799999999999999,5,5,2.7000000000000002,4.9500000000000002,2.54,2.7000000000000002,3.7799999999999998,4.2400000000000002,3.7799999999999998,4.0099999999999998,4.8200000000000003,4.1699999999999999,1.6699999999999999,3.0499999999999998,2.54,3.6899999999999999,2.9100000000000001,5,2.9300000000000002,2.2599999999999998,4.8600000000000003,4.8399999999999999,3.9399999999999999,2.6600000000000001,4.0599999999999996,1.9399999999999999,4.6299999999999999,3.1400000000000001,4.5599999999999996,4.9800000000000004,4.2400000000000002,2.2000000000000002,4.1699999999999999,2.2000000000000002,4.1500000000000004,4.1500000000000004,4.0099999999999998,4.5599999999999996,4.4900000000000002,3.4399999999999999,3.0499999999999998,3.8300000000000001,2.79,2.75,2.0299999999999998,4.2000000000000002,4.7199999999999998,3.3900000000000001,4.0800000000000001,3.8300000000000001,2.7000000000000002,3.4399999999999999,3.9700000000000002,1.8300000000000001,4.4699999999999998,4.5599999999999996,4.4299999999999997,4.8600000000000003,5,3.8500000000000001,2.77,3.3900000000000001,1.3700000000000001,3.0499999999999998,4.8600000000000003,2.98,3.8500000000000001,3.8300000000000001,4.8899999999999997,1.97,3.1400000000000001,4.3099999999999996,2.52,3.5099999999999998,2.54,2.4700000000000002,2.3599999999999999,3.21,3.0899999999999999,2.0800000000000001,2.8199999999999998,3.5499999999999998,3.8500000000000001,3.5699999999999998,2.8599999999999999,3.4399999999999999,5,3.3399999999999999,3.9900000000000002,4.0599999999999996,3.21,4.1699999999999999,2.7200000000000002,3.7999999999999998,3.7799999999999998,3.7400000000000002,2.8599999999999999,4.4500000000000002,4.8899999999999997,5,2.2599999999999998,2.6600000000000001,4.0300000000000002,2.6299999999999999,3.5099999999999998,4.1500000000000004,4.0800000000000001,2.5600000000000001,3.3399999999999999,5,3.8700000000000001,1,2.3100000000000001,3.3399999999999999,3.25,4.0999999999999996,3.0899999999999999,4.7699999999999996,3.6200000000000001,4.8600000000000003,3,4.79,3.4100000000000001,4.6799999999999997,5,4.0300000000000002,3.6899999999999999,1.8500000000000001,4.2000000000000002,5,2.3799999999999999,3.9900000000000002,3.25,2.8900000000000001,3.2799999999999998,2.98,3.23,3.0899999999999999,3.4100000000000001,1.6899999999999999,3.7599999999999998,2.75,5,4.75,4.5899999999999999,1.8300000000000001,4.29,3.6899999999999999,2.6600000000000001,3.8999999999999999,2.6099999999999999,3.8999999999999999,3.4100000000000001,3.6699999999999999,1.99,1.3700000000000001,2.3799999999999999,4.7199999999999998,3.48,3.6000000000000001,3.1800000000000002,4.7699999999999996,4.0300000000000002,4.2199999999999998,4.0999999999999996,3.6400000000000001,2.29,3.5499999999999998,2.6600000000000001,3.48,2.8900000000000001,3.5699999999999998,4.3600000000000003,2.79,3.6000000000000001,3.3900000000000001,3.3199999999999998,3.4100000000000001,3.6899999999999999,3.71,4.3099999999999996,4.6100000000000003,4.3300000000000001,4.7000000000000002,3.5699999999999998,2.0099999999999998,3.1400000000000001,3.0499999999999998,4.7199999999999998,5,5,4.8600000000000003,5,4.3799999999999999,5,5,2.8199999999999998,3.4100000000000001,1.6000000000000001,4.1699999999999999,2.54],"z":["0","0","1","0","1","1","0","0","0","0","0","1","0","0","0","0","0","0","0","1","1","0","0","0","1","1","0","0","0","1","0","0","0","1","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","1","1","0","0","0","0","0","1","0","1","0","1","0","1","1","0","0","0","0","0","0","0","0","0","0","0","1","1","0","0","0","1","0","0","1","1","0","0","1","0","0","1","1","1","0","0","0","0","0","1","0","0","0","0","0","0","1","0","0","0","0","1","1","0","0","0","0","0","0","1","1","0","1","1","0","0","0","1","1","0","1","0","0","0","0","0","1","0","0","1","0","0","1","0","1","0","0","0","0","0","0","0","0","1","0","1","1","1","1","0","0","1","0","0","0","1","0","0","0","1","1","0","0","1","1","0","1","0","1","1","0","0","1","0","1","0","0","1","1","1","0","0","0","0","0","0","0","1","0","1","1","0","1","1","0","0","0","0","0","1","0","0","1","0","0","0","0","0","1","1","1","0","0","1","0","0","0","0","1","0","1","1","0","0","0","0","0","0","1","0","1","1","0","0","0","1","0","0","1","0","0","0","0","1","0","1","1","1","0","0","0","0","0","0","0","0","1","1","0","0","0","0","0","1","0","1","0","0","0","1","0","1","0","0","0","0","1","0","0","0","1","1","1","0","0","0","0","0","0","1","0","0","0","1","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","0","0","1","0","0","1","0","0","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","0","1","1","0","0","0","0","0"],"mode":"markers","type":"scatter3d","marker":{"color":"blue","size":5,"symbol":104,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observations","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"promoted","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[0.51304378747036605,0.0023290366332891065,0.96791325675030548,0.085055170809734876,0.88899508542332994,0.99999614001934445,3.9415846469361267e-05,3.3526370619405734e-05,5.0106017726651335e-05,0.0007071882478328032,0.27697229377769361,0.9971420892282028,0.27693778229032362,0.0070709812140732431,0.00012184293115255033,0.00014545030098594099,6.8491527318655281e-06,0.00014321157953073557,0.0015289282925672232,0.99999699754650007,0.99241673066241354,0.0048063312419649954,0.050910564453653703,5.8628538706006359e-05,0.99980982241345651,0.99694331729095742,0.00011861546537628204,2.2450005899478837e-06,2.1214280004874962e-05,0.99775580694991239,0.016036248583680499,0.017208780600967997,3.251729612447346e-06,0.99995799273909569,0.00065964499544458085,0.99899372927826513,0.99800632442776238,0.0016350908703035426,0.99999879815048631,0.14282719379847175,0.00052361457648374002,1.0280406943761884e-05,0.9999989978988828,0.055555992634510949,7.955724398476975e-05,0.00078402305155006707,0.015720918156021738,0.0012331624585504774,0.0091551843473662662,0.96463508618113492,0.29899149817867704,0.0017216911187149686,0.007311494155589922,0.0029926706262928251,8.1348627735226153e-05,0.00099196858908720781,0.99984904633485516,0.0001694047115080356,0.99881729861370983,0.00087874561177735556,0.99940564349091898,0.0019427004013640537,0.99495641215681219,0.96940927140424382,0.00098304502670207902,0.044122862433742437,0.036672143743223105,0.011895056870476307,0.09730877461781412,5.9320821088611448e-06,5.1821303396215897e-05,0.0001226828026446934,0.1303804896447664,0.0012194896705953113,0.086098936085861319,0.6542575318601207,0.99981109171099136,0.0020825258785462184,0.00057400940535780245,0.033912541626999776,0.99974082925032526,0.18841996879762404,0.0055577353214504178,0.98533826894573773,0.91573089402718411,0.00085938512230808624,0.0039938342606074045,0.99911826782934277,3.917346746439537e-06,0.0010972604175767157,0.9994829412829439,0.63263064701271443,0.99999875165777696,0.1902037381917544,0.00022284075695328494,0.0018085228949677428,0.032682631919285936,0.0015050822414446679,0.87598158130774106,0.00066712164962269441,0.00038420744661276995,0.00086723691551421366,0.00026654467279782994,0.0044752567006351406,0.023412393006146358,0.900628860957564,0.00082007828253893638,0.0046996361138977889,5.252917843669373e-05,0.00045658288387549756,0.99830783831499526,0.76223049790284414,0.00058311244522132855,0.014266535715782272,0.00051651721979582702,0.00042785910127370784,0.00036655278360342555,0.0096102989524477697,0.99999773695658756,0.99733307281658834,0.23508414554955109,0.99648077593570106,0.22953554089936407,0.0017602922467597644,0.01000385048558659,0.0014389537746719518,0.99895925582698197,0.99998765784876553,0.0057462539301270758,0.99999932639894273,6.3450182926456959e-06,0.0015423128093645928,0.00032619079841053573,0.0062146454450793479,3.9059811993662433e-05,0.99999674495314139,5.7389157505198448e-07,1.5528951992312294e-05,0.99967637866056114,0.0028810388182021127,0.00018369090980264739,0.93634930744356992,0.0061865986718418024,0.99999902695747767,0.0011019758107580072,0.00015877189117223891,0.00029161240487343736,0.023774400373556635,0.0044547698399391676,0.0013726235398632688,0.0090904836788513119,6.3601135172150033e-06,0.99973845989494048,2.455282180494587e-05,0.02173265115569983,0.99982182024230593,0.99984354833540134,0.99744436066869857,8.9989444663976005e-05,3.77781855569173e-06,0.99997553514464321,0.00067783650979910584,0.00041191882277807359,0.00504967855302502,0.99999933692064391,0.01254623269098374,0.014103604670684779,0.00043470779617807903,0.99999932959264359,0.99835930918194693,0.0025582934759559128,0.12709933152061911,0.99676094350262989,0.9833337325224093,6.9235778881309706e-05,0.93278095717935705,0.048899492572319819,0.91572646085629072,0.26231270573387228,0.012079812579912514,0.00033142215555053784,0.40915925473354198,0.87256176682338482,0.99934408759309046,0.0011974120550461587,0.00079629398434484457,0.89118761320262929,0.94415399972261016,0.99609126241652035,6.693257702375403e-05,0.0008291100475419273,0.0028813688978400471,0.0017846030462786576,0.0007650392307984548,2.8734167952632146e-06,3.8965990441351794e-05,0.9582346238623578,0.50406583559299933,0.99973967717072032,0.9999722038690072,0.0010492121203519965,0.97159380299331299,0.9480637808410376,0.0082988346485234257,0.0042790652228158789,0.34092411529973055,0.00012625904134048321,0.00028440922633490384,0.99997909217582026,0.00047980611565371435,0.0018004798476501578,0.99999582677410637,0.74093313103188507,0.030953852350014122,0.014746076810555789,0.0003335720671835084,0.0096545400760163751,0.99535644062420714,0.99251922871018627,0.96126841290298504,0.013111641004797512,9.9223647506070375e-07,0.99999727434573704,0.0012331270819546582,0.0010901312531541433,0.022118752212858823,3.5870396105639534e-05,0.99947358263883412,0.0013248996220309875,0.99999976957577241,0.99915314059995619,0.0074253806275747496,0.0004950469066561465,0.039974192039368167,0.028009469676191438,0.045849207277917504,0.090815582137632245,0.99713530950094942,0.0087740321902467239,0.99999968380429438,0.99689496461488714,0.00028836783398084888,0.0017723552205261295,0.45706390398533314,0.57859012934016429,0.0014554592433055347,0.0026457962736067321,0.99553943054857741,0.0018202893948287753,0.00059112433197931475,0.00036250592960833958,3.0790540845672001e-05,0.99999806228423704,8.9640057315764252e-06,0.99999787981238353,0.9996412130899206,0.99991692048426917,0.00081626354708175936,0.013552896352098231,6.4886446899741766e-06,0.00026369249809228285,1.1130584183612454e-07,0.0042608129416333747,0.0011502334142987174,1.8262585398574769e-06,0.98878648267574698,0.86614512691256074,7.0155318164974044e-05,0.0012606073776191724,3.0315699065442381e-05,0.27242678089870687,0.17739435943512827,0.99998355251200965,0.00028440922633490384,0.99303164717284576,5.0924160768164551e-06,0.00019389483785481934,0.02900019442193261,0.99985502079134347,0.00051669519286774376,0.99995516374662963,3.7181286730171841e-05,0.1278667108490763,0.0027001263816906875,0.00061540202446684774,0.10914417668720119,0.0001982545475683514,0.026754892253536024,1.318973140819558e-05,0.99524180883524693,0.8842861661854563,0.99995683477809993,0.29803239897586514,0.026984649886865969,0.001900379281822701,0.0024851474680909215,0.0041385990217831306,6.1328525300673422e-05,0.99994622856928528,2.8922196085978874e-05,0.045077852937050078,0.00012265814096524061,0.99998359201971809,8.0105660752544523e-05,0.011034808635005819,0.99828481113350198,0.99939895815811008,2.9921888791460203e-06,0.42719484557894283,0.0067477527548034039,0.26101869185273024,0.0036028483025855903,0.99999833979515551,0.0005650321730378741,5.8394230014411486e-06,0.013174026557107621,0.19017719453631229,0.00095505938651264466,0.18983358399887817,0.085941470536857412,0.99762640497296862,0.018464395284543848,0.0011869830472102326,0.97502954110509676,0.0079743174081626251,2.0341874859758533e-06,0.0024304808904848331,6.7529079488795276e-05,0.99972156085602804,0.99957164311569224,0.015787484366577485,0.99770963899072662,0.20326334646761521,7.1302115311237519e-06,0.0024201638965908293,0.9998407430005416,1.8389014027178512e-06,0.023407796251841164,0.00020589866357522137,2.4214986433855472e-05,0.0053733720102237077,0.99999734016410868,0.37231494492211686,0.99910812264828941,0.77655885885669473,0.015618494795571179,0.58024336221972106,0.0059572199248791452,0.017943635993395747,0.00067319868260558497],"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.9399999999999999,4.0599999999999996,3.8300000000000001,3.6200000000000001,4.4000000000000004,4.54,3.0899999999999999,4.8899999999999997,3.7400000000000002,3,2.4300000000000002,3.1600000000000001,3.5099999999999998,3.21,3.02,3.8700000000000001,2.4900000000000002,2.6600000000000001,3.1400000000000001,5,3.5299999999999998,4.2400000000000002,4.4699999999999998,3.6000000000000001,4.4500000000000002,3.9399999999999999,2.54,4.0599999999999996,4.4699999999999998,2.98,3.48,3.7400000000000002,2.4700000000000002,3.3199999999999998,3.5299999999999998,2.6600000000000001,4.8899999999999997,3.6200000000000001,4.4000000000000004,2.5600000000000001,3.3399999999999999,2.5600000000000001,4.3099999999999996,3.02,2.8599999999999999,2.98,3.3900000000000001,2.3599999999999999,2.3300000000000001,1.9399999999999999,4.1699999999999999,3.0699999999999998,3,3.6200000000000001,3.9199999999999999,3.8500000000000001,5,4.4900000000000002,3.7400000000000002,4.75,4.8899999999999997,4.1500000000000004,5,4.29,4.29,3.7400000000000002,2.2200000000000002,3.5699999999999998,3.7400000000000002,3.4100000000000001,3.71,2.1499999999999999,3.4100000000000001,2.0099999999999998,4.4000000000000004,4.0300000000000002,4.6600000000000001,3.6200000000000001,3.6899999999999999,4.2000000000000002,4.1500000000000004,5,3.21,3.7999999999999998,4.2000000000000002,3.8700000000000001,2.75,3.5499999999999998,2.52,3.7599999999999998,3.1099999999999999,4.3300000000000001,3.21,2.4700000000000002,1.51,3.5299999999999998,4.6299999999999999,3.3700000000000001,4.0800000000000001,3.1600000000000001,3.7599999999999998,3.0699999999999998,3.8700000000000001,3.6200000000000001,3.46,2.4900000000000002,2.2200000000000002,4.9800000000000004,3.0499999999999998,4.4699999999999998,1.8999999999999999,5,3.46,2.29,4.54,4.0599999999999996,3.3700000000000001,4.7699999999999996,5,4.4299999999999997,4.9299999999999997,4.0300000000000002,3.0499999999999998,4.4900000000000002,3.8700000000000001,4.1299999999999999,3.0499999999999998,5,3.8999999999999999,3.9199999999999999,3.5299999999999998,4.6799999999999997,3.5099999999999998,2.0299999999999998,3.71,5,2.7200000000000002,5,4.2400000000000002,3.5099999999999998,3.23,4.4699999999999998,2.4300000000000002,2.7000000000000002,4.9800000000000004,3,2.8900000000000001,3.4100000000000001,4.3799999999999999,5,5,2.7000000000000002,4.9500000000000002,2.54,2.7000000000000002,3.7799999999999998,4.2400000000000002,3.7799999999999998,4.0099999999999998,4.8200000000000003,4.1699999999999999,1.6699999999999999,3.0499999999999998,2.54,3.6899999999999999,2.9100000000000001,5,2.9300000000000002,2.2599999999999998,4.8600000000000003,4.8399999999999999,3.9399999999999999,2.6600000000000001,4.0599999999999996,1.9399999999999999,4.6299999999999999,3.1400000000000001,4.5599999999999996,4.9800000000000004,4.2400000000000002,2.2000000000000002,4.1699999999999999,2.2000000000000002,4.1500000000000004,4.1500000000000004,4.0099999999999998,4.5599999999999996,4.4900000000000002,3.4399999999999999,3.0499999999999998,3.8300000000000001,2.79,2.75,2.0299999999999998,4.2000000000000002,4.7199999999999998,3.3900000000000001,4.0800000000000001,3.8300000000000001,2.7000000000000002,3.4399999999999999,3.9700000000000002,1.8300000000000001,4.4699999999999998,4.5599999999999996,4.4299999999999997,4.8600000000000003,5,3.8500000000000001,2.77,3.3900000000000001,1.3700000000000001,3.0499999999999998,4.8600000000000003,2.98,3.8500000000000001,3.8300000000000001,4.8899999999999997,1.97,3.1400000000000001,4.3099999999999996,2.52,3.5099999999999998,2.54,2.4700000000000002,2.3599999999999999,3.21,3.0899999999999999,2.0800000000000001,2.8199999999999998,3.5499999999999998,3.8500000000000001,3.5699999999999998,2.8599999999999999,3.4399999999999999,5,3.3399999999999999,3.9900000000000002,4.0599999999999996,3.21,4.1699999999999999,2.7200000000000002,3.7999999999999998,3.7799999999999998,3.7400000000000002,2.8599999999999999,4.4500000000000002,4.8899999999999997,5,2.2599999999999998,2.6600000000000001,4.0300000000000002,2.6299999999999999,3.5099999999999998,4.1500000000000004,4.0800000000000001,2.5600000000000001,3.3399999999999999,5,3.8700000000000001,1,2.3100000000000001,3.3399999999999999,3.25,4.0999999999999996,3.0899999999999999,4.7699999999999996,3.6200000000000001,4.8600000000000003,3,4.79,3.4100000000000001,4.6799999999999997,5,4.0300000000000002,3.6899999999999999,1.8500000000000001,4.2000000000000002,5,2.3799999999999999,3.9900000000000002,3.25,2.8900000000000001,3.2799999999999998,2.98,3.23,3.0899999999999999,3.4100000000000001,1.6899999999999999,3.7599999999999998,2.75,5,4.75,4.5899999999999999,1.8300000000000001,4.29,3.6899999999999999,2.6600000000000001,3.8999999999999999,2.6099999999999999,3.8999999999999999,3.4100000000000001,3.6699999999999999,1.99,1.3700000000000001,2.3799999999999999,4.7199999999999998,3.48,3.6000000000000001,3.1800000000000002,4.7699999999999996,4.0300000000000002,4.2199999999999998,4.0999999999999996,3.6400000000000001,2.29,3.5499999999999998,2.6600000000000001,3.48,2.8900000000000001,3.5699999999999998,4.3600000000000003,2.79,3.6000000000000001,3.3900000000000001,3.3199999999999998,3.4100000000000001,3.6899999999999999,3.71,4.3099999999999996,4.6100000000000003,4.3300000000000001,4.7000000000000002,3.5699999999999998,2.0099999999999998,3.1400000000000001,3.0499999999999998,4.7199999999999998,5,5,4.8600000000000003,5,4.3799999999999999,5,5,2.8199999999999998,3.4100000000000001,1.6000000000000001,4.1699999999999999,2.54],"type":"mesh3d","name":"Fitted values","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-log-reg-3d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.11: 3D visualization of the fitted <code>simpler_model</code> against the <code>salespeople</code> data
</figcaption>
</figure>
<p>Now let’s look at the summary of our <code>simpler_model</code>.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simpler_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = promoted ~ sales + customer_rate, family = "binomial", 
    data = salespeople)

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.517689   3.346762  -5.832 5.48e-09 ***
sales           0.040389   0.006525   6.190 6.03e-10 ***
customer_rate  -1.122064   0.466958  -2.403   0.0163 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 440.303  on 349  degrees of freedom
Residual deviance:  65.131  on 347  degrees of freedom
AIC: 71.131

Number of Fisher Scoring iterations: 8</code></pre>
</div>
</div>
<p>Note that, unlike what we saw for linear regression in <a href="linear_regression.html#sec-lin-good-fit" class="quarto-xref"><span>Section 4.3.3</span></a>, our summary does not provide a statistic on overall model fit or goodness-of-fit. The main reason for this is that there is no clear unified point of view in the statistics community on a single appropriate measure for model fit in the case of logistic regression. Nevertheless, a number of options are available to analysts for estimating fit and goodness-of-fit for these models.</p>
<p><em>Pseudo-<span class="math inline">\(R^2\)</span></em> measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the <span class="math inline">\(R^2\)</span> in linear regression. There are numerous variants of pseudo-<span class="math inline">\(R^2\)</span> with some of the most common listed here:</p>
<ul>
<li>McFadden’s <span class="math inline">\(R^2\)</span> works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome.</li>
<li>Cox and Snell’s <span class="math inline">\(R^2\)</span> works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating <span class="math inline">\(R^2\)</span> in linear regression. However, this usually means that the maximum value is less than 1 and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an <span class="math inline">\(R^2\)</span>.</li>
<li>Nagelkerke’s <span class="math inline">\(R^2\)</span> resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s <span class="math inline">\(R^2\)</span> by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation.</li>
<li>Tjur’s <span class="math inline">\(R^2\)</span> is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations.</li>
</ul>
<p>Standard modeling functions generally do not offer the calculation of pseudo-<span class="math inline">\(R^2\)</span> as standard, but numerous methods are available for their calculation. For example:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>DescTools<span class="sc">::</span><span class="fu">PseudoR2</span>(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  simpler_model, </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">which =</span> <span class="fu">c</span>(<span class="st">"McFadden"</span>, <span class="st">"CoxSnell"</span>, <span class="st">"Nagelkerke"</span>, <span class="st">"Tjur"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  McFadden   CoxSnell Nagelkerke       Tjur 
 0.8520759  0.6576490  0.9187858  0.8784834 </code></pre>
</div>
</div>
<p>We see that the Cox and Snell variant is notably lower than the other estimates, which is consistent with the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit.</p>
<p>Goodness-of-fit tests for logistic regression models compare the predictions to the observed outcome and test the null hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually 10) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there is a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness-of-fit tests, and not rely entirely on any one specific approach.</p>
<p>In R, the <code>generalhoslem</code> package can perform the popular Hosmer-Lemeshow test of goodness of fit for logistic regression models, and is recommended for exploration. Here is an example using the <code>logitgof()</code> function for assessing goodness-of-fit, which uses 10 groups as default.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(generalhoslem)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># run Hosmer-Lemeshow GOF test on observed versus fitted values</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>simpler_model_diagnostics <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  salespeople<span class="sc">$</span>promoted, </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fitted</span>(simpler_model) </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># view results</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>simpler_model_diagnostics</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Hosmer and Lemeshow test (binary model)

data:  salespeople$promoted, fitted(simpler_model)
X-squared = 3.4458, df = 8, p-value = 0.9034</code></pre>
</div>
</div>
<p>The non-significant result of the Hosmer-Lemeshow test suggests a good fit for our model.</p>
<p>Various measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness-of-fit tests for logistic regression models is <span class="citation" data-cites="hosmer-logistic">Hosmer, Lemeshow, and Sturdivant (<a href="bibliography.html#ref-hosmer-logistic" role="doc-biblioref">2013</a>)</span>.</p>
</section>
<section id="model-parsimony" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="model-parsimony"><span class="header-section-number">5.3.3</span> Model parsimony</h3>
<p>We saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of <em>Occam’s Razor</em> states that—all else being equal—the simplest explanation is the best. In this sense, a model that contains information that does not contribute to its primary inference objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return.</p>
<p><em>Parsimony</em> describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or very close to the same) fit with a smaller number of inputs. The <em>Akaike Information Criterion</em> or <em>AIC</em> is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sales only model</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(sales_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 76.49508</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sales and customer rating model</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(simpler_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 71.13145</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model with all inputs</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(full_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 76.37433</code></pre>
</div>
</div>
<p>We can see that the model which is limited to our two significant inputs—sales and customer rating—is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence—it is possible that the lowest AIC might still be a very poor fit.</p>
<p>Model parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpret clearly, and the greater the risk of measurement problems, such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex, and in extreme cases there are more inputs than there are observations. The primary way to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony.</p>
</section>
</section>
<section id="other-considerations-in-binomial-logistic-regression" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="other-considerations-in-binomial-logistic-regression"><span class="header-section-number">5.4</span> Other considerations in binomial logistic regression</h2>
<p>To predict from new data, just use the <code>predict()</code> function as in the previous chapter. This function recognizes the type of model being used—in this case a generalized linear model—and adjusts its prediction approach accordingly. In particular, if you want to return the probability of the new observations being promoted, you need to use <code>type = "response"</code> as an argument.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define new observations</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>(new_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">sales =</span> <span class="fu">c</span>(<span class="dv">420</span>, <span class="dv">510</span>, <span class="dv">710</span>), </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">customer_rate =</span> <span class="fu">c</span>(<span class="fl">3.4</span>, <span class="fl">2.3</span>, <span class="fl">4.2</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  sales customer_rate
1   420           3.4
2   510           2.3
3   710           4.2</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict probability of promotion</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(simpler_model, new_data, <span class="at">type =</span> <span class="st">"response"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>         1          2          3 
0.00171007 0.18238565 0.98840506 </code></pre>
</div>
</div>
<p>Many of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental mathematical transformations which take place in a logistic regression model when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on <span class="math inline">\(y\)</span>, in logistic regression they have a direct additive impact on the log odds of <span class="math inline">\(y\)</span>, or alternatively their exponents have a direct multiplicative impact on the odds of <span class="math inline">\(y\)</span>. Therefore coefficient overestimation such as that which can occur when collinearity is not managed can result in inferences that could substantially overstate the importance or effect of input variables.</p>
<p>Because of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined, but they can be useful in identifying outliers or particularly influential observations and in assessing goodness-of-fit. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the <em>Pearson residual</em> is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large-enough samples. We can see in <a href="#fig-pearson-resids" class="quarto-xref">Figure&nbsp;<span>5.13</span></a> that this is the case for our <code>simpler_model</code>, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is <span class="citation" data-cites="menard">Menard (<a href="bibliography.html#ref-menard" role="doc-biblioref">2010</a>)</span>.</p>
<div id="fig-pearson-resids" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pearson-resids-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="fu">residuals</span>(simpler_model, <span class="st">"pearson"</span>))</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d, <span class="at">main=</span> <span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="binomial_logistic_regression_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pearson-resids-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.13: Distribution of Pearson residuals in <code>simpler_model</code>
</figcaption>
</figure>
</div>
</section>
<section id="binomial-logistic-regression-using-python" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="binomial-logistic-regression-using-python"><span class="header-section-number">5.5</span> Binomial logistic regression using Python</h2>
<p>In Python, binomial logistic regression models can be generated in a similar way to OLS linear regression models using the <code>statsmodels</code> formula API, calling the binomial family from the general <code>statsmodels</code> API.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain salespeople data</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://peopleanalytics-regression-book.org/data/salespeople.csv"</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>salespeople <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define model</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.glm(formula <span class="op">=</span> <span class="st">"promoted ~ sales + customer_rate"</span>, </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>                data <span class="op">=</span> salespeople, </span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>                family <span class="op">=</span> sm.families.Binomial())</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>promotion_model <span class="op">=</span> model.fit()</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="co"># see results summary</span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(promotion_model.summary())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:               promoted   No. Observations:                  350
Model:                            GLM   Df Residuals:                      347
Model Family:                Binomial   Df Model:                            2
Link Function:                  Logit   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -32.566
Date:                Wed, 17 Dec 2025   Deviance:                       65.131
Time:                        23:39:03   Pearson chi2:                     198.
No. Iterations:                     9   Pseudo R-squ. (CS):             0.6576
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          z      P&gt;|z|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept       -19.5177      3.347     -5.831      0.000     -26.078     -12.958
sales             0.0404      0.007      6.189      0.000       0.028       0.053
customer_rate    -1.1221      0.467     -2.403      0.016      -2.037      -0.207
=================================================================================</code></pre>
</div>
</div>
</section>
<section id="learning-exercises" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="learning-exercises"><span class="header-section-number">5.6</span> Learning exercises</h2>
<section id="discussion-questions" class="level3" data-number="5.6.1">
<h3 data-number="5.6.1" class="anchored" data-anchor-id="discussion-questions"><span class="header-section-number">5.6.1</span> Discussion questions</h3>
<ol type="1">
<li><p>Draw the shape of a logistic function. Describe the three population growth phases it was originally intended to model.</p></li>
<li><p>Explain why the logistic function is useful to statisticians in modeling.</p></li>
<li><p>In the formula for the logistic function in <a href="#sec-logistic-origins" class="quarto-xref"><span>Section 5.1.1</span></a>, what might be a common value for <span class="math inline">\(L\)</span> in probabilistic applications? Why?</p></li>
<li><p>What types of problems are suitable for logistic regression modeling?</p></li>
<li><p>Can you think of some modeling scenarios in your work or studies that could use a logistic regression approach?</p></li>
<li><p>Explain the concept of odds. How do odds differ from probability? How do odds change as probability increases?</p></li>
<li><p>Complete the following:</p></li>
</ol>
<ol type="a">
<li>If an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
<li>If an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
</ol>
<ol start="8" type="1">
<li><p>Describe how the coefficients of a logistic regression model affect the fitted outcome. If <span class="math inline">\(\beta\)</span> is a coefficient estimate, how is the odds ratio associated with <span class="math inline">\(\beta\)</span> calculated and what does it mean?</p></li>
<li><p>What are some of the options for determining the fit of a binomial logistic regression model?</p></li>
<li><p>Describe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model?</p></li>
</ol>
</section>
<section id="data-exercises" class="level3" data-number="5.6.2">
<h3 data-number="5.6.2" class="anchored" data-anchor-id="data-exercises"><span class="header-section-number">5.6.2</span> Data exercises</h3>
<p>A nature preservation charity has asked you to analyze some data to help them understand the features of those members of the public who donated in a given month. Load the <code>charity_donation</code> data set via the <code>peopleanalyticsdata</code> package or download it from the internet<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. It contains the following data:</p>
<ul>
<li><code>n_donations</code>: The total number of times the individual donated previous to the month being studied.</li>
<li><code>total_donations</code>: The total amount of money donated by the individual previous to the month being studied</li>
<li><code>time_donating</code>: The number of months between the first donation and the month being studied</li>
<li><code>recent_donation</code>: Whether or not the individual donated in the month being studied</li>
<li><code>last_donation</code>: The number of months between the most recent previous donation and the month being studied</li>
<li><code>gender</code>: The gender of the individual</li>
<li><code>reside</code>: Whether the person resides in an Urban or Rural Domestic location or Overseas</li>
<li><code>age</code>: The age of the individual</li>
</ul>
<ol type="1">
<li>View the data and obtain statistical summaries. Ensure data types are appropriate and there is no missing data. Determine the outcome and input variables.</li>
<li>Using a pairplot or by plotting or correlating selected fields, try to hypothesize which variables may be significant in explaining who recently donated.</li>
<li>Run a binomial logistic regression model using all input fields. Determine which input variables have a significant effect on the outcome and the direction of that effect.</li>
<li>Calculate the odds ratios for the significant variables and explain their impact on the outcome.</li>
<li>Check for collinearity or multicollinearity in your model using methods from previous chapters.</li>
<li>Experiment with model parsimony by reducing input variables that do not have a significant impact on the outcome. Decide on the most parsimonious model.</li>
<li>Calculate a variety of Pseudo-<span class="math inline">\(R^2\)</span> variants for your model. How would you explain these to someone with no statistics expertise?</li>
<li>Report the conclusions of your modeling exercise to the charity by writing a simple explanation that assumes no knowledge of statistics.</li>
<li><strong>Extension:</strong> Using a variety of methods of your choice, test the hypothesis that your model fits the data. How conclusive are your tests?</li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hosmer-logistic" class="csl-entry" role="listitem">
Hosmer, David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. <em>Applied Logistic Regression</em>.
</div>
<div id="ref-menard" class="csl-entry" role="listitem">
Menard, Scott. 2010. <em>Logistic Regression: From Introductory to Advanced Concepts and Applications</em>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>I usually find that <span class="math inline">\(n &gt; 50\)</span> and <span class="math inline">\(0.2 &lt; p &lt; 0.8\)</span> are decent rules of thumb for my needs.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The logistic function plotted in <a href="#fig-norm-log-curves" class="quarto-xref">Figure&nbsp;<span>5.5</span></a> takes the simple form <span class="math inline">\(y = \frac{1}{1 + e^{-\frac{x}{0.61}}}\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Often in sports the odds are expressed in the reverse order, but the concept is the same.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In this case, a more general form of the Ordinary Least Squares procedure is used to fit the model, known as <em>maximum likelihood estimation</em>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Note that most standard modeling functions have a built-in capability to deal with categorical variables, meaning that it’s often not necessary to explicitly construct dummies. However, it is shown here for completion sake. You may wish to try running the subsequent code without explicitly constructing dummies, but note that constructing your own dummies gives you greater control over how they are labeled in any modeling output.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>https://peopleanalytics-regression-book.org/data/charity_donation.csv<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/peopleanalytics-regression-book-2nd-edition\.org\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./linear_regression.html" class="pagination-link" aria-label="Linear Regression for Continuous Outcomes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression for Continuous Outcomes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./multinomial_regression.html" class="pagination-link" aria-label="Multinomial Logistic Regression for Nominal Category Outcomes">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Multinomial Logistic Regression for Nominal Category Outcomes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/keithmcnulty/regression-handbook-2nd-edition/edit/main/binomial_logistic_regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/keithmcnulty/regression-handbook-2nd-edition/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>