[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handbook of Regression Modeling in People Analytics (2nd edition)",
    "section": "",
    "text": "Welcome\nWelcome to the website for the book Handbook of Regression Modeling in People Analytics (2nd Edition) by Keith McNulty.\nThis new edition is to be published by Chapman & Hall/CRC Press in 2026. The online version of this book is free to read here (thanks to Chapman & Hall/CRC Press), and licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. If you have any feedback, please feel free to file an issue on GitHub. Thank you!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#notes-on-data-used-in-this-book",
    "href": "index.html#notes-on-data-used-in-this-book",
    "title": "Handbook of Regression Modeling in People Analytics (2nd edition)",
    "section": "Notes on data used in this book",
    "text": "Notes on data used in this book\nEach of the data sets used in this book can be downloaded individually by following the code in each chapter. Alternatively, packages containing all the data sets used in this book are now available in R and Python. For R users, install and load the peopleanalyticsdata R package.\n\n# install peopleanalyticsdata package\ninstall.packages(\"peopleanalyticsdata\")\nlibrary(peopleanalyticsdata)\n\n# see a list of data sets\ndata(package = \"peopleanalyticsdata\")\n\n# find out more about a specific data set ('managers' example)\nhelp(managers)\n\nFor Python users , use pip install peopleanalyticsdata to install the package into your environment. Then, to use the package:\n\n# import peopleanalyticsdata package\nimport peopleanalyticsdata as pad\nimport pandas as pd\n\n# see a list of data sets\npad.list_sets()\n\n# load data into a dataframe\ndf = pad.managers()\n\n# find out more about a specific data set ('managers' example)\npad.managers().info()\n\nHappy modeling!\nLast update: 10 November 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "quarto/foreword.html",
    "href": "quarto/foreword.html",
    "title": "Foreword by Alexis Fink",
    "section": "",
    "text": "Over the past decade or so, increases in compute power, emergence of friendly analytic tools and an explosion of data have created a wonderful opportunity to bring more analytical rigor to nearly every imaginable question. Not coincidentally, organizations are increasingly looking to apply all that data and capability to what is typically their greatest area of expense and their greatest strategic differentiator—their people. For too long, many of the most critical decisions in an organization—people decisions—had been guided by gut instinct or borrowed ‘best practices’ and the democratization of people analytics opened up enticing pathways to fix that. Suddenly, analysts who were originally interested in data problems began to be interested in people problems, and HR professionals who had dedicated their careers to solving people problems needed more sophisticated analysis and data storytelling to make their cases and to refine their approaches for greater efficiency, effectiveness and impact.\nDoing data work with people in organizations has complexities that some other types of data work doesn’t. Often, the employee populations are relatively smaller than data sets used in other areas, sometimes limiting the methods that can be used. Various regulatory requirements may dictate what data can be gathered and used, and what types of evidence might be required for various programs or people strategies. Human behavior and organizations are sufficiently complex that typically, multiple factors work together in influencing an outcome. Effects can be subtle or meaningful only in combination, or difficult to tease apart. While in many disciplines, prediction is the most important aim, for most people analytics projects and practitioners, understanding why something is happening is critical.\nWhile the universe of analytical approaches is wonderful and vast, the best ‘Swiss army knife’ we have in people analytics is regression. This volume is an accessible, targeted work aimed directly at supporting professionals doing people analytics work. I’ve had the privilege of knowing and respecting Keith McNulty for many years – he is the rare and marvelous individual who is deeply expert in the mechanics of data and analytics, curious about and steeped in the opportunities to improve the effectiveness and well-being of people at work, and a gifted teacher and storyteller. He is among the most prolific standard-bearers for people analytics. This new open-source volume is in keeping with many years of contributions to the practice of understanding people at work.\nAfter nearly 30 years of doing people analytics work and the privilege of leading people analytics teams at several leading global organizations, I am still excited by the problems we get to solve, the insights we get to spawn, and the tremendous impact we can have on organizations and the people that comprise them. This work is human and technical and important and exciting and deeply gratifying. I hope that you will find this Handbook of Regression Modeling in People Analytics helps you uncover new truths and create positive impacts in your own work.\nAlexis A. Fink\nDecember 2020\nAlexis A. Fink, PhD is a leading figure in people analytics and has led major people analytics teams at Microsoft and Intel before her current role as Vice President of People Analytics and Workforce Strategy at Facebook. She is a Fellow of the Society for Industrial and Organizational Psychology and is a frequent author, journal editor and research leader in her field.",
    "crumbs": [
      "Foreword by Alexis Fink"
    ]
  },
  {
    "objectID": "quarto/introduction.html",
    "href": "quarto/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "As a fresh-faced undergraduate in mathematics in the 1990s, I took an introductory course in statistics in my first term. I would never take another. I struggled with the subject, scored my lowest grade in it and swore I would never go anywhere near it again.\nHow wrong I was. Today I live and breathe statistics. How did that happen?\nFirstly, statistics is about solving real-world problems, and amazingly there was not a single mention of a relatable problem from real life in that course I took all those years ago, just abstract mathematics. Nowadays, I know from my work and my personal learning activities that the mathematics has no meaning without a motivating problem to apply it to, and you’ll see example problems all through this book.\nSecondly, statistics is all about data, and working with real data has encouraged me to reengage with statistics and come at it from a different angle—bottom-up you could say. Suddenly all those concepts that were put up on whiteboards using abstract formulas now had real meaning and consequence to the data I was working with. For me, real data helps statistical theory come to life, and this book is supported by numerous data sets designed for the reader to engage with.\nBut one more step solidified my newfound love of statistics, and that was when I put regression modeling into practice. Faced with data sets that I initially believed were just far too messy and random to be able to produce genuine insights, I progressively became more and more fascinated by how regression can cut through the messiness, compartmentalize the randomness and lead you straight to inferences that are often surprising both in their clarity and in their conclusions.\nHence my motivation for writing this book, which is to give others—whether working in people analytics or otherwise—a starting point for a practical learning of regression methods, with the hope that they will see immediate applications to their work and take advantage of a much-underused toolkit that provides strong support for evidence-based practice.\nI am a mathematician who is now a practitioner of analytics. For this reason you should see that this book is neither afraid of nor obsessed with the mathematics of the methodologies covered. It is my general observation that many students and practitioners make the mistake of trying to run multivariate models without even a basic understanding of the underlying mathematics of those models, and I find it very difficult to see how they can be credible in responding to a wide range of questions or critique about their work without such an understanding. That said, it is also not necessary for students and practitioners to understand the deepest levels of theory in order to be fluent in running and interpreting multivariate models. In this book I have tried to limit the mathematical exposition to a level that allows confident and fluent execution and interpretation.\nI subscribe strongly to the principles of open source sharing of knowledge. If you want to reference the material in this book or use the exercises or data sets in trainings or classes, you are free to do so and you do not need to request my permission. I only ask that you make reference to this book as the source.\nI expect this book to improve over time. If you found this book or any part of it helpful to solving a problem, I’d love to hear about it. If you have comments to improve or question any aspect of the contents of this book I encourage you to leave an issue on its Github repository. This is the most reliable way for me to see your comment. I promise to consider all comments and input, but I do have to make a personal judgment about whether they are helpful to the aims and purpose of this book. If I do make changes or additions based on your input I will make a point to acknowledge your contribution.\nI would like to thank the following individuals who have reviewed or contributed to this book at some point during its development: Liz Romero, Alex LoPilato, Kevin Jaggs, Seth Saavedra, Akshay Kotha. My sincere thanks to Alexis Fink for drawing on her years of people analytics experience to set the context for this book in her foreword. My thanks to the people analytics community for their constant encouragement and support in sharing theory, content and method, and to the R community for all the work they do in giving us amazing and constantly improving statistical tools to work with. Finally, I would like to thank my family for their patience and understanding on the evenings and weekends I dedicated to the writing of this book, and for tolerating far too much dinner conversation on the topic of statistics.\nKeith McNulty\nDecember 2020",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "quarto/01-intro.html",
    "href": "quarto/01-intro.html",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "",
    "text": "1.1 Why is regression modeling so important in people analytics?\nIn the 19th century, when Francis Galton first used the term ‘regression’ to describe a statistical phenomenon (see Chapter @ref(linear-reg-ols)), little did he know how important that term would be today. Many of the most powerful tools of statistical inference that we now have at our disposal can be traced back to the types of early analysis that Galton and his contemporaries were engaged in. The sheer number of different regression-related methodologies and variants that are available to researchers and practitioners today is mind-boggling, and there are still rich veins of ongoing research that are focused on defining and refining new forms of regression to tackle new problems.\nNeither could Galton have imagined the advent of the age of data we now live in. Those of us (like me) who entered the world of work even as recently as 20 years ago remember a time when most problems could not be expected to be solved using a data-driven approach, because there simply was no data. Things are very different now, with data being collected and processed all around us and available to use as direct or indirect measures of the phenomena we are interested in.\nAlong with the growth in data that we have seen in recent years, we have also seen a rapid growth in the availability of statistical tools—open source and free to use—that fundamentally change how we go about analytics. Gone are the clunky, complex, repeated steps on calculators or spreadsheets. In their place are lean statistical programming languages that can implement a regression analysis in milliseconds with a single line of code, allowing us to easily run and reproduce multivariate analysis at scale.\nSo given that we have access to well-developed methodology, rich sources of data and readily accessible tools, it is somewhat surprising that many analytics practitioners have a limited knowledge and understanding of regression and its applications. The aim of this book is to encourage inexperienced analytics practitioners to ‘dip their toes’ further into the wide and varied world of regression in order to deliver more targeted and precise insights to their organizations and stakeholders on the problems they are most interested in. While the primary subject matter focus of this book is the analysis of people-related phenomena, the material is easily and naturally transferable to other disciplines. Therefore this book can be regarded as a practical introduction to a wide range of regression methods for any analytics student or practitioner.\nIt is my firm belief that all people analytics professionals should have a strong understanding of regression models and how to implement and interpret them in practice, and my aim with this book is to provide those who need it with help in getting there. In this chapter we will set the scene for the technical learning in the remainder of the book by outlining the relevance of regression models in people analytics practice. We also touch on some general inferential modeling theory to set a context for later chapters, and we provide a preview of the contents, structure and learning objectives of this book.\nPeople analytics involves the study of the behaviors and characteristics of people or groups in relation to important business, organizational or institutional outcomes. This can involve both qualitative methods and quantitative methods, but if data is available related to a particular topic of interest, then quantitative methods are almost always considered important. With such a specific focus on outcomes, any analyst working in people analytics will frequently need to model these outcomes both to understand what influences them and to potentially predict them in the future.\nModeling an outcome with the primary goal of understanding what influences it can be quite a different matter to modeling an outcome with the primary goal of predicting if it will happen in the future. If we need to understand what influences an outcome, we need to get inside a model and construct a formula or structure to infer how each variable acts on that outcome, we need to get a sense of which variables are meaningful or not, and we need to quantify the ‘explainability’ of the outcome based on our variables. If our primary aim is to predict the outcome, getting inside the model is less important because we don’t have to explain the outcome, we just need to be confident that it predicts accurately.\nA model constructed to understand an outcome is often called an inferential model. Regression models are the most well-known and well-used inferential models available, providing a wide range of measures and insights that help us explain the relationship between our input variables and our outcome of interest, as we shall see in later chapters of this book.\nThe current reality in the field of people analytics is that inferential models are more required than predictive models. There are two reasons for this. First, data sets in people analytics are rarely large enough to facilitate satisfactory prediction accuracy, and so attention is usually shifted to inference for this reason alone. Second, in the field of people analytics, decisions often have a real impact on individuals. Therefore, even in the rare situations where accurate predictive modeling is attainable, stakeholders are unlikely to trust the output and bear the consequences of predictive models without some sort of elementary understanding of how the predictions are generated. This requires the analyst to consider inference power as well as predictive accuracy in selecting their modeling approach. Again, many regression models come to the fore because they are commonly able to provide both inferential and predictive value.\nFinally, the growing importance of evidence-based practice in many clinical and professional fields has generated a need for more advanced modeling skills to satisfy rising demand for quantitative evidence from decision makers. In people-related fields such as human resources, many varieties of specialized regression-based models such as survival models or latent variable models have crossed from academic and clinical settings into business settings in recent years, and there is an increasing need for qualified individuals who understand and can implement and interpret these models in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "quarto/01-intro.html#theory-modeling",
    "href": "quarto/01-intro.html#theory-modeling",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "1.2 What do we mean by ‘modeling’‍?",
    "text": "1.2 What do we mean by ‘modeling’‍?\nThe term ‘modeling’ has a very wide range of meaning in everyday life and work. In this book we are focused on inferential modeling, and we define that as a specific form of statistical learning, which tries to discover and understand a mathematical relationship between a set of measurements of certain constructs and a measurement of an outcome of interest, based on a sample of data on each. Modeling is both a concept and a process.\n\n1.2.1 The theory of inferential modeling\nWe will start with a theoretical description and then provide a real example from a later chapter to illustrate.\nImagine we have a population \\(\\mathscr{P}\\) for which we believe there may be a non-random relationship between a certain construct or set of constructs \\(\\mathscr{C}\\) and a certain measurable outcome \\(\\mathscr{O}\\). Imagine that for a certain sample \\(S\\) of observations from \\(\\mathscr{P}\\), we have a collection of data which we believe measure \\(\\mathscr{C}\\) to some acceptable level of accuracy, and for which we also have a measure of the outcome \\(\\mathscr{O}\\).\nBy convention, we denote the set of data that measure \\(\\mathscr{C}\\) on our sample \\(S\\) as \\(X = x_1, x_2, \\dots, x_p\\), where each \\(x_i\\) is a vector (or column) of data measuring at least one of the constructs in \\(\\mathscr{C}\\). We denote the set of data that measure \\(\\mathscr{O}\\) on our sample set \\(S\\) as \\(y\\). An upper-case \\(X\\) is used because the expectation is that there will be several columns of data measuring our constructs, and a lower-case \\(y\\) is used because the expectation is that the outcome is a single column.\nInferential modeling is the process of learning about a relationship (or lack of relationship) between the data in \\(X\\) and \\(y\\) and using that to describe a relationship (or lack of relationship) between our constructs \\(\\mathscr{C}\\) and our outcome \\(\\mathscr{O}\\) that is valid to a high degree of statistical certainty on the population \\(\\mathscr{P}\\). This process may include:\n\nTesting a proposed mathematical relationship in the form of a function, structure or iterative method\nComparing that relationship against other proposed relationships\nDescribing the relationship statistically\nDetermining whether the relationship (or certain elements of it) can be generalized from the sample set \\(S\\) to the population \\(\\mathscr{P}\\)\n\nWhen we test a relationship between \\(X\\) and \\(y\\), we acknowledge that data and measurements are imperfect and so each observation in our sample \\(S\\) may contain random error that we cannot control. Therefore we define our relationship as:\n\\[\ny = f(X) + \\epsilon\n\\] where \\(f\\) is some transformation or function of the data in \\(X\\) and \\(\\epsilon\\) is a random, uncontrollable error.\n\\(f\\) can take the form of a predetermined function with a formula defined on \\(X\\), like a linear function for example. In this case we can call our model a parametric model. In a parametric model, the modeled value of \\(y\\) is known as soon as we know the values of \\(X\\) by simply applying the formula. In a non-parametric model, there is no predetermined formula that defines the modeled value of \\(y\\) purely in terms of \\(X\\). Non-parametric models need further information in addition to \\(X\\) in order to determine the modeled value of \\(y\\)—for example the value of \\(y\\) in other observations with similar \\(X\\) values.\nRegression models are designed to derive \\(f\\) using estimation based on statistical likelihood and expectation, founded on the theory of the distribution of random variables. Regression models can be both parametric and non-parametric, but by far the most commonly used methods (and the majority of those featured in this book) are parametric. Because of their foundation in statistical likelihood and expectation, they are particularly suited to helping answer questions of generalizability—that is, to what extent can the relationship being observed in the sample \\(S\\) be inferred for the population \\(\\mathscr{P}\\), which is usually the driving force in any form of inferential modeling.\nNote that there is a difference between establishing a statistical relationship between \\(\\mathscr{C}\\) and \\(\\mathscr{O}\\) and establishing a causal relationship between the two. This can be a common trap that inexperienced statistical analysts fall into when communicating the conclusions of their modeling. Establishing that a relationship exists between a construct and an outcome is a far cry from being able to say that one causes the other. This is the common truism that ‘correlation does not equal causation’‍.\nTo bring our theory to life, consider the walkthrough example in Chapter @ref(linear-reg-ols) of this book. In this example, we discuss how to establish a relationship between the academic results of students in the first three years of their education program and their results in the fourth year. In this case, our population \\(\\mathscr{P}\\) is all past, present and future students who take similar examinations, and our sample \\(S\\) is the students who completed their studies in the past three years. \\(X = x_1, x_2, x_3\\) are each of the three scores from the first three years, and \\(y\\) is the score in the fourth year. We test \\(f\\) to be a linear relationship, and we establish that such a relationship can be generalized to the entire population \\(\\mathscr{P}\\) with a substantial level of statistical confidence1.\nAlmost all our work in this book will refer to the variables \\(X\\) as input variables and the variable \\(y\\) as the outcome variable. There are many other common terms for these which you may find in other sources—for example \\(X\\) are often known as independent variables or covariates while \\(y\\) is often known as a dependent or response variable.\n\n\n1.2.2 The process of inferential modeling\nInferential modeling—regression or otherwise—is a process of numerous steps. Typically the main steps are:\n\nDefining the outcome of interest \\(\\mathscr{O}\\) and the input constructs \\(\\mathscr{C}\\) based on a broader evidence-based objective\nConfirming that \\(\\mathscr{O}\\) has reliable measurement data\nDetermining which data can be used to measure \\(\\mathscr{C}\\)\nDetermining a sample \\(S\\) and collecting, refining and cleaning data.\nPerforming exploratory data analysis (EDA) and proposing a set of models to test for \\(f\\)\nPutting the data in an appropriate format for each model\nRunning the models\nInterpreting the outputs and performing model diagnostics\nSelecting an optimal model or models\nArticulating the inferences that can be generalized to apply to \\(\\mathscr{P}\\)\n\nThis book is primarily focused on steps 7–10 of this process2. That is not to say that steps 1–6 are not important. Indeed these steps are critical and often loaded with analytic traps. Defining the problem, collecting reliable measures and cleaning and organizing data are still the source of much pain and angst for analysts, but these topics are for another day.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "quarto/01-intro.html#the-structure-system-and-organization-of-this-book",
    "href": "quarto/01-intro.html#the-structure-system-and-organization-of-this-book",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "1.3 The structure, system and organization of this book",
    "text": "1.3 The structure, system and organization of this book\nThe purpose of this book is to put inexperienced practitioners firmly on a path to the confident and appropriate use of regression techniques in their day-to-day work. This requires enough of an understanding of the underlying theory so that judgments can be made about results, but also a practical set of steps to help practitioners apply the most common regression methods to a variety of typical modeling scenarios in a reliable and reproducible way.\nIn most chapters, time is spent on the underlying mathematics. Not to the degree of an academic theorist, but enough to ensure that the reader can associate some mathematical meaning to the outputs of models. While it may be tempting to skip the math, I strongly recommend against it if you intend to be a high performer in your field. The best analysts are those who can genuinely understand what the numbers are telling them.\nThe statistical programming language R is used for most of the practical demonstration in each chapter. Because R is open source and particularly well geared to inferential statistics, it is an excellent choice for those whose work involves a lot of inferential analysis. In later chapters, we show implementations of all of the available methodologies in Python and Julia, which are also powerful open source tools for this sort of work.\nEach chapter involves a walkthrough example to illustrate the specific method and to allow the reader to replicate the analysis for themselves. The exercises at the end of each chapter are designed so that the reader can try the same method on a different data set, or a different problem on the same data set, to test their learning and understanding. All in all, eleven different data sets are used as walkthrough or exercise examples, and all of these data sets are fictitious constructions unless otherwise indicated. Despite the fiction, they are deliberately designed to present the reader with something resembling how the data might look in practice, albeit cleaner and more organized.\nThe chapters of this book are arranged as follows:\n\nChapter 2 covers the basics of the R programming language for those who want to attempt to jump straight in to the work in subsequent chapters but have very little R experience. Experienced R programmers can skip this chapter.\nChapter 3 covers the essential statistical concepts needed to understand multivariate regression models. It also serves as a tutorial in univariate and bivariate statistics illustrated with real data. If you need help developing a decent understanding of descriptive statistics, random distribution and hypothesis testing, this is an important chapter to study.\nChapter 4 covers linear regression and in the course of that introduces many other foundational concepts. The walkthrough example involves modeling academic results from prior results. The exercises involve modeling income levels based on various work and demographic factors.\nChapter 5 covers binomial logistic regression. The walkthrough example involves modeling promotion likelihood based on performance metrics. The exercises involve modeling charitable donation likelihood based on prior donation behavior and demographics.\nChapter 6 covers multinomial regression. The walkthrough example and exercise involves modeling the choice of three health insurance products by company employees based on demographic and position data.\nChapter 7 covers ordinal regression. The walkthrough example involves modeling in-game disciplinary action against soccer players based on prior discipline and other factors. The exercises involve modeling manager performance based on varied data.\nChapter 8 covers modeling options for data with explicit or latent hierarchy. The first part covers mixed modeling and uses a model of speed dating decisions as a walkthrough and example. The second part covers structural equation modeling and uses a survey for a political party as a walkthrough example. The exercises involve modeling latent variables in an employee engagement survey.\nChapter 9 covers survival analysis, Cox proportional hazard regression and frailty models. The chapter uses employee attrition as a walkthrough example and exercise.\nChapter 10 outlines alternative technical approaches to regression modeling in R, Python and Julia. Models from previous chapters are used to illustrate these alternative approaches.\nChapter 11 covers power analysis, focusing in particular on estimating the required minimum sample sizes in establishing meaningful inferences for both simple statistical tests and multivariate models. Examples related to experimental studies are used to illustrate, such as concurrent validity studies of selection instruments. Example implementations in R, Python and Julia are outlined.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "quarto/01-intro.html#footnotes",
    "href": "quarto/01-intro.html#footnotes",
    "title": "1  The Importance of Regression in People Analytics",
    "section": "",
    "text": "We also determine that \\(x_1\\) (the first-year examination score) plays no significant role in \\(f\\) and that introducing some non-linearity into \\(f\\) further improves the statistical accuracy of the inferred relationship.↩︎\nThe book also addresses Steps 5 and 6 in some chapters.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Importance of Regression in People Analytics</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html",
    "href": "quarto/02-basic_r.html",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "2.1 What is R?\nMost of the work in this book is implemented in the R statistical programming language which, along with Python, is one of the two languages that I use in my day-to-day statistical analysis. Sample implementations in Python are also provided at the end of the relevant chapters or sections. I have made efforts to keep the code as simple as possible, and I have tried to avoid the use of too many external packages. For the most part, readers should see (especially in the earlier chapters) that code blocks are short and simple, relying wherever possible on base R functionality. No doubt there are neater and more effective ways to code some of the material in this book using a wide array of R packages, but my priority has been to keep the code simple, consistent and easily reproducible.\nFor those who wish to follow the method and theory without the implementations in this book, there is no need to read this chapter. However, the style of this book is to use implementation to illustrate theory and practice, and so tolerance of many code blocks will be necessary as you read onward.\nFor those who wish to simply replicate the models as quickly as possible, they will be able to avail of the code block copying feature, which appears whenever you scroll over an input code block. Assuming all the required external packages have been installed, these code blocks should all be transportable and immediately usable. For those who are extra-inquisitive and want to explore how I constructed graphics used for illustration (for which code is usually not displayed), the best place to go is the Github repository for this book.\nThis chapter is for those who wish to learn the methods in this book but do not know how to use R. However, it is not intended to be a full tutorial on R. There are many more qualified individuals and existing resources that would better serve that purpose—in particular I recommend Wickham and Grolemund (2016). It is recommended that you consult these resources and become comfortable with the basics of R before proceeding into the later chapters of this book. However, acknowledging that many will want to dive in sooner rather than later, this chapter covers the absolute basics of R that will allow the uninitiated reader to proceed with at least some orientation.\nR is a programming language that was originally developed by and for statisticians, but in recent years its capabilities and the environments in which it is used have expanded greatly, with extensive use nowadays in academia and the public and private sectors. There are many advantages to using a programming language like R. Here are some:\nThere is often heated debate about which tools are better for doing non-trivial statistical analysis. I personally find that R provides the widest array of resources for those interested in inferential modeling, while Python has a more well-developed toolkit for predictive modeling and machine learning. Since the primary focus of this book is inferential modeling, the in-depth walkthroughs are coded in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#what-is-r",
    "href": "quarto/02-basic_r.html#what-is-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "It is completely free and open source.\nIt is faster and more efficient with memory than popular graphical user interface analytics tools.\nIt facilitates easier replication of analysis from person to person compared with many alternatives.\nIt has a large and growing global community of active users.\nIt has a large and rapidly growing universe of packages, which are all free and which provide the ability to do an extremely wide range of general and highly specialized tasks, statistical and otherwise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#how-to-start-using-r",
    "href": "quarto/02-basic_r.html#how-to-start-using-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.2 How to start using R",
    "text": "2.2 How to start using R\nJust like most programming languages, R itself is an interpreter which receives input and returns output. It is not very easy to use without an IDE. An IDE is an Integrated Development Environment, which is a convenient user interface allowing an R programmer to do all their main tasks including writing and running R code, saving files, viewing data and plots, integrating code into documents and many other things. By far the most popular IDE for R is RStudio. An example of what the RStudio IDE looks like can be seen in Figure 2.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: The RStudio IDE\n\n\n\nTo start using R, follow these steps:\n\nDownload and install the latest version of R from https://www.r-project.org/. Ensure that the version suits your operating system.\nDownload the latest version of the RStudio Desktop IDE from https://posit.co/download/rstudio-desktop/. Consult the user guide at https://docs.posit.co/ide/user/ to help orient yourself to the features of RStudio.\n\nOpen RStudio and play around.\n\nThe initial stages of using R can be challenging, mostly due to the need to become familiar with how R understands, stores and processes data. Extensive trial and error is a learning necessity. Perseverance is important in these early stages, as well as an openness to seek help from others either in person or via online forums. Try not to rely too much on Generative AI tools for help, as they can often lead to you missing important learning steps, and they can also generate error-prone code.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#data-in-r",
    "href": "quarto/02-basic_r.html#data-in-r",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.3 Data in R",
    "text": "2.3 Data in R\nAs you start to do tasks involving data in R, you will generally want to store the things you create so that you can refer to them later. Simply calculating something does not store it in R. For example, a simple calculation like this can be performed easily:\n\n3 + 3\n\n[1] 6\n\n\nHowever, as soon as the calculation is complete, it is forgotten by R because the result hasn’t been assigned anywhere. To store something in your R session, you will assign it a name using the &lt;- operator. So I can assign my previous calculation to an object called my_sum, and this allows me to access the value at any time.\n\n# store the result\nmy_sum &lt;- 3 + 3\n\n# now I can work with it\nmy_sum + 3\n\n[1] 9\n\n\nYou will see above that you can comment your code by simply adding a # to the start of a line to ensure that the line is ignored by the interpreter.\nNote that assignment to an object does not result in the value being displayed. To display the value, the name of the object must be typed, the print() command used or the command should be wrapped in parentheses.\n\n# show me the value of my_sum\nmy_sum\n\n[1] 6\n\n# assign my_sum + 3 to new_sum and show its value\n(new_sum &lt;- my_sum + 3)\n\n[1] 9\n\n\n\n2.3.1 Data types\nAll data in R has an associated type, to reflect the wide range of data that R is able to work with. The typeof() function can be used to see the type of a single scalar value. Let’s look at the most common scalar data types.\nNumeric data can be in integer form or double (decimal) form.\n\n# integers can be signified by adding an 'L' to the end\nmy_integer &lt;- 1L  \nmy_double &lt;- 6.38\n\ntypeof(my_integer)\n\n[1] \"integer\"\n\ntypeof(my_double)\n\n[1] \"double\"\n\n\nCharacter data is text data surrounded by single or double quotes.\n\nmy_character &lt;- \"THIS IS TEXT\"\ntypeof(my_character)\n\n[1] \"character\"\n\n\nLogical data takes the form TRUE or FALSE.\n\nmy_logical &lt;- TRUE\ntypeof(my_logical)\n\n[1] \"logical\"\n\n\n\n\n2.3.2 Homogeneous data structures\nVectors are one-dimensional structures containing data of the same type and are notated by using c(). The type of the vector can also be viewed using the typeof() function, but the str() function can be used to display both the contents of the vector and its type.\n\nmy_double_vector &lt;- c(2.3, 6.8, 4.5, 65, 6)\nstr(my_double_vector)\n\n num [1:5] 2.3 6.8 4.5 65 6\n\n\nCategorical data—which takes only a finite number of possible values—can be stored as a factor vector to make it easier to perform grouping and manipulation.\n\ncategories &lt;- factor(\n  c(\"A\", \"B\", \"C\", \"A\", \"C\")\n)\n\nstr(categories)\n\n Factor w/ 3 levels \"A\",\"B\",\"C\": 1 2 3 1 3\n\n\nIf needed, the factors can be given order.\n\n# character vector \nranking &lt;- c(\"Medium\", \"High\", \"Low\")\nstr(ranking)\n\n chr [1:3] \"Medium\" \"High\" \"Low\"\n\n# turn it into an ordered factor\nranking_factors &lt;- ordered(\n  ranking, levels = c(\"Low\", \"Medium\", \"High\")\n)\n\nstr(ranking_factors)\n\n Ord.factor w/ 3 levels \"Low\"&lt;\"Medium\"&lt;..: 2 3 1\n\n\nThe number of elements in a vector can be seen using the length() function.\n\nlength(categories)\n\n[1] 5\n\n\nSimple numeric sequence vectors can be created using shorthand notation.\n\n(my_sequence &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIf you try to mix data types inside a vector, it will usually result in type coercion, where one or more of the types are forced into a different type to ensure homogeneity. Often this means the vector will become a character vector.\n\n# numeric sequence vector\nvec &lt;- 1:5\nstr(vec)\n\n int [1:5] 1 2 3 4 5\n\n# create a new vector containing vec and the character \"hello\"\nnew_vec &lt;- c(vec, \"hello\")\n\n# numeric values have been coerced into their character equivalents\nstr(new_vec)\n\n chr [1:6] \"1\" \"2\" \"3\" \"4\" \"5\" \"hello\"\n\n\nBut sometimes logical or factor types will be coerced to numeric.\n\n# attempt a mixed logical and numeric\nmix &lt;- c(TRUE, 6)\n\n# logical has been converted to binary numeric (TRUE = 1)\nstr(mix)\n\n num [1:2] 1 6\n\n# try to add a numeric to our previous categories factor vector\nnew_categories &lt;- c(categories, 1)\n\n# categories have been coerced to background integer representations\nstr(new_categories)\n\n num [1:6] 1 2 3 1 3 1\n\n\nMatrices are two-dimensional data structures of the same type and are built from a vector by defining the number of rows and columns. Data is read into the matrix down the columns, starting left and moving right. Matrices are rarely used for non-numeric data types.\n\n# create a 2x2 matrix with the first four integers\n(m &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nArrays are n-dimensional data structures with the same data type and are not used extensively by most R users.\n\n\n2.3.3 Heterogeneous data structures\nLists are one-dimensional data structures that can take data of any type.\n\nmy_list &lt;- list(6, TRUE, \"hello\")\nstr(my_list)\n\nList of 3\n $ : num 6\n $ : logi TRUE\n $ : chr \"hello\"\n\n\nList elements can be any data type and any dimension. Each element can be given a name.\n\nnew_list &lt;- list(\n  scalar = 6, \n  vector = c(\"Hello\", \"Goodbye\"), \n  matrix = matrix(1:4, nrow = 2, ncol = 2)\n)\n\nstr(new_list)\n\nList of 3\n $ scalar: num 6\n $ vector: chr [1:2] \"Hello\" \"Goodbye\"\n $ matrix: int [1:2, 1:2] 1 2 3 4\n\n\nNamed list elements can be accessed by using $.\n\nnew_list$matrix\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nDataframes are the most used data structure in R; they are effectively a named list of vectors of the same length, with each vector as a column. As such, a dataframe is very similar in nature to a typical database table or spreadsheet.\n\n# two vectors of different types but same length\nnames &lt;- c(\"John\", \"Ayesha\")\nages &lt;- c(31, 24)\n\n# create a dataframe\n(df &lt;- data.frame(names, ages))\n\n   names ages\n1   John   31\n2 Ayesha   24\n\n# get types of columns\nstr(df)\n\n'data.frame':   2 obs. of  2 variables:\n $ names: chr  \"John\" \"Ayesha\"\n $ ages : num  31 24\n\n# get dimensions of df\ndim(df)\n\n[1] 2 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#working-with-dataframes",
    "href": "quarto/02-basic_r.html#working-with-dataframes",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.4 Working with dataframes",
    "text": "2.4 Working with dataframes\nThe dataframe is the most common data structure used by analysts in R, due to its similarity to data tables found in databases and spreadsheets. We will work almost entirely with dataframes in this book, so let’s get to know them.\n\n2.4.1 Loading and tidying data in dataframes\nTo work with data in R, you usually need to pull it in from an outside source into a dataframe1. R facilitates numerous ways of importing data from simple .csv files, from Excel files, from online sources or from databases. Let’s load a data set that we will use later—the salespeople data set, which contains some information on the sales, average customer ratings and performance ratings of salespeople. The read.csv() function can accept a URL address of the file if it is online.\n\n# url of data set \nurl &lt;- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\n\n# load the data set and store it as a dataframe called salespeople\nsalespeople &lt;- read.csv(url)\n\nWe might not want to display this entire data set before knowing how big it is. We can view the dimensions, and if it is too big to display, we can use the head() function to display just the first few rows.\n\ndim(salespeople)\n\n[1] 351   4\n\n# hundreds of rows, so view first few\nhead(salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\nWe can view a specific column by using $, and we can use square brackets to view a specific entry. For example if we wanted to see the 6th entry of the sales column:\n\nsalespeople$sales[6]\n\n[1] 918\n\n\nAlternatively, we can use a [row, column] index to get a specific entry in the dataframe.\n\nsalespeople[34, 4]\n\n[1] 3\n\n\nWe can take a look at the data types using str().\n\nstr(salespeople)\n\n'data.frame':   351 obs. of  4 variables:\n $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n $ performance  : int  2 3 4 2 3 2 3 1 3 3 ...\n\n\nWe can also see a statistical summary of each column using summary(), which tells us various statistics depending on the type of the column.\n\nsummary(salespeople)\n\n    promoted          sales       customer_rate    performance \n Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  \n 1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  \n Median :0.0000   Median :475.0   Median :3.620   Median :3.0  \n Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  \n 3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  \n Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  \n                  NA's   :1       NA's   :1       NA's   :1    \n\n\nNote that there is missing data in this dataframe, indicated by NAs in the summary. Missing data is identified by a special NA value in R. This should not be confused with \"NA\", which is simply a character string. The function is.na() will look at all values in a vector or dataframe and return TRUE or FALSE based on whether they are NA or not. By adding these up using the sum() function, it will take TRUE as 1 and FALSE as 0, which effectively provides a count of missing data.\n\nsum(is.na(salespeople))\n\n[1] 3\n\n\nThis is a small number of NAs given the dimensions of our data set and we might want to remove the rows of data that contain NAs. The easiest way is to use the complete.cases() function, which identifies the rows that have no NAs, and then we can select those rows from the dataframe based on that condition. Note that you can overwrite objects with the same name in R.\n\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# confirm no NAs\nsum(is.na(salespeople))\n\n[1] 0\n\n\nWe can see the unique values of a vector or column using the unique() function.\n\nunique(salespeople$performance)\n\n[1] 2 3 4 1\n\n\nIf we need to change the type of a column in a dataframe, we can use the as.numeric(), as.character(), as.logical() or as.factor() functions. For example, given that there are only four unique values for the performance column, we may want to convert it to a factor.\n\nsalespeople$performance &lt;- as.factor(salespeople$performance)\nstr(salespeople)\n\n'data.frame':   350 obs. of  4 variables:\n $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n $ performance  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 3 4 2 3 2 3 1 3 3 ...\n\n\n\n\n2.4.2 Manipulating dataframes\nDataframes can be subsetted to contain only rows that satisfy specific conditions.\n\n(sales_720 &lt;- subset(salespeople, subset = sales == 720))\n\n    promoted sales customer_rate performance\n290        1   720          3.76           3\n\n\nNote the use of ==, which is used in many programming languages, to test for precise equality. Similarly we can select columns based on inequalities (&gt; for ‘greater than’‍, &lt; for ‘less than’‍, &gt;= for ‘greater than or equal to’‍, &lt;= for ‘less than or equal to’‍, or != for ‘not equal to’). For example:\n\nhigh_sales &lt;- subset(salespeople, subset = sales &gt;= 700)\nhead(high_sales)\n\n   promoted sales customer_rate performance\n6         1   918          4.54           2\n12        1   716          3.16           3\n20        1   937          5.00           2\n21        1   702          3.53           4\n25        1   819          4.45           2\n26        1   736          3.94           4\n\n\nTo select specific columns use the select argument.\n\nsalespeople_sales_perf &lt;- subset(salespeople, \n                                 select = c(\"sales\", \"performance\"))\nhead(salespeople_sales_perf)\n\n  sales performance\n1   594           2\n2   446           3\n3   674           4\n4   525           2\n5   657           3\n6   918           2\n\n\nTwo dataframes with the same column names can be combined by their rows.\n\nlow_sales &lt;- subset(salespeople, subset = sales &lt; 400)\n\n# bind the rows of low_sales and high_sales together\nlow_and_high_sales = rbind(low_sales, high_sales)\nhead(low_and_high_sales)\n\n   promoted sales customer_rate performance\n7         0   318          3.09           3\n8         0   364          4.89           1\n9         0   342          3.74           3\n10        0   387          3.00           3\n15        0   344          3.02           2\n16        0   372          3.87           3\n\n\nTwo dataframes with different column names can be combined by their columns.\n\n# two dataframes with two columns each\nsales_perf &lt;- subset(salespeople, \n                     select = c(\"sales\", \"performance\"))\nprom_custrate &lt;- subset(salespeople, \n                        select = c(\"promoted\", \"customer_rate\"))\n\n# bind the columns to create a dataframe with four columns\nfull_df &lt;- cbind(sales_perf, prom_custrate)\nhead(full_df)\n\n  sales performance promoted customer_rate\n1   594           2        0          3.94\n2   446           3        0          4.06\n3   674           4        1          3.83\n4   525           2        0          3.62\n5   657           3        1          4.40\n6   918           2        1          4.54",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#functions-packages-and-libraries",
    "href": "quarto/02-basic_r.html#functions-packages-and-libraries",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.5 Functions, packages and libraries",
    "text": "2.5 Functions, packages and libraries\nIn the code so far we have used a variety of functions. For example head(), subset(), rbind(). Functions are operations that take certain defined inputs and return an output. Functions exist to perform common useful operations.\n\n2.5.1 Using functions\nFunctions usually take one or more arguments. Often there are a large number of arguments that a function can take, but many are optional and not required to be specified by the user. For example, the function head(), which displays the first rows of a dataframe2, has only one required argument x: the name of the dataframe. A second argument is optional, n: the number of rows to display. If n is not entered, it is assumed to have the default value n = 6.\nWhen running a function, you can either specify the arguments by name or you can enter them in order without their names. If you enter arguments without naming them, R expects the arguments to be entered in exactly the right order.\n\n# see the head of salespeople, with the default of six rows\nhead(salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\n\n# see fewer rows - arguments need to be in the right order if not named\nhead(salespeople, 3)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n\n\n\n# or if you don't know the right order, \n# name your arguments and you can put them in any order\nhead(n = 3, x = salespeople)\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n\n\n\n\n2.5.2 Help with functions\nMost functions in R have excellent help documentation. To get help on the head() function, type help(head) or ?head. This will display the results in the Help browser window in RStudio. Alternatively you can open the Help browser window directly in RStudio and do a search there. An example of the browser results for head() is in Figure 2.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Results of a search for the head() function in the RStudio Help browser\n\n\n\nThe help page normally shows the following:\n\nDescription of the purpose of the function\nUsage examples, so you can quickly see how it is used\nArguments list so you can see the names and order of arguments\nDetails or notes on further considerations on use\nExpected value of the output (for example head() is expected to return a similar object to its first input x)\nExamples to help orient you further (sometimes examples can be very abstract in nature and not so helpful to users)\n\n\n\n2.5.3 Writing your own functions\nFunctions are not limited to those that come packaged in R. Users can write their own functions to perform tasks that are helpful to their objectives. Experienced programmers in most languages subscribe to a principle called DRY (Don’t Repeat Yourself). Whenever a task needs to be done repeatedly, it is poor practice to write the same code numerous times. It makes more sense to write a function to do the task.\nIn this example, a simple function is written which generates a report on a dataframe:\n\n# create df_report function\ndf_report &lt;- function(df) {\n  paste(\"This dataframe contains\", nrow(df), \"rows and\", \n        ncol(df), \"columns. There are\", sum(is.na(df)), \"NA entries.\")\n}\n\nWe can test our function by using the built-in mtcars data set in R.\n\ndf_report(mtcars)\n\n[1] \"This dataframe contains 32 rows and 11 columns. There are 0 NA entries.\"\n\n\n\n\n2.5.4 Installing packages\nAll the common functions that we have used so far exist in the base R installation. However, the beauty of open source languages like R is that users can write their own functions or resources and release them to others via packages. A package is an additional module that can be installed easily; it makes resources available which are not in the base R installation. In this book we will be using functions from both base R and from popular and useful packages. As an example, a popular package used for statistical modeling is the MASS package, which is based on methods in a popular applied statistics book3.\nBefore an external package can be used, it must be installed into your package library using install.packages(). So to install MASS, type install.packages(\"MASS\") into the console. This will send R to the main internet repository for R packages (known as CRAN). It will find the right version of MASS for your operating system and download and install it into your package library. If MASS needs other packages in order to work, it will also install these packages.\nIf you want to install more than one package, put the names of the packages inside a character vector—for example:\n\nmy_packages &lt;- c(\"MASS\", \"DescTools\", \"dplyr\")\ninstall.packages(my_packages)\n\nOnce you have installed a package, you can see what functions are available by calling for help on it, for example using help(package = MASS). One package you may wish to install now is the peopleanalyticsdata package, which contains all the data sets used in this book. By installing and loading this package, all the data sets used in this book will be loaded into your R session and ready to work with. If you do this, you can ignore the read.csv() commands later in the book, which download the data from the internet.\n\n\n2.5.5 Using packages\nOnce you have installed a package into your package library, to use it in your R session you need to load it using the library() function. For example, to load MASS after installing it, use library(MASS). Often nothing will happen when you use this command, but rest assured the package has been loaded and you can start to use the functions inside it. Sometimes when you load the package a series of messages will display, usually to make you aware of certain things that you need to keep in mind when using the package. Note that whenever you see the library() command in this book, it is assumed that you have already installed the package in that command. If you have not, the library() command will fail.\nOnce a package is loaded from your library, you can use any of the functions inside it. For example, the stepAIC() function is not available before you load the MASS package but becomes available after it is loaded. In this sense, functions ‘belong’ to packages.\nProblems can occur when you load packages that contain functions with the same name as functions that already exist in your R session. Often the messages you see when loading a package will alert you to this. When R is faced with a situation where a function exists in multiple packages you have loaded, R always defaults to the function in the most recently loaded package. This may not always be what you intended.\nOne way to completely avoid this issue is to get in the habit of namespacing your functions. To namespace, you simply use package::function(), so to safely call stepAIC() from MASS, you use MASS::stepAIC(). Most of the time in this book when a function is being called from a package outside base R, I use namespacing to call that function. This should help avoid confusion about which packages are being used for which functions.\n\n\n2.5.6 The pipe operator\nEven in the most elementary briefing about R, it is very difficult to ignore the pipe operator. The pipe operator makes code more natural to read and write and reduces the typical computing problem of many nested operations inside parentheses.\nAs an example, imagine we wanted to do the following two operations in one command:\n\nSubset salespeople to only the sales values of those with sales less than 500\nTake the mean of those values\n\nIn base R, one way to do this is:\n\nmean(subset(salespeople$sales, subset = salespeople$sales &lt; 500))\n\n[1] 388.6684\n\n\nThis is nested and needs to be read from the inside out in order to align with the instructions. The pipe operator |&gt; takes the command that comes before it and places it inside the function that follows it (by default as the first argument). This reduces complexity and allows you to follow the logic more clearly.\n\n# use the pipe operator to lay out the steps more logically\nsubset(salespeople$sales, subset = salespeople$sales &lt; 500) |&gt; \n  mean() \n\n[1] 388.6684\n\n\nThis can be extended to perform arbitrarily many operations in one piped command.\n\nsalespeople$sales |&gt; # start with all data\n  subset(subset = salespeople$sales &lt; 500) |&gt; # get the subsetted data\n  mean() |&gt; # take the mean value\n  round() # round to the nearest integer\n\n[1] 389\n\n\nThe pipe operator is unique to R and is very widely used because it helps to make code more readable, it reduces complexity, and it helps orient around a common ‘grammar’ for the manipulation of data. The pipe operator helps you structure your code more clearly around nouns (objects), verbs (functions) and adverbs (arguments of functions). One of the most developed sets of packages in R that follows these principles is the tidyverse family of packages, which I encourage you to explore.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#errors-warnings-and-messages",
    "href": "quarto/02-basic_r.html#errors-warnings-and-messages",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.6 Errors, warnings and messages",
    "text": "2.6 Errors, warnings and messages\nAs I mentioned earlier in this chapter, getting familiar with R can be frustrating at the beginning if you have never programmed before. You can expect to regularly see messages, warnings or errors in response to your commands. I encourage you to regard these as your friend rather than your enemy. It is very tempting to take the latter approach when you are starting out, but over time I hope you will appreciate some wisdom from my words.\nErrors are serious problems which usually result in the halting of your code and a failure to return your requested output. They usually come with an indication of the source of the error, and these can sometimes be easy to understand and sometimes frustratingly vague and abstract. For example, an easy-to-understand error is:\n\nsubset(salespeople, subset = sales = 720)\n\nError: unexpected '=' in \"subset(salespeople, subset = sales =\"\nThis helps you see that you have used sales = 720 as a condition to subset your data, when you should have used sales == 720 for precise equality.\nA much more challenging error to understand is:\n\nhead[salespeople]\n\nError in head[salespeople] : object of type 'closure' is not subsettable\nWhen first faced with an error that you can’t understand, try not to get frustrated and proceed in the knowledge that it usually can be fixed easily and quickly. Often the problem is much more obvious than you think, and if not, there is still a 99% likelihood that others have made this error and you can read about it online. The first step is to take a look at your code to see if you can spot what you did wrong. In this case, you may see that you have used square brackets [] instead of parentheses () when calling your head() function. If you cannot see what is wrong, the next steps are to ask a colleague, do an internet search with the text of the error message you receive, ask a Generative AI coding assistant or consult online forums like https://stackoverflow.com. The more experienced you become, the easier it is to interpret error messages.\nWarnings are less serious and usually alert you to something that you might be overlooking and which could indicate a problem with the output. In many cases you can ignore warnings, but sometimes they are an important reminder to go back and edit your code. For example, you may run a model which doesn’t converge, and while this does not stop R from returning results, it is also very useful for you to know that it didn’t converge.\nMessages are pieces of information that may or may not be useful to you at a particular point in time. Sometimes you will receive messages when you load a package from your library. Sometimes messages will keep you up to date on the progress of a process that is taking a long time to execute.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#plotting-and-graphing",
    "href": "quarto/02-basic_r.html#plotting-and-graphing",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.7 Plotting and graphing",
    "text": "2.7 Plotting and graphing\nAs you might expect in a well-developed programming language, there are numerous ways to plot and graph information in R. If you are doing exploratory data analysis on fairly simple data and you don’t need to worry about pretty appearance or formatting, the built-in plot capabilities of base R are fine. If you need a pretty appearance, more precision, color coding or even 3D graphics or animation, there are also specialized plotting and graphing packages for these purposes. In general when working interactively in RStudio, graphical output will be rendered in the Plots pane, where you can copy it or save it as an image.\n\n2.7.1 Plotting in base R\nThe simplest plot function in base R is plot(). This performs basic X-Y plotting. As an example, this code will generate a scatter plot of customer_rate against sales in the salespeople data set, with the results displayed in Figure 2.3. Note the use of the arguments main, xlab and ylab for customizing the axis labels and title for the plot.\n\n\n\n\n# scatter plot of customer_rate against sales\nplot(x = salespeople$sales, y = salespeople$customer_rate,\n     xlab = \"Sales ($m)\", ylab = \"Average customer rating\",\n     main = \"Scatterplot of Sales vs Customer Rating\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Simple scatterplot of customer_rate against sales in the salespeople data set\n\n\n\nHistograms of data can be generated using the hist() function. This command will generate a histogram of performance as displayed in Figure 2.4. Note the use of breaks to customize how the bars appear.\n\n\n\n\n# convert performance ratings back to numeric data type for histogram\nsalespeople$performance &lt;- as.numeric(salespeople$performance)\n\n# histogram of performance ratings\nhist(salespeople$performance, breaks = 0:4,\n     xlab = \"Performance Rating\", \n     main = \"Histogram of Performance Ratings\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Simple histogram of performance in the salespeople data set\n\n\n\nBox and whisker plots are excellent ways to see the distribution of a variable, and can be grouped against another variable to see bivariate patterns. For example, this command will show a box and whisker plot of sales grouped against performance, with the output shown in Figure 2.5. Note the use of the formula and data notation here to define the variable we are interested in and how we want it grouped. We will study this formula notation in greater depth later in this book.\n\n\n\n\n# box plot of sales by performance rating\nboxplot(formula = sales ~ performance, data = salespeople,\n        xlab = \"Performance Rating\", ylab = \"Sales ($m)\",\n        main = \"Boxplot of Sales by Performance Rating\")\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Simple box plot of sales grouped against performance in the salespeople data set\n\n\n\nThese are among the most common plots used for data exploration purposes. They are examples of a wider range of plotting and graphing functions available in base R, such as line plots, bar plots and other varieties which you may see later in this book.\n\n\n2.7.2 Specialist plotting and graphing packages\nBy far the most commonly used specialist plotting and graphing package in R is ggplot2. ggplot2 allows the flexible construction of a very wide range of charts and graphs, but uses a very specific command grammar which can take some getting used to. However, once learned, ggplot2 can be an extremely powerful tool. Many of the illustratory figures used in this book are developed using ggplot2 and while the code for these figures is generally not included for the sake of brevity, you can always find it in the source code of this book on Github4. A great learning resource for ggplot2 is Wickham (2016).\nThe plotly package allows the use of the plotly graphing library in R. This is an excellent package for interactive graphing and is used for 3D illustrations in this book. Output can be rendered in HTML—allowing the user to play with and explore the graphs interactively—or can be saved as static 2D images.\nGGally is a package that extends ggplot2 to allow easy combination of charts and graphs. This is particularly valuable for quicker exploratory data analysis. One of its most popular functions is ggpairs(), which produces a pairplot. A pairplot is a visualization of all univariate and bivariate patterns in a data set, with univariate distributions in the diagonal and bivariate relationships or correlations displayed in the off-diagonal. Figure 2.6 is an example of a pairplot for the salespeople data set, which we will explore further in Chapter @ref(bin-log-reg).\n\n\n\n\nlibrary(GGally)\n\n# convert performance and promotion to categorical\nsalespeople$promoted &lt;- as.factor(salespeople$promoted)\nsalespeople$performance &lt;- as.factor(salespeople$performance)\n\n# pairplot of salespeople\nGGally::ggpairs(salespeople)\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Pairplot of the salespeople data set",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#documenting-your-work-using-quarto",
    "href": "quarto/02-basic_r.html#documenting-your-work-using-quarto",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.8 Documenting your work using Quarto",
    "text": "2.8 Documenting your work using Quarto\nFor anyone performing any sort of multivariable analysis using a statistical programming language, appropriate documentation and reproducibility of the work is essential to its success and longevity. If your code is not easily obtained or run by others, it is likely to have a very limited impact and lifetime. Learning how to create integrated documents that contain both text and code is critical to providing access to your code and narration of your work.\nQuarto is a package which allows you to create integrated documents containing both formatted text and executed code. It is, in my opinion, one of the best resources available currently for this purpose. This entire book has been created using Quarto. You can start a Quarto document in RStudio by installing the by opening a new Quarto document file, which will have the suffix .qmd.\nQuarto documents always start with a particular heading type called a YAML header, which contains overall information on the document you are creating. Care must be taken with the precise formatting of the YAML header, as it is sensitive to spacing and indentation. Usually a basic YAML header is created for you in RStudio when you start a new .qmd file. Here is an example.\n---\ntitle: \"My new document\"\nauthor: \"Keith McNulty\"\ndate: 2025-11-10\nformat: html\n---\nThe format part of this header has numerous options, but the most commonly used are html, which generates your document as a web page, and pdf, which generates your document as a PDF using the open source LaTeX software package. If you wish to create PDF documents you will need to have a version of LaTeX installed on your system. Running the system command quarto install tinytex will install a minimal version of LaTeX which is fine for most purposes.\nQuarto allows you to build a formatted document using many shorthand formatting commands. Here are a few examples of how to format headings and place web links or images in your document:\n# My top heading\n\nThis section is about this general topic.\n\n## My first sub heading \n\nTo see more information on this sub-topic visit [here](https://my.web.link).\n\n## My second sub heading\n\nHere is a nice picture about this sub-topic.\n\n![](path/to/image)\nCode can be written and executed and the results displayed inline using backticks. For example, writing\n`r nrow(salespeople)`\ninline will display 351 in the final document. Entire code blocks can be included and executed by using triple-backticks. The following code block:\n```{r}\n# show the first few rows of salespeople\nhead(salespeople)\n```\nwill display this output:\n\n\n  promoted sales customer_rate performance\n1        0   594          3.94           2\n2        0   446          4.06           3\n3        1   674          3.83           4\n4        0   525          3.62           2\n5        1   657          4.40           3\n6        1   918          4.54           2\n\n\nThe {} wrapping allows you to specify different languages for your code chunk. For example, if you wanted to run Python code instead of R code you can use {python}.\nYou can set options for your code chunks inside a code block by starting your line with #|. For example, if you want the results of your code to be displayed, but without the code itself being displayed, you can start your code block with the line #| echo: false.\nThe process of compiling your Quarto code to produce a document is known as ‘rendering’. To create a rendered document, you simply need to click on the ‘Render’ button in RStudio that appears above your Quarto code.\nIf you are not familiar with Quarto, I strongly encourage you to learn it alongside R and to challenge yourself to write up any practice exercises you take on in this book using Quarto. You can find a really helpful and thorough user guide for Quarto at https://quarto.org/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#learning-exercises",
    "href": "quarto/02-basic_r.html#learning-exercises",
    "title": "2  The Basics of the R Programming Language",
    "section": "2.9 Learning exercises",
    "text": "2.9 Learning exercises\n\n2.9.1 Discussion questions\n\nDescribe the following data types: numeric, character, logical, factor.\nWhy is a vector known as a homogeneous data structure?\nGive an example of a heterogeneous data structure in R.\nWhat is the difference between NA and \"NA\"?\nWhat operator is used to return named elements of a list and named columns of a dataframe?\nDescribe some functions that are used to manipulate dataframes.\nWhat is a package and how do you install and use a new package?\nDescribe what is meant by ‘namespacing’ and why it might be useful.\nWhat is the pipe operator, and why is it popular in R?\nWhat is the difference between an error and a warning in R?\nName some simple plotting functions in base R.\nName some common specialist plotting and graphing packages in R.\nWhat is Quarto, and why is it useful to someone performing analysis using programming languages?\n\n\n\n2.9.2 Data exercises\n\nCreate a character vector called my_names that contains all your first, middle and last names as elements. Calculate the length of my_names.\nCreate a second numeric vector called which which corresponds to my_names. The entries should be the position of each name in the order of your full name. Verify that it has the same length as my_names.\nCreate a dataframe called names, which consists of the two vectors my_names and which as columns. Calculate the dimensions of names.\nCreate a new dataframe new_names with the which column converted to character type. Verify that your command worked using str().\nLoad the ugtests data set via the peopleanalyticsdata package or download it from the internet5. Calculate the dimensions of ugtests and view the first three rows only.\nView a statistical summary of all of the columns of ugtests. Determine if there are any missing values.\nView the subset of ugtests for values of Yr1 greater than 50.\nInstall and load the package dplyr. Look up the help for the filter() function in this package and try to use it to repeat the task in the previous question.\nWrite code to find the mean of the Yr1 test scores for all those who achieved Yr3 test scores greater than 100. Round this mean to the nearest integer.\nFamiliarize yourself with the two functions filter() and pull() from dplyr. Use these functions to try to do the same calculation in the previous question using a single unbroken piped command. Be sure to namespace where necessary.\nCreate a scatter plot using the ugtests data with Final scores on the \\(y\\) axis and Yr3 scores on the \\(x\\) axis.\nCreate your own 5-level grading logic and use it to create a new finalgrade column in the ugtests data set with grades 1–5 of increasing attainment based on the Final score in ugtests. Generate a histogram of this finalgrade column.\nUsing your new ugtests data with the extra column from the previous exercise, create a box plot of Yr3 scores grouped by finalgrade.\nRender all of your answers to these exercises into a Quarto document. Create one version that displays your code and answers, and another that just displays the answers.\n\n\n\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/02-basic_r.html#footnotes",
    "href": "quarto/02-basic_r.html#footnotes",
    "title": "2  The Basics of the R Programming Language",
    "section": "",
    "text": "R also has some built-in data sets for testing and playing with. For example, check out mtcars by typing it into the terminal, or type data() to see a full list of built-in data sets.↩︎\nIt actually has a broader definition but is mostly used for showing the first rows of a dataframe.↩︎\nVenables and Ripley (2002)↩︎\nhttps://github.com/keithmcnulty/peopleanalytics-regression-book↩︎\nhttp://peopleanalytics-regression-book.org/data/ugtests.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Basics of the R Programming Language</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html",
    "href": "quarto/03-primer_stats.html",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "3.1 Elementary descriptive statistics of populations and samples\nTo properly understand multivariable models, an analyst needs to have a decent grasp of foundational statistics. Many of the assumptions and results of multivariable models require an understanding of these foundations in order to be properly interpreted. There are three topics that are particularly important for those proceeding further in this book:\nIf you have never really studied these topics, I would strongly recommend taking a course in them and spending good time getting to know them. Again, just as the last chapter was not intended to be a comprehensive tutorial on R, neither is this chapter intended to be a comprehensive tutorial on introductory statistics. However, we will introduce some key concepts here that are critical to understanding later chapters, and as always we will illustrate using real data examples.\nIn preparation for this chapter we are going to download a data set that we will work through in a later chapter, and use it for practical examples and illustration purposes. The data are a set of information on the sales, customer ratings and performance ratings on a set of 351 salespeople as well as an indication of whether or not they were promoted.\nLet’s take a brief look at the first few rows of this data to make sure we know what is inside it.\nAnd let’s understand the structure of this data.\nIt looks like:\nAny collection of numerical data on one or more variables can be described using a number of common statistical concepts. Let \\(x = x_1, x_2, \\dots, x_n\\) be a sample of \\(n\\) observations of a variable drawn from a population.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#elementary-descriptive-statistics-of-populations-and-samples",
    "href": "quarto/03-primer_stats.html#elementary-descriptive-statistics-of-populations-and-samples",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "3.1.1 Mean, variance and standard deviation\nThe mean is the average value of the observations and is defined by adding up all the values and dividing by the number of observations. The mean \\(\\bar{x}\\) of our sample \\(x\\) is defined as:\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^{n}x_i\n\\] While the mean of a sample \\(x\\) is denoted by \\(\\bar{x}\\), the mean of an entire population is usually denoted by \\(\\mu\\). The mean can have a different interpretation depending on the type of data being studied. Let’s look at the mean of three different columns of our salespeople data, making sure to ignore any missing data.\n\nmean(salespeople$sales, na.rm = TRUE)\n\n[1] 527.0057\n\n\nThis looks very intuitive and appears to be the average amount of sales made by the individuals in the data set.\n\nmean(salespeople$promoted, na.rm = TRUE)\n\n[1] 0.3219373\n\n\nGiven that this data can only have the value of 0 or 1, we interpret this mean as the likelihood or expectation that an individual will be labeled as 1. That is, the average probability of promotion in the data set. If this data showed a perfectly random likelihood of promotion, we would expect this to take the value of 0.5. But it is lower than 0.5, which tells us that the majority of individuals are not promoted.\n\nmean(salespeople$performance, na.rm = TRUE)\n\n[1] 2.5\n\n\nGiven that this data can only have the values 1, 2, 3 or 4, we interpret this as the expected value of the performance rating in the data set. Higher or lower means inform us about the distribution of the performance ratings. A low mean will indicate a skew towards a low rating, and a high mean will indicate a skew towards a high rating.\nOther common statistical summary measures include the median, which is the middle value when the values are ranked in order, and the mode, which is the most frequently occurring value.\nThe variance is a measure of how much the data varies around its mean. There are two different definitions of variance. The population variance assumes that that we are working with the entire population and is defined as the average squared difference from the mean:\n\\[\n\\mathrm{Var}_p(x) = \\frac{1}{n}\\sum_{i = 1}^{n}(x_i - \\bar{x})^2\n\\] The sample variance assumes that we are working with a sample and attempts to estimate the variance of a larger population by applying Bessel’s correction to account for potential sampling error. The sample variance is:\n\\[\n\\mathrm{Var}_s(x) = \\frac{1}{n-1}\\sum_{i = 1}^{n}(x_i - \\bar{x})^2\n\\]\nYou can see that\n\\[\n\\mathrm{Var}_p(x) = \\frac{n - 1}{n}\\mathrm{Var}_s(x)\n\\] So as the data set gets larger, the sample variance and the population variance become less and less distinguishable, which intuitively makes sense.\nBecause we rarely work with full populations, the sample variance is calculated by default in R and in many other statistical software packages.\n\n# sample variance \n(sample_variance_sales &lt;- var(salespeople$sales, na.rm = TRUE))\n\n[1] 34308.11\n\n\nSo where necessary, we need to apply a transformation to get the population variance.\n\n# population variance (need length of non-NA data)\nn &lt;- length(na.omit(salespeople$sales))\n(population_variance_sales &lt;- ((n-1)/n) * sample_variance_sales)\n\n[1] 34210.09\n\n\nVariance does not have intuitive scale relative to the data being studied, because we have used a ‘squared distance metric’‍, therefore we can square-root it to get a measure of ‘deviance’ on the same scale as the data. We call this the standard deviation \\(\\sigma(x)\\), where \\(\\mathrm{Var}(x) = \\sigma(x)^2\\). As with variance, standard deviation has both population and sample versions, and the sample version is calculated by default. Conversion between the two takes the form\n\\[\n\\sigma_p(x) = \\sqrt{\\frac{n-1}{n}}\\sigma_s(x)\n\\]\n\n# sample standard deviation\n(sample_sd_sales &lt;- sd(salespeople$sales, na.rm = TRUE))\n\n[1] 185.2245\n\n# verify that sample sd is sqrt(sample var)\nsample_sd_sales == sqrt(sample_variance_sales)\n\n[1] TRUE\n\n# calculate population standard deviation\n(population_sd_sales &lt;- sqrt((n-1)/n) * sample_sd_sales)\n\n[1] 184.9597\n\n\nGiven the range of sales is [151, 945] and the mean is 527, we see that the standard deviation gives a more intuitive sense of the ‘spread’ of the data relative to its inherent scale.\n\n\n3.1.2 Covariance and correlation\nThe covariance between two variables is a measure of the extent to which one changes as the other changes. If \\(y = y_1, y_2, \\dots, y_n\\) is a second variable, and \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of \\(x\\) and \\(y\\), respectively, then the sample covariance of \\(x\\) and \\(y\\) is defined as\n\\[\n\\mathrm{cov}_s(x, y) = \\frac{1}{n - 1}\\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\\] and as with variance, the population covariance is\n\\[\n\\mathrm{cov}_p(x, y) = \\frac{n-1}{n}\\mathrm{cov}_s(x, y)\n\\]\nAgain, the sample covariance is the default in R, and we need to transform to obtain the population covariance.\n\n# get sample covariance for sales and customer_rate, \n# ignoring observations with missing data\n(sample_cov &lt;- cov(salespeople$sales, salespeople$customer_rate, \n                   use = \"complete.obs\"))\n\n[1] 55.81769\n\n# convert to population covariance (need number of complete obs)\ncols &lt;- subset(salespeople, select = c(\"sales\", \"customer_rate\"))\nn &lt;- nrow(cols[complete.cases(cols), ])\n(population_cov &lt;- ((n-1)/n) * sample_cov)\n\n[1] 55.65821\n\n\nAs can be seen, the difference in covariance is very small between the sample and population versions, and both confirm a positive relationship between sales and customer rating. However, we again see this issue that there is no intuitive sense of scale for this measure.\nPearson’s correlation coefficient divides the covariance by the product of the standard deviations of the two variables:\n\\[\nr_{x, y} = \\frac{\\mathrm{cov}(x, y)}{\\sigma(x)\\sigma(y)}\n\\] This creates a scale of \\(-1\\) to \\(1\\) for \\(r_{x, y}\\), which is an intuitive way of understanding both the direction and strength of the relationship between \\(x\\) and \\(y\\), with \\(-1\\) indicating that \\(x\\) increases perfectly as \\(y\\) decreases, \\(1\\) indicating that \\(x\\) increases perfectly as \\(y\\) increases, and \\(0\\) indicating that there is no relationship between the two.\nAs before, there is a sample and population version of the correlation coefficient, and R calculates the sample version by default. Similar transformations can be used to determine a population correlation coefficient and over large samples the two measures converge.\n\n# calculate sample correlation between sales and customer_rate\ncor(salespeople$sales, salespeople$customer_rate, use = \"complete.obs\")\n\n[1] 0.337805\n\n\nThis tells us that there is a moderate positive correlation between sales and customer rating.\nYou will notice that we have so far used two variables on a continuous scale to demonstrate covariance and correlation. Pearson’s correlation can also be used between a continuous scale and a dichotomous (binary) scale variable, and this is known as a point-biserial correlation.\n\ncor(salespeople$sales, salespeople$promoted, use = \"complete.obs\")\n\n[1] 0.8511283\n\n\nCorrelating ranked variables involves an adjusted approach leading to Spearman’s rho (\\(\\rho\\)) or Kendall’s tau (\\(\\tau\\)), among others. We will not dive into the mathematics of this here, but a good source is Bhattacharya and Burman (2016). Spearman’s or Kendall’s variant should be used whenever at least one of the variables is a ranked variable, and both variants are available in R.\n\n# spearman's rho correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"spearman\", use = \"complete.obs\")\n\n[1] 0.2735446\n\n# kendall's tau correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"kendall\", use = \"complete.obs\")\n\n[1] 0.2073609\n\n\nIn this case, both indicate a low to moderate correlation. Spearman’s rho or Kendall’s tau can also be used to correlate a ranked and a dichotomous variable, and this is known as a rank-biserial correlation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#distribution-of-random-variables",
    "href": "quarto/03-primer_stats.html#distribution-of-random-variables",
    "title": "3  Statistics Foundations",
    "section": "3.2 Distribution of random variables",
    "text": "3.2 Distribution of random variables\nAs we outlined in Section @ref(theory-modeling), when we build a model we are using a set of sample data to infer a general relationship on a larger population. A major underlying assumption in our inference is that we believe the real-life variables we are dealing with are random in nature. For example, we might be trying to model the drivers of the voting choice of millions of people in a national election, but we may only have sample data on a few thousand people. When we infer nationwide voting intentions from our sample, we assume that the characteristics of the voting population are random variables.\n\n3.2.1 Sampling of random variables\nWhen we describe variables as random, we are assuming that they take a form which is independent and identically distributed. Using our salespeople data as an example, we are assuming that the sales of one person in the data set is not influenced by the sales of another person in the data set. In this case, this seems like a reasonable assumption, and we will be making it for many (though not all) of the statistical methods used in this book. However, it is good to recognize that there are scenarios where this assumption cannot be made. For example, if the salespeople worked together in serving the same customers on the same products, and each individual’s sales represented some proportion of the overall sales to the customer, we cannot say that the sales data is independent and identically distributed. In this case, we will expect to see some hierarchy in our data and will need to adjust our techniques accordingly to take this into consideration.\nUnder the central limit theorem, if we take samples from a random variable and calculate a summary statistic for each sample, that statistic is itself a random variable, and its mean converges to the true population statistic with more and more sampling. Let’s test this with a little experiment on our salespeople data. Figure 3.1 shows the results of taking 10, 100 and 1000 different random samples of 50, 100 and 150 salespeople from the salespeople data set and creating a histogram of the resulting mean sales values. We can see how greater numbers of samples (down the rows) lead to a more normal distribution curve and larger sample sizes (across the columns) lead to a ‘spikier’ distribution with a smaller standard deviation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Histogram and density of mean sales from the salespeople data set based on sample sizes of 50, 100 and 150 (columns) and 10, 100 and 1000 samplings (rows)\n\n\n\n\n\n3.2.2 Standard errors, the \\(t\\)-distribution and confidence intervals\nOne consequence of the observations in Figure 3.1 is that the summary statistics calculated from larger sample sizes fall into distributions that are ‘narrower’ and hence represent more precise estimations of the population statistic. The standard deviation of a sampled statistic is called the standard error of that statistic. In the special case of a sampled mean, the formula for the standard error of the mean can be derived to be\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(\\sigma\\) is the (sample) standard deviation and \\(n\\) is the sample size1. This confirms that the standard error of the mean decreases with greater sample size, confirming our intuition that the estimation of the mean is more precise with larger samples.\nTo apply this logic to our salespeople data set, let’s take a random sample of 100 values of customer_rate.\n\n# set seed for reproducibility of sampling\nset.seed(123)\n\n# generate a sample of 100 observations\ncustrate &lt;- na.omit(salespeople$customer_rate)\nn &lt;- 100\nsample_custrate &lt;- sample(custrate, n)\n\nWe can calculate the mean of the sample and the standard error of the mean.\n\n# mean\n(sample_mean &lt;- mean(sample_custrate))\n\n[1] 3.6485\n\n# standard error\n(se &lt;- sd(sample_custrate)/sqrt(n))\n\n[1] 0.08494328\n\n\nBecause the normal distribution is a frequency (or probability) distribution, we can interpret the standard error as a fundamental unit of ‘sensitivity’ around the sample mean. For greater multiples of standard errors around the sample mean, we can have greater certainty that the range contains the true population mean.\nTo calculate how many standard errors we would need around the sample mean to have a 95% probability of including the true population mean, we need to use the \\(t\\)-distribution. The \\(t\\)-distribution is essentially an approximation of the normal distribution acknowledging that we only have a sample estimate of the true population standard deviation in how we calculate the standard error. In this case where we are dealing with a single sample mean, we use the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can use the qt() function in R to find the standard error multiple associated with the level of certainty we need. In this case, we are looking for our true population mean to be outside the top 2.5% or bottom 2.5% of the distribution2.\n\n# get se multiple for 0.975\n(t &lt;- qt(p = 0.975, df = n - 1))\n\n[1] 1.984217\n\n\nWe see that approximately 1.98 standard errors on either side of our sample mean will give us 95% confidence that our range contains the true population mean. This is called the 95% confidence interval3.\n\n# 95% confidence interval lower and upper bounds\nlower_bound &lt;- sample_mean - t*se\nupper_bound &lt;- sample_mean + t*se\n\ncat(paste0('[', lower_bound, ', ', upper_bound, ']')) \n\n[3.47995410046738, 3.81704589953262]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#hyp-tests",
    "href": "quarto/03-primer_stats.html#hyp-tests",
    "title": "3  Statistics Foundations",
    "section": "3.3 Hypothesis testing",
    "text": "3.3 Hypothesis testing\nObservations about the distribution of statistics on samples of random variables allow us to construct tests for hypotheses of difference or similarity. Such hypothesis testing is useful in itself for simple bivariate analysis in practice settings, but it will be particularly critical in later chapters in determining whether models are useful or not. Before we go through some technical examples of hypothesis testing, let’s overview the logic and intuition for how hypothesis testing works.\nThe purpose of hypothesis testing is to establish a high degree of statistical certainty regarding a claim of difference in a population based on the properties of a sample. Consistent with a high burden of proof, we start from the hypothesis that there is no difference, called the null hypothesis. We only reject the null hypothesis if the statistical properties of the sample data render it very unlikely, in which case we confirm the alternative hypothesis that a statistical difference does exist in the population.\nMost hypothesis tests can return a p-value, which is the maximum probability of finding the sample results (or results that are more extreme or unusual than the sample results) when the null hypothesis is true for the population. The analyst must decide on the level of p-value needed to reject the null hypothesis. This threshold is referred to as the significance level \\(\\alpha\\) (alpha). A common standard is to set \\(\\alpha\\) at 0.05. That is, we reject the null hypothesis if the p-value that we find for our sample results is less than 0.05. If we reject the null hypothesis at \\(\\alpha = 0.05\\), this means that the results we observe in the sample are so extreme or unusual that they would only occur by chance at most 1 in 20 times if the null hypothesis were true. An alpha of 0.05 is not the only standard of certainty used in research and practice, and in some fields of study smaller alphas are the norm, particularly if erroneous conclusions might have very serious consequences.\nThree of the most common types of hypothesis tests are4:\n\nTesting for a difference in the means of two groups\nTesting for a non-zero correlation between two variables\nTesting for a difference in frequency distributions between different categories\n\nWe will go through an example of each of these. In each case, you will see a three-step process. First, we calculate a test statistic. Second, we determine an expected distribution for that test statistic. Finally, we determine where our calculated statistic falls in that distribution in order to assess the likelihood of our sample occurring if the null hypothesis is true. During these examples, we will go through all the logic and calculation steps needed to do the hypothesis testing, before we demonstrate the simple functions that perform all the steps for you in R. Readers don’t absolutely need to know all the details contained in this section, but a strong understanding of the underlying methods is encouraged.\n\n3.3.1 Testing for a difference in means (Welch’s \\(t\\)-test)\nImagine that we are asked if, in general, the sales of low-performing salespeople are different from the sales of high-performing salespeople. This question refers to all salespeople, but we only have data for the sample in our salespeople data set. Let’s take two subsets of our data for those with a performance rating of 1 and those with a performance rating of 4, and calculate the difference in mean sales.\n\n# take two performance group samples \nperf1 &lt;- subset(salespeople, subset = performance == 1)\nperf4 &lt;- subset(salespeople, subset = performance == 4)\n\n# calculate the difference in mean sales\n(diff &lt;- mean(perf4$sales) - mean(perf1$sales))\n\n[1] 154.9742\n\n\nWe can see that those with a higher performance rating in our sample did generate higher mean sales than those with a lower performance rating. But these are just samples, and we are being asked to give a conclusion about the populations they are drawn from.\nLet’s take a null hypothesis that there is no difference in true mean sales between the two performance groups that these samples are drawn from. We combine the two samples and calculate the distribution around the difference in means. To reject the null hypothesis at \\(\\alpha = 0.05\\), we would need to determine that the 95% confidence interval of this distribution does not contain zero.\nWe calculate the standard error of the combined sample using the formula5:\n\\[\n\\sqrt{\\frac{\\sigma_{\\mathrm{perf1}}^2}{n_{\\mathrm{perf1}}} + \\frac{\\sigma_{\\mathrm{perf4}}^2}{n_{\\mathrm{perf4}}}}\n\\] where \\(\\sigma_{\\mathrm{perf1}}\\) and \\(\\sigma_{\\mathrm{perf4}}\\) are the standard deviations of the two samples and \\(n_{\\mathrm{perf1}}\\) and \\(n_{\\mathrm{perf4}}\\) are the two sample sizes.\nWe use a special formula called the Welch-Satterthwaite approximation to calculate the degrees of freedom for the two samples, which in this case calculates to 100.986. This allows us to construct a 95% confidence interval for the difference between the means, and we can test whether this contains zero.\n\n# calculate standard error of the two sets\nse &lt;- sqrt(sd(perf1$sales)^2/length(perf1$sales) \n           + sd(perf4$sales)^2/length(perf4$sales))\n\n# calculate the required t-statistic\nt &lt;- qt(p = 0.975, df = 100.98)\n\n# calculate 95% confidence interval\n(lower_bound &lt;- diff - t*se)\n\n[1] 88.56763\n\n(upper_bound &lt;- diff + t*se)\n\n[1] 221.3809\n\n# test if zero is inside this interval\n(0 &lt;= upper_bound) & (0 &gt;= lower_bound)\n\n[1] FALSE\n\n\nSince this has returned FALSE, we conclude that a mean difference of zero is outside the 95% confidence interval of our sample mean difference, and so we cannot have 95% certainty that the difference in population means is zero. We reject the null hypothesis that the mean sales of both performance levels are the same.\nLooking at this graphically, we are assuming a \\(t\\)-distribution of the mean difference, and we are determining where zero sits in that distribution, as in Figure 3.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: \\(t\\)-distribution of the mean sales difference between perf1 and perf4, 95% confidence intervals (red dashed lines) and a zero difference (blue dot-dash line)\n\n\n\nThe red dashed lines in this diagram represent the 95% confidence interval around the mean difference of our two samples. The ‘tails’ of the curve outside of these two lines each represent a maximum of 0.025 probability for the true population mean. So we can see that the position of the blue dot-dashed line can correspond to a maximum probability that the population mean difference is zero. This is the p-value of the hypothesis test7.\nThe p-value can be derived by calculating the standard error multiple associated with zero in the \\(t\\)-distribution (called the \\(t\\)-statistic or \\(t\\)-value), by applying the conversion function pt() to obtain the upper tail probability and then multiplying by 2 to get the probability associated with both tails of the distribution.\n\n# get t-statistic\nt_actual &lt;- diff/se \n\n# convert t-statistic to p-value\n2*pt(t_actual, df = 100.98, lower = FALSE)\n\n[1] 1.093212e-05\n\n\nNowadays, it is never necessary to do these manual calculations ourselves because hypothesis tests are a standard part of statistical software. In R, the t.test() function performs a hypothesis test of difference in means of two samples and confirms our manually calculated p-value and 95% confidence interval.\n\nt.test(perf4$sales, perf1$sales)\n\n\n    Welch Two Sample t-test\n\ndata:  perf4$sales and perf1$sales\nt = 4.6295, df = 100.98, p-value = 1.093e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  88.5676 221.3809\nsample estimates:\nmean of x mean of y \n 619.8909  464.9167 \n\n\nBecause our p-value is less than our alpha of 0.05, we reject the null hypothesis in favor of the alternative hypothesis. The standard \\(\\alpha = 0.05\\) is associated with the term statistically significant. Therefore we could say here that the two performance groups have a statistically significant difference in mean sales.\nIn practice, there are numerous alphas that are of interest to analysts, each reflecting different levels of certainty. While 0.05 is the most common standard in many disciplines, more stringent alphas of 0.01 and 0.001 are often used in situations where a high degree of certainty is desirable (for example, some medical fields). Similarly, a less stringent alpha standard of 0.1 can be of interest particularly when sample sizes are small and the analyst is satisfied with ‘indications’ from the data. In many statistical software packages, including those that we will see in this book, tests that meet an \\(\\alpha = 0.1\\) standard are usually marked with period(.), those that meet \\(\\alpha = 0.05\\) with an asterisk(*), \\(\\alpha = 0.01\\) a double asterisk(**) and \\(\\alpha = 0.001\\) a triple asterisk(***).\nMany leading statisticians have argued that p-values are more a test of sample size than anything else and have cautioned against too much of a focus on p-values in making statistical conclusions from data. In particular, situations where data and methodology have been deliberately manipulated to achieve certain alpha standards—a process known as ‘p-hacking’—has been of increasing concern recently. See Chapter @ref(power-tests) for a better understanding of how the significance level and the sample size contribute to determining statistical power in hypothesis testing.\n\n\n3.3.2 Testing for a non-zero correlation between two variables (\\(t\\)-test for correlation)\nImagine that we are given a sample of data for two variables and we are asked if the variables are correlated in the overall population. We can take a null hypothesis that the variables are not correlated, determine a t-statistic associated with a zero correlation and convert this to a p-value. The t-statistic associated with a correlation \\(r\\) between two samples of length \\(n\\) is often notated \\(t^*\\) and is defined as\n\\[\nt^* = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\] \\(t^*\\) can be converted to an associated p-value using a \\(t\\)-distribution in a similar way to the previous section, this time with \\(n - 2\\) degrees of freedom in our \\(t\\)-distribution. As an example, let’s calculate \\(t^*\\) for the correlation between sales and customer rating in our sample and convert it to a p-value.\n\n# remove NAs from salespeople\nsalespeople &lt;- salespeople[complete.cases(salespeople), ]\n\n# calculate t_star\nr &lt;- cor(salespeople$sales, salespeople$customer_rate)\nn &lt;- nrow(salespeople)\nt_star &lt;- (r*sqrt(n - 2))/sqrt(1 - r^2)\n\n# convert to p-value on t-distribution with n - 2 degrees of freedom\n2*pt(t_star, df = n - 2, lower = FALSE)\n\n[1] 8.647952e-11\n\n\nAgain, there is a useful function in R to cut out the need for all our manual calculations. The cor.test() function in R performs a hypothesis test on the null hypothesis that two variables have zero correlation.\n\ncor.test(salespeople$sales, salespeople$customer_rate)\n\n\n    Pearson's product-moment correlation\n\ndata:  salespeople$sales and salespeople$customer_rate\nt = 6.6952, df = 348, p-value = 8.648e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2415282 0.4274964\nsample estimates:\n     cor \n0.337805 \n\n\nThis confirms our manual calculations, and we see the null hypothesis has been rejected and we can conclude that there is a significant correlation between sales and customer rating.\n\n\n3.3.3 Testing for a difference in frequency distribution between different categories in a data set (Chi-square test)\nImagine that we are asked if the performance category of each person in the salespeople data set has a relationship with their promotion likelihood. We will test the null hypothesis that there is no difference in the distribution of promoted versus not promoted across the four performance categories.\nFirst we can produce a contingency table, which is a matrix containing counts of how many people were promoted or not promoted in each category.\n\n# create contingency table of promoted vs performance\n(contingency &lt;- table(salespeople$promoted, salespeople$performance))\n\n   \n     1  2  3  4\n  0 50 85 77 25\n  1 10 25 48 30\n\n\nWe can see by summing each row that for the total sample we can expect 113 people to be promoted and 237 to miss out on promotion. We can use this ratio to compute an expected proportion in each performance category under the assumption that the distribution was exactly the same across all four categories.\n\n# calculate expected promoted and not promoted\n(expected_promoted &lt;- (sum(contingency[2, ])/sum(contingency)) * \n   colSums(contingency))\n\n       1        2        3        4 \n19.37143 35.51429 40.35714 17.75714 \n\n(expected_notpromoted &lt;- (sum(contingency[1, ])/sum(contingency)) * \n    colSums(contingency))\n\n       1        2        3        4 \n40.62857 74.48571 84.64286 37.24286 \n\n\nNow we can compare our observed versus expected values using the difference metric:\n\\[\n\\frac{(\\mathrm{observed} - \\mathrm{expected})^2}{\\mathrm{expected}}\n\\] and add these all up to get a total, known as the \\(\\chi^2\\) statistic.\n\n# calculate the difference metrics for promoted and not promoted\npromoted &lt;- sum((expected_promoted - contingency[2, ])^2/\n                  expected_promoted)\n\nnotpromoted &lt;- sum((expected_notpromoted - contingency[1, ])^2/\n                     expected_notpromoted)\n\n# calculate chi-squared statistic\n(chi_sq_stat &lt;- notpromoted + promoted)\n\n[1] 25.89541\n\n\nThe \\(\\chi^2\\) statistic has an expected distribution that can be used to determine the p-value associated with this statistic. As with the \\(t\\)-distribution, the \\(\\chi^2\\)-distribution depends on the degrees of freedom. This is calculated by subtracting one from the number of rows and from the number of columns in the contingency table and multiplying them together. In this case we have 2 rows and 4 columns, which calculates to 3 degrees of freedom. Armed with our \\(\\chi^2\\) statistic and our degrees of freedom, we can now calculate the p-value for the hypothesis test using the pchisq() function.\n\n# calculate p-value from chi_squared stat\npchisq(chi_sq_stat, df = 3, lower.tail=FALSE)\n\n[1] 1.003063e-05\n\n\nThe chisq.test() function in R performs all the steps involved in a chi-square test of independence on a contingency table and returns the \\(\\chi^2\\) statistic and associated p-value for the null hypothesis, in this case confirming our manual calculations.\n\nchisq.test(contingency)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency\nX-squared = 25.895, df = 3, p-value = 1.003e-05\n\n\nAgain, we can reject the null hypothesis and confirm the alternative hypothesis that there is a difference in the distribution of promoted/not promoted individuals between the four performance categories.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#foundational-statistics-in-python",
    "href": "quarto/03-primer_stats.html#foundational-statistics-in-python",
    "title": "3  Statistics Foundations",
    "section": "3.4 Foundational statistics in Python",
    "text": "3.4 Foundational statistics in Python\nElementary descriptive statistics can be performed in Python using various packages. Descriptive statistics of numpy arrays are usually available as methods.\n\nimport pandas as pd\nimport numpy as np\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# mean sales\nmean_sales = salespeople.sales.mean()\nprint(mean_sales)\n\n527.0057142857142\n\n\n\n# sample variance\nvar_sales = salespeople.sales.var()\nprint(var_sales)\n\n34308.11458043389\n\n\n\n# sample standard deviation\nsd_sales = salespeople.sales.std()\nprint(sd_sales)\n\n185.2244977869663\n\n\nPopulation statistics can be obtained by setting the ddof parameter to zero.\n\n# population standard deviation\npopsd_sales = salespeople.sales.std(ddof = 0)\nprint(popsd_sales)\n\n184.9597020864771\n\n\nThe numpy covariance function produces a covariance matrix.\n\n# generate a sample covariance matrix between two variables\nsales_rate = salespeople[['sales', 'customer_rate']]\nsales_rate = sales_rate[~np.isnan(sales_rate)]\ncov = sales_rate.cov()\nprint(cov)\n\n                      sales  customer_rate\nsales          34308.114580      55.817691\ncustomer_rate     55.817691       0.795820\n\n\nSpecific covariances between variable pairs can be pulled out of the matrix.\n\n# pull out specific covariances\nprint(cov['sales']['customer_rate'])\n\n55.81769119934507\n\n\nSimilarly for Pearson correlation:\n\n# sample pearson correlation matrix\ncor = sales_rate.corr()\nprint(cor)\n\n                  sales  customer_rate\nsales          1.000000       0.337805\ncustomer_rate  0.337805       1.000000\n\n\nSpecific types of correlation coefficients can be accessed via the stats module of the scipy package.\n\nfrom scipy import stats\n\n# spearman's correlation\nstats.spearmanr(salespeople.sales, salespeople.performance, \nnan_policy='omit')\n\nSignificanceResult(statistic=np.float64(0.27354459847452534), pvalue=np.float64(2.0065434379079837e-07))\n\n\n\n# kendall's tau\nstats.kendalltau(salespeople.sales, salespeople.performance, \nnan_policy='omit')\n\nSignificanceResult(statistic=np.float64(0.20736088105812), pvalue=np.float64(2.7353258226376403e-07))\n\n\nCommon hypothesis testing tools are available in scipy.stats. Here is an example of how to perform Welch’s \\(t\\)-test on a difference in means of samples of unequal variance.\n\n# get sales for top and bottom performers\nperf1 = salespeople[salespeople.performance == 1].sales\nperf4 = salespeople[salespeople.performance == 4].sales\n\n# welch's t-test with unequal variance\nttest = stats.ttest_ind(perf4, perf1, equal_var=False)\nprint(ttest)\n\nTtestResult(statistic=np.float64(4.629477606844271), pvalue=np.float64(1.0932443461577037e-05), df=np.float64(100.9768911762055))\n\n\nAs seen above, hypothesis tests for non-zero correlation coefficients are performed automatically as part of scipy.stats correlation calculations.\n\n# calculate correlation and p-value \nsales = salespeople.sales[~np.isnan(salespeople.sales)]\n\ncust_rate = salespeople.customer_rate[\n  ~np.isnan(salespeople.customer_rate)\n]\n\ncor = stats.pearsonr(sales, cust_rate)\nprint(cor)\n\nPearsonRResult(statistic=np.float64(0.337805044858678), pvalue=np.float64(8.647952212092207e-11))\n\n\nFinally, a chi-square test of difference in frequency distribution can be performed on a contingency table as follows. The first value of the output is the \\(\\chi^2\\) statistic, and the second value is the p-value.\n\n# create contingency table for promoted versus performance\ncontingency = pd.crosstab(salespeople.promoted, salespeople.performance)\n\n# perform chi-square test\nchi2_test = stats.chi2_contingency(contingency)\nprint(chi2_test)\n\nChi2ContingencyResult(statistic=np.float64(25.895405268094862), pvalue=np.float64(1.0030629464566802e-05), dof=3, expected_freq=array([[40.62857143, 74.48571429, 84.64285714, 37.24285714],\n       [19.37142857, 35.51428571, 40.35714286, 17.75714286]]))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#learning-exercises",
    "href": "quarto/03-primer_stats.html#learning-exercises",
    "title": "3  Statistics Foundations",
    "section": "3.5 Learning exercises",
    "text": "3.5 Learning exercises\n\n3.5.1 Discussion questions\nWhere relevant in these discussion exercises, let \\(x = x_1, x_2, \\dots, x_n\\) and \\(y = y_1, y_2, \\dots, y_m\\) be samples of two random variables of length \\(n\\) and \\(m\\) respectively.\n\nIf the values of \\(x\\) can only take the form 0 or 1, and if their mean is 0.25, how many of the values equal 0?\nIf \\(m = n\\) and \\(x + y\\) is formed from the element-wise sum of \\(x\\) and \\(y\\), show that the mean of \\(x + y\\) is equal to the sum of the mean of \\(x\\) and the mean of \\(y\\).\nFor a scalar multiplier \\(a\\), show that \\(\\mathrm{Var}(ax) = a^2\\mathrm{Var}(x)\\).\nExplain why the standard deviation of \\(x\\) is a more intuitive measure of the deviation in \\(x\\) than the variance.\nDescribe which two types of correlation you could use if \\(x\\) is an ordered ranking.\nDescribe the role of sample size and sampling frequency in the distribution of sampling means for a random variable.\nDescribe what a standard error of a statistic is and how it can be used to determine a confidence interval for the true population statistic.\nIf we conduct a t-test on the null hypothesis that \\(x\\) and \\(y\\) are drawn from populations with the same mean, describe what a p-value of 0.01 means.\nExtension: The sum of variance law states that, for independent random variables \\(x\\) and \\(y\\), \\(\\mathrm{Var}(x \\pm y) = \\mathrm{Var}(x) + \\mathrm{Var}(y)\\). Use this together with the identity from Exercise 3 to derive the formula for the standard error of the mean of \\(x = x_1, x_2, \\dots, x_n\\):\n\n\\[\nSE = \\frac{\\sigma(x)}{\\sqrt{n}}\n\\]\n\nExtension: In a similar way to Exercise 9, show that the standard error for the difference between the means of \\(x\\) and \\(y\\) is\n\n\\[\n\\sqrt{\\frac{\\sigma(x)^2}{n} + \\frac{\\sigma(y)^2}{m}}\n\\]\n\n\n3.5.2 Data exercises\nFor these exercises, load the charity_donation data set via the peopleanalyticsdata package, or download it from the internet8. This data set contains information on a sample of individuals who made donations to a nature charity.\n\nCalculate the mean total_donations from the data set.\nCalculate the sample variance for total_donation and convert this to a population variance.\nCalculate the sample standard deviation for total_donations and verify that it is the same as the square root of the sample variance.\nCalculate the sample correlation between total_donations and time_donating. By using an appropriate hypothesis test, determine if these two variables are independent in the overall population.\nCalculate the mean and the standard error of the mean for the first 20 entries of total_donations.\nCalculate the mean and the standard error of the mean for the first 50 entries of total_donations. Verify that the standard error is less than in Exercise 5.\nBy using an appropriate hypothesis test, determine if the mean age of those who made a recent donation is different from those who did not.\nBy using an appropriate hypothesis test, determine if there is a difference in whether or not a recent donation was made according to where people reside.\nExtension: By using an appropriate hypothesis test, determine if the age of those who have recently donated is at least 10 years older than those who have not recently donated in the population.\nExtension: By using an appropriate hypothesis test, determine if the average donation amount is at least 10 dollars higher for those who recently donated versus those who did not. Retest for 20 dollars higher.\n\n\n\n\n\nBhattacharya, P. K., and Prabir Burman. 2016. Theory and Methods of Statistics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  },
  {
    "objectID": "quarto/03-primer_stats.html#footnotes",
    "href": "quarto/03-primer_stats.html#footnotes",
    "title": "3  Statistics Foundations",
    "section": "",
    "text": "Note that this formula assumes that the sample standard deviation is a close approximation of the population standard deviation, which is generally fine for samples that are not very small.↩︎\nAs sample sizes increase and sample statistics get very close to population statistics, whether we use a \\(t\\)-distribution or a \\(z\\)-distribution (normal distribution) for determining confidence intervals or p-values becomes less important as they become almost identical on large samples. The output of some later models will refer to \\(t\\)-statistics and others to \\(z\\)-statistics, but the difference is only likely to matter in small samples of less than 50 or so observations. In this chapter we will use the \\(t\\)-distribution as it is a better choice for all sample sizes.↩︎\nOften we can use a rough estimate for larger samples that the 95% confidence interval is 2 standard errors either side of the sample mean.↩︎\nWe go through these three examples both because they are relatively common and to illustrate the details of the logic behind hypothesis testing. By understanding how hypothesis tests work, this will allow the reader to grasp the meaning of other such tests like the F-test or the Wald test, which we will refer to in later chapters of this book↩︎\nIf you are inquisitive about this formula, see the exercises at the end of this chapter.↩︎\nI’ve kept the gory details of how this is derived out of view, but you can see them if you view the source code for this book.↩︎\nWe call this type of hypothesis test a two-tailed test, because the tested population mean can be either higher or lower than the sample mean, thus it can appear in any of the two tails for the null hypothesis to be rejected. One-tailed tests are used when you are testing for an alternative hypothesis that the difference is specifically ‘less than zero’ or ‘greater than zero’‍. In the t.test() function in R, you can specify this in the arguments.↩︎\nhttp://peopleanalytics-regression-book.org/data/charity_donation.csv↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Foundations</span>"
    ]
  }
]