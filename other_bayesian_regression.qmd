# Fitting Other Regression Models Using Bayesian Inference

```{r}
#| echo: false
#| eval: false
# models and other large computational objects are loaded from saved objects
# model fitting code chunks are set to eval: false to avoid long computation times
# during rendering 
# to create the models from scratch, run the code chunks in your R/Python session
# and save the objects to a 'models/other' folder as .RDS or .pkl files

# utility function for saving models in R
save_model_to_other <- function(model, filename) {
  # check the 'models/other' directory exists, if not create it
  if (!dir.exists("models/other")) {
    dir.create("models/other", recursive = TRUE)
  }
  # save to file within the 'models/other' directory
  saveRDS(model, file = paste0("models/other/", filename))
}
```

```{python}
#| echo: false
#| eval: false
# utility function for saving models in Python
import os
import pickle

def save_model_to_other(model, filename):
    # check the 'models/other' directory exists, if not create it
    if not os.path.exists("models/other"):
        os.makedirs("models/other")
    # save to file within the 'models/other' directory
    with open(os.path.join("models/other", filename), 'wb') as f:
        pickle.dump(model, f)
```


In the earlier chapters of this book, we dedicated significant attention to each variety of regression modeling technique, explaining the specifics of formulation, estimation, and interpretation. Moving forward, we should not need to do this for the Bayesian versions of these techniques.  Given that we understand the underlying frequentist models from our earlier chapters, and now that we have a solid grasp of the principles and mechanics of Bayesian inference from the last two chapters, we can now focus on how to implement these models in a Bayesian framework without delving too deeply into the foundational details again.

In this chapter, we will explore how to fit several other types of regression models using Bayesian methods. We will mainly focus on the mechanics of how to implement the models, and touch on any matters specific to the model type that are relevant in a Bayesian context. We will start with binomial logistic regression, and then move on to the other common logistic regression models, Poisson regression, negative binomial regression and finally we will touch on mixed effects models and Cox proportional hazards models. For each model type, we will provide code examples using the `brms` package in R which, as we have seen, allows for flexible Bayesian modeling using `stan` as the backend.

Before we proceed, let's review a few technical tips to help ensure good performance from `brms` models in general, and to reduce the amount of time needed for these models to run.  Readers may find the following helpful to avoid long wait times or crashes when running models using `brms`:

* In general, computational resources with more CPUs (cores) and more memory (RAM) will lead to faster model fitting times, and will also reduce the chances of your models crashing due to insufficient memory.  To run some of the models in this chapter, a minimum of 8GB of RAM is needed, with 16GB or more recommended.

* Using more cores is helpful.  If you are not sure how many cores you have, you can use the function `parallel::detectCores()` to find out.  You can then specify the number of cores to use in `brms` using the `options(mc.cores = <number_of_cores>)` command before fitting your model.  You can also specify how many cores in the `brm()` function using the `cores` argument.  Note that using more cores will not always lead to a linear speedup in model fitting time, as there is some overhead in managing multiple chains across multiple cores.

* The number of iterations you will need to ensure convergence will vary by model and data.  In the examples in this chapter we use 10,000 iterations, which is a generous number for most models.  

* Your choice of backend for `brms` can have a significant impact on model fitting time.  The backend is the R package which 'translates' your specified model into instructions that `stan` will undersand.  The default backend is `rstan`, which is generally reliable for most models but not very fast.  Using the `cmdstanr` backend can lead to faster fitting times.  To use `cmdstanr`, you will need to install the `cmdstanr` package and set it up according to the instructions in the package documentation.  Once set up, you can specify the backend in `brms` using the `backend = "cmdstanr"` argument in the `brm()` function or by setting the option `options(brms.backend = "cmdstanr")` before fitting your model.

In preparation for fitting a variety of models in this chapter, we will set some default options for `brms` to save having to specify these in every model run.

```{r}
# set default options for brms
library(brms)
options(
  brms.iter = 10000, # number of iterations
  brms.chains = 4,   # number of chains
  brms.refresh = 0,  # suppress output during fitting
  brms.seed = 123,    # set seed for reproducibility
  brms.save_pars = save_pars('all') # save all parameters
)
```

## Bayesian Binomial Logistic Regression {#sec-bayes-bin-log-reg}

As we saw in @sec-bin-log-reg, binomial logistic regression is used when each observation of our outcome is a Bernoulli random variable that can take a value of 1 (e.g., 'success') with probability $p_i$ and a value of 0 (e.g., 'failure') with probability $1-p_i$.  That is 

$$
y_i \sim \text{Bernoulli}(p_i)
$$

Recall also that we model the log-odds of success as a linear function of our input variables, that is:

$$
\ln\left(\frac{P(y_i = 1)}{P(y_i = 0)}\right) =  \ln\left(\frac{p_i}{1-p_i}\right) =  \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ip}
$$

This means that our slope coefficients must be interpreted in terms of changes in the log-odds of success for a one unit increase in the corresponding input variable, assuming no change in the other input variables.  We learned that by exponentiating the coefficients, we can interpret them in terms of odds ratios, which represents the multiple of the odds of the outcome variable associated with a unit change in the input variable, assuming no change in the other input variables.

In a Bayesian framework, we can fit a binomial logistic regression model using the `brms` package in a similar way to how we fit a linear regression model. The syntax is similar to that of frequentist logistic regression, but we specify the family as `bernoulli()` to explicitly indicate that each observation of our outcome is a Bernoulli random variable.

To illustrate, let's use a random sample from our `salespeople` dataset from @sec-bin-log-reg. 

```{r}
# Load the salespeople dataset
url <- "https://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople <- read.csv(url)

# remove rows with NAs
salespeople <- salespeople[complete.cases(salespeople), ]

# Take a random sample of 100 observations
set.seed(123)
salespeople_bayes <- salespeople[sample(nrow(salespeople), 100), ]
```

We are interested in how the sales, customer ratings and performance ratings of our 100 sales people affect their likelihood of promotion.   We will fit a Bayesian binomial logistic regression model to this subset of data using the default flat priors.

```{r}
#| label: bayes-binomial-logistic
#| eval: false
# Fit Bayesian Binomial Logistic Regression model
library(brms)

uninf_binomial_model <- brm(
  formula = promoted ~ sales + customer_rate + performance,
  data = salespeople_bayes,
  family = bernoulli() # for binary outcome variable (logistic function is default link)
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_binomial_model <- readRDS("models/other/uninf_binomial_model.RDS")
```


After fitting the model, we can examine the summary of the posterior distributions of the coefficients, which will represent log-odds coefficients.


```{r}
summary(uninf_binomial_model)
```

And we can transform these to posterior distributions of the odds ratios.


```{r}
# convert log-odds to odds ratios
(odds_ratio_summary <- fixef(uninf_binomial_model) |> 
   exp())
```

We can also visualize the posterior distributions of the odds ratios using the `mcmc_areas()` function from the `bayesplot` package.  An example for the `sales` coefficient is shown in @fig-sales-or, indicating the posterior distribution of the multiple of the odds of promotion associated with an extra thousand dollars in sales.

:::{#fig-sales-or}

```{r}
library(bayesplot)
library(ggplot2)

mcmc_areas(
  exp(as.matrix(uninf_binomial_model, pars = c("b_sales"))),
  prob = 0.66, 
  prob_outer = 0.95
) + 
  theme_minimal()

```

Posterior distribution of the odds ratio for the `sales` coefficient in the Bayesian Binomial Logistic Regression model.
:::

Posterior predictive checks on a Bernoulli distributed outcome are not particularly intuitive unless converted into a summary statistic, such as the mean value (which is the proportion of successes).  We can do this using the `pp_check()` function, but specifying the `stat` argument to indicate that we want to compare the mean in the observed data to that in the posterior predictive simulations.   As can be seen in @fig-binom-ppc, the model appears to fit the data reasonably well.

:::{#fig-binom-ppc}

```{r}
pp_check(uninf_binomial_model, type = "stat", ndraws = 1000, binwidth = 0.01) +
  theme_minimal()
```

Posterior predictive check comparing the proportion of successes in the observed data (black line) to that in the posterior predictive simulations for the Bayesian Binomial Logistic Regression model (histogram).
:::

For a new data point, we can obtain the posterior predictive probability of success using the `posterior_predict()` function, which runs draws of our posterior parameters and predicts a success or failure for each draw based on our new data.  For example, for a sales person with $575k in sales, an average customer rating of 3.9 and a performance rating of 3, we can determine the posterior predictive probability of promotion as follows:


```{r}
# determine posterior binary predictions new data point
new_data <- data.frame(sales = 575, customer_rate = 3.9, performance = 3)
posterior_preds <- posterior_predict(uninf_binomial_model, newdata = new_data, 
                                     ndraws = 10000, seed = 123)

# Convert to probability by taking mean
mean(posterior_preds)
```

## Bayesian Multinomial Logistic Regression

In @sec-multinom, we learned that multinomial logistic regression is used when the outcome variable is categorical with more than two categories that do not have a natural ordering.  In this case, we model the log-odds of each category relative to a reference category as a linear function of our input variables.  In a Bayesian framework, we can fit a multinomial logistic regression model using the `brms` package by specifying the family as `categorical()`. As usual we will take a sample from our `health_insurance` dataset from @sec-multinom to illustrate.


```{r}
#| label: bayes-multinomial-logistic
#| eval: false
# Load the health insurance dataset
url <- "https://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance <- read.csv(url)

# take a sample of 100 observations
set.seed(123)
health_insurance_bayes <- health_insurance[sample(nrow(health_insurance), 100), ]

# Fit Bayesian Multinomial Logistic Regression model with flat priors
uninf_multinom_model <- brm(
  formula = product ~ age + household + position_level + gender + absent,
  data = health_insurance_bayes,
  family = categorical() # for multinomial outcome variable
)
```

```{r}
#| echo: false
url <- "https://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance <- read.csv(url)

# take a sample of 100 observations
set.seed(123)
health_insurance_bayes <- health_insurance[sample(nrow(health_insurance), 100), ]

# load the model from saved file
uninf_multinom_model <- readRDS("models/other/uninf_multinom_model.RDS")
```

Now we can view our summary.

```{r}
summary(uninf_multinom_model)
```

Here we can see two sets of posterior coefficient distributions, one giving the log odds of a choice of product B over product A (beginning `muB`), and the other giving the log odds of a choice of product C over product A (beginning `muC`).  As before, we can exponentiate these coefficients to obtain odds ratios.

Our posterior predictions will be in the form of integers representing the three product categories (by default this will be A=1, B=2, C=3).  To do a posterior predictive check, we can use the `pp_bars()` function to view the proportion of each product choice in the observed data compared to that in the posterior predictive simulations, as shown in @fig-multinom-ppc.

:::{#fig-multinom-ppc}

```{r}
# convert observations to numeric
y_observed <- as.numeric(as.factor(health_insurance_bayes$product))

# get 1000 posterior predictions
y_sim <- posterior_predict(uninf_multinom_model, ndraws = 1000, seed = 123)

# posterior predictive check showing proportions for each product
ppc_bars(y_observed, y_sim) +
  scale_x_continuous(breaks = c(1, 2, 3), labels = c("A", "B", "C")) +
  theme_minimal()
```

Posterior predictive check comparing the proportions of each product choice in the observed data (light blue bars) to those in the posterior predictions (dark lines) for the Bayesian Multinomial Logistic Regression model.
:::

We see that the model fits the data very well, with the median predictions closely matching the observed data.

Given new data for an individual, we can calculate the probability of the choice of each product using our posterior draws.

```{r}
# new data point
new_data <- data.frame(
  age = 35,
  household = 3,
  position_level = 3,
  gender = "Male",
  absent = 2
)

# posterior predictions for new data point
posterior_preds <- posterior_predict(uninf_multinom_model, newdata = new_data, 
                                     ndraws = 10000, seed = 123) |> 
  factor(labels = c("A", "B", "C"))

# convert to probabilities of each product choice
(product_probs <- prop.table(table(posterior_preds)))

```

Since this model sees our first appearance of categorical input variables in the Bayesian modeling framework, it is worth making a note that `brms` will automatically create dummies for these variables, just as we saw in the `lm()` and `glm()` functions for frequentist regression models. If we wish to set an informed prior for the coefficient of a specific categorical input variable, we will need to specify the dummy variable associated with our prior.  To determine the correct way to specify the prior, we can first look at all our priors in the model:

```{r}
get_prior(uninf_multinom_model)
```

We can use this syntax to specify our informed prior.  As an example, if we had information that being male is associated with a 20% increase in the odds of choosing product B over product A, we would set a prior for the `b` coefficient of `genderMale` in the `muB` set of parameters.  We would specify the mean on the log-odds scale as `log(1.2)`, and we might set a standard deviation of `log(0.1)` to reflect a strong belief about this effect.  This prior would be specified as follows:


```{r}
# set a prior for the effect of being male on the log-odds of choosing product B over product A
priors <- c(
  prior(normal(log(1.2), log(0.1)), class = "b", coef = "genderMale", dpar = "muB")
)
```

We could then refit the model with this prior specified using the `prior` argument in the `brm()` function.

### Bayesian Ordinal Logistic Regression

In @sec-ord-reg, we learned that ordinal logistic regression is used when the outcome variable is categorical with more than two categories that have a natural ordering.  In this case, we model the log-odds of being in a category less than or equal to a certain level as a linear function of our input variables.  In particular, we focused on the proportional odds model, which assumes that the effect of each input variable is the same across all thresholds of the outcome variable, allowing us to have a single coefficient for each input variable to indicate the strength of effect on the log-odds of being in a higher category in general.  We also learned that it is essential to check the proportional odds assumption when fitting a proportional odds logistic regression model, as violation of this assumption can lead to false inferences from the model.

In a Bayesian framework, we can fit a proportional odds logistic regression model using the `brms` package by specifying the family as `cumulative()`, representing a cumulative logistic model. As usual we will take a sample from our `soccer` dataset from @sec-ord-reg to illustrate.  We are aiming to estimate the effect of numerous factors on the discliplinary record of a player in a game - a three level ordinal outcome:  "None" for no discipline, "Yellow" for a formal warning and "Red" for a sending-off.  As with the `polr()` function for the frequentist proportional odds logistic regression model, `brms` will expect us to ensure our outcome variable is an ordinal factor.


```{r}
#| label: bayes-ordinal-logistic
#| eval: false
# load the soccer dataset
url <- "https://peopleanalytics-regression-book.org/data/soccer.csv"
soccer <- read.csv(url)

# take a sample of 100 observations
set.seed(123)
soccer_bayes <- soccer[sample(nrow(soccer), 100), ]

# convert discipline to an ordered factor
soccer_bayes$discipline <- ordered(
  soccer_bayes$discipline,
  levels = c("None", "Yellow", "Red")
)

# Fit Bayesian proportional odds Logistic Regression model with flat priors
uninf_polr_model <- brm(
  formula = discipline ~ .,
  data = soccer_bayes,
  family = cumulative() # for ordinal outcome variable
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_polr_model <- readRDS("models/other/uninf_polr_model.RDS")
```

Now we can view our summary.

```{r}
summary(uninf_polr_model)
```

Here we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log-odds of being in a higher discipline category.  

In a Bayesian regression framework, an effective way to test the proportional odds assumption is to run an alternative model that does not assume proportional odds, and then compare the two models using information criteria such as from Leave-One-Out Cross-Validation (LOO-CV) .  If the non-proportional odds model has a significantly better fit according to these criteria, it suggests that the proportional odds assumption may be violated.  A common alternative model for ordinal regression is an adjacent category model.   Here is how we can fit an adjacent category ordinal model in `brms`.


```{r}
#| label: bayes-adjacent-category-logistic
#| eval: false
# Fit Bayesian adjacent category Logistic Regression model with flat priors
uninf_acat_model <- brm(
  formula = discipline ~ .,
  data = soccer_bayes,
  family = acat() # for adjacent category ordinal outcome variable
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_acat_model <- readRDS("models/other/uninf_acat_model.RDS")
```

Now we can compare the fit using LOO:

```{r}
# compare LOO for both models
loo_polr <- loo(uninf_polr_model, seed = 123)
loo_acat <- loo(uninf_acat_model, seed = 123)
loo_compare(loo_polr, loo_acat)
```

The LOO comparison suggests that there is no meaningful difference between the models, so we can be reasonably confident that the proportional odds assumption holds in this case.


## Bayesian Poisson and Negative Binomial Regression

In @sec-count-reg, we learned that Poisson regression is used when the outcome variable is a count of events that occur in a fixed period of time or space.  We model the log of the expected count as a linear function of our input variables, to ensure that the count outcomes are always non-negative.  This means that when we exponentiate the coefficients of our model, we see that the effects of the input variables are multiplicative on the count outcome.  We also learned that Poisson regression assumes that the mean and variance of the outcome variable are equal, which may not hold in practice.  When the variance exceeds the mean, we have overdispersion, which can lead to underestimated standard errors and misleading inferences from the model.  In such cases, negative binomial regression can be used as an alternative, as it includes an additional shape parameter $\theta$ to account for overdispersion.

In a Bayesian framework, we can fit a Poisson regression model using the `brms` package by specifying the family as `poisson()`. As usual we will take a sample from our `absenteeism` dataset from @sec-count-reg to illustrate.

```{r}
#| label: bayes-poisson
#| eval: false
# Load the absenteeism dataset
url <- "https://peopleanalytics-regression-book.org/data/absenteeism.csv"
absenteeism <- read.csv(url)

# take a sample of 100 observations
set.seed(123)
absenteeism_bayes <- absenteeism[sample(nrow(absenteeism), 100), ]

# Fit Bayesian Poisson Regression model with flat priors
uninf_poisson_model <- brm(
  formula = days_absent ~ tenure + is_manager + performance_rating,
  data = absenteeism_bayes,
  family = poisson() # for count outcome variable
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_poisson_model <- readRDS("models/other/uninf_poisson_model.RDS")
```

We can now inspect the model summary.

```{r}
summary(uninf_poisson_model)
```

Here we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log of the expected count of days absent.  By exponentiating these coefficient distributions, we can interpret them as multiplicative effects on the expected count.

We can do a posterior predictive check in the usual way as in @fig-poisson-pp.

:::{#fig-poisson-pp}

```{r}
pp_check(uninf_poisson_model, ndraws = 1000) +
  theme_minimal()
```
Posterior predictive check for the Bayesian Poisson regression model.
:::

We can see some issues with our predictive fit.  This suggests that we may have overdispersion in our data.  We can address this by fitting a negative binomial regression model instead, which we can do in `brms` by specifying the family as `negbinomial()`.


```{r}
#| label: bayes-negative-binomial
#| eval: false
# Fit Bayesian Negative Binomial Regression model with flat priors
uninf_negbin_model <- brm(
  formula = days_absent ~ tenure + is_manager + performance_rating,
  data = absenteeism_bayes,
  family = negbinomial() # for count outcome variable with overdispersion
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_negbin_model <- readRDS("models/other/uninf_negbin_model.RDS")
```

We can now inspect the model summary.

```{r}
summary(uninf_negbin_model)
```

We can see that our estimated errors have increased, leading to more conservative inferences, and we see a posterior distribution for the shape parameter $\theta$, with the low values indicating substantial overdispersion.  We can do another posterior predictive check to verify if the model fits the data better as in @fig-negbin-pp.

:::{#fig-negbin-pp}

```{r}
pp_check(uninf_negbin_model, ndraws = 1000) +
  xlim(0, 50) +
  theme_minimal()
```

Posterior predictive check for the Bayesian negative binomial regression model.
:::

Finally, we can also use LOO-CV to formally compare the fit of the two models, which establishes a substantially superior fit for the negative binomial model.


```{r}
# compare LOO for both models
loo_poisson <- loo(uninf_poisson_model, seed = 123)
loo_negbin <- loo(uninf_negbin_model, seed = 123)
loo_compare(loo_poisson, loo_negbin)
```

Note that zero-inflated versions of both Poisson and negative binomial regression models can also be fit in `brms` by specifying the families as `zero_inflated_poisson()` and `zero_inflated_negbinomial()` respectively.  These models will generate the posterior of an additional zero-inflation parameter `zi`, representing the probability that a zero observation is an "excess zero".

## Bayesian Mixed Effects Models

In @sec-hierarchical-data, we learned that mixed effects models (also known as multilevel or hierarchical models) are used when our data has a hierarchical structure, such as students nested within schools or employees nested within departments.  These models allow us to account for the non-independence of observations within groups by including random effects that capture the variability between groups.  In a Bayesian framework, we can fit mixed effects models using the `brms` package by specifying random effects in the model formula using the `(1 | group_variable)` syntax which we learned in @sec-hierarchical-data.  There is no need to specify a different family for mixed effects models in `bems`; we simply include the random effects in the formula and apply the appropriate model family for the outcome variable type. 

To illustrate, let's use a random sample from our `speed_dating` dataset from @sec-hierarchical-data and run a Bayesian binomial mixed effects model to understand how the various factors considered by individuals over multiple speed dates relate to the overall binary decision they made about their dates.


```{r}
#| label: bayes-mixed-binomial
#| eval: false
# Load the speed dating dataset
url <- "https://peopleanalytics-regression-book.org/data/speed_dating.csv"
speed_dating <- read.csv(url)

# remove rows with NAs
speed_dating <- speed_dating[complete.cases(speed_dating), ]

# take a sample of 1000 observations
set.seed(123)
speed_dating_bayes <- speed_dating[sample(nrow(speed_dating), 1000), ]

# Fit Bayesian Binomial Logistic Mixed Effects model with flat priors

uninf_mixed_binomial_model <- brm(
  formula = dec ~ agediff + samerace + attr + intel + prob + (1 | iid),
  data = speed_dating_bayes,
  family = bernoulli(), # for binary outcome variable
  save_pars = save_pars('all', group = TRUE) # save group-level parameters
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_mixed_binomial_model <- readRDS("models/other/uninf_mixed_binomial_model.RDS")
```

Now we can view our summary.

```{r}
summary(uninf_mixed_binomial_model)
```

Here we can see the posterior coefficient distributions for each fixed effect, representing the effect of a unit change in that variable on the log-odds of a positive decision about a date.  We can also see the estimated standard deviation of the random intercepts for each individual under the Multilevel Hyperparameters, which captures the variability in baseline log-odds of a positive decision across individuals. A posterior predictive check can be done in the same way as for a standard Bayesian binomial logistic regression model.

## Bayesian Cox Proportional Hazards Models

In @sec-survival, we learned that Cox proportional hazards models are used in survival analysis to model an event that occurs over time, such as employee turnover or customer churn.  These models estimate the hazard function, which represents the instantaneous risk of the event occurring at a given time, conditional on survival up to that time.  The Cox model assumes that the hazard ratios between individuals are proportional over time, allowing us to estimate the effect of input variables on the hazard function without specifying the baseline hazard function.

In a Bayesian framework, we can fit Cox proportional hazards models using the `brms` package by specifying the family as `cox()`.  If your data contains censored observations, the model formula should take the general form `time | cens(1 - event) ~ x1 + x2 + ...`, where `time` is the variable indicating the time of the observation and `event` is the binary variable indicating if the event had occurred.  As usual we will take a sample from our `job_retention` dataset from @sec-survival to illustrate.  

```{r}
#| label: bayes-cox-ph
#| eval: false
# Load the job retention dataset
url <- "https://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention <- read.csv(url)

# take a sample of 1000 observations
set.seed(123)
job_retention_bayes <- job_retention[sample(nrow(job_retention), 1000), ]

# Fit Bayesian Cox Proportional Hazards model with flat priors
uninf_cox_model <- brm(
  formula = month | cens(1 - left) ~ gender + 
    field + level + sentiment,
  data = job_retention_bayes,
  family = cox() # for survival outcome variable
)
```

```{r}
#| echo: false
# load the model from saved file
uninf_cox_model <- readRDS("models/other/uninf_cox_model.RDS")
```

We can now view the model summary.

```{r}
summary(uninf_cox_model)
```

Here we can see the posterior coefficient distributions for each input variable, representing the effect of a unit change in that variable on the log-odds of employee attrition, assuming no change in the other input variables.  By exponentiating these coefficient distributions, we can interpret them as adds ratios describing the multiple on the attrition hazard associated with a unit change in the input variable assuming no change in the other input variables.  

Note that it is not currently possible to do posterior predictive checks for Cox proportional hazards models in `brms`, due to the complexity of the survival outcome and censoring mechanism.  However, we can assess the fit of the model using other methods, such as examining the estimated survival curves for different groups or using LOO-CV to estimate the expected log predictive density (ELPD).

## Fitting other regression models using Python

To fit similar models using Python, we can use `bambi` for model specification and fitting with `arviz` for summarizing and visualizing posteriors.  The workflow is very similar to that which we described in @sec-py-linear-bayes, but specifying the appropriate model type in `bambi` using the `family` argument in the `Model()` constructor.  The following table gives the corresponding family names in `bambi` for various regression models we have discussed in this chapter:


| Regression Model                     | `bambi` Family Name          |
|-------------------------------------|------------------------------|
| Binomial Logistic Regression        | `Bernoulli`                  |
| Multinomial Logistic Regression     | `categorical`                |
| Ordinal Logistic Regression         | `cumulative`                 |
| Poisson Regression                  | `poisson`                    |
| Negative Binomial Regression        | `negativebinomial`           |

As an example, to fit our Bayesian binomial logistic regression model from @sec-bayes-bin-log-reg in this chapter using `bambi` in Python, first we specify a and build the model.

```{python}
#| label: py-bayes-binomial-logistic

import pandas as pd
import bambi as bmb
import arviz as az

# Load the salespeople dataset
url = "https://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople = pd.read_csv(url)

# remove rows with NAs
salespeople = salespeople.dropna()

# Take a random sample of 100 observations
salespeople_bayes = salespeople.sample(n=100, random_state=123)

# spwecify uninformed Bayesian Binomial Logistic Regression model
uninf_binomial_model = bmb.Model(
    formula="promoted ~ sales + customer_rate + performance",
    data=salespeople_bayes,
    family="bernoulli"  # for binary outcome variable
)

# build and view model
uninf_binomial_model.build()
print(uninf_binomial_model)
```
If we are comfortable with our priors, we proceed to fit the model:

```{python}
#| label: py-bayes-binomial-logistic-fit
#| eval: false
# fit the model
fitted = uninf_binomial_model.fit(draws=10000, chains=4, random_seed=123)
```

```{python}
#| echo: false
# load the model from saved file
import pickle
with open("models/other/uninf_binomial_model.pkl", 'rb') as f:
    fitted = pickle.load(f)
```

We can then summarize the posterior distributions of the coefficients using `arviz` as follows:

```{python}
# summarize the posterior distributions
az.summary(fitted)
```

We can plot the posterior distributions of the log-odds coefficients as in @fig-py-bayes-bin-posterior.

::: {#fig-py-bayes-bin-posterior}
```{python}
#| label: py-bayes-bin-posterior
#| results: 'hide'
# Plot posterior distributions
az.plot_posterior(fitted)
```

Posterior distributions of the coefficients for the Bayesian Binomial Logistic Regression model using `bambi` in Python.
:::

And the posterior predictive check can be performed as in @fig-py-bayes-bin-ppc.

::: {#fig-py-bayes-bin-ppc}

```{python}
#| label: py-bayes-bin-ppc
#| results: 'hide'
# perform posterior predictive check
uninf_binomial_model.predict(fitted, kind="response", random_seed=123)
az.plot_ppc(fitted, num_pp_samples=1000, random_seed=123)
```

Posterior predictive check comparing the proportion of successes in the observed data to that in the posterior predictive simulations for the Bayesian Binomial Logistic Regression model using `bambi` in Python.

:::

## Learning exercises

### Discussion questions

1. What is the correct family to specify in `brms` for fitting a Bayesian binomial logistic regression model with a binary outcome variable?

2. What statistic is most appropriate for a posterior predictive check for a Bayesian binomial logistic regression model?

3. What is the most appropriate way to do a posterior predictive check for a Bayesian multinomial logistic regression model?

4. What is the correct family to specify for fitting a Bayesian proportional odds logistic regression model in `brms`?

5. How can we test the proportional odds assumption for a Bayesian proportional odds logistic regression model using `brms`?

6. Describe some ways of diagnosing overdispersion when fitting a Bayesian Poisson regression model.

7. Describe how to set informed priors for categorical input variables in a Bayesian regression model in `brms`.

8. If you are modeling a censored survival outcome using a Bayesian Cox proportional hazards model in `brms`, how would you write the outcome variable in your model formula?

### Data exercises

For each of the earlier (frequentist) chapters on the methods outlined in this chapter, use the problem and dataset provided in the exercises at the end of each chapter.  

1.  Generate two smaller versions of the dataset by taking random samples of the rows - aim for around 10-20% of the original observations in each dataset. Don't forget to remove your first sample before taking the second sample.

2. Fit a Bayesian version of the appropriate regression model using the first dataset using flat priors.  

3. Visualize and examine the posterior distributions of the coefficients and interpret them in the context of the problem.

4. For each model, conduct appropriate posterior predictive checks to assess the fit of the model to the data.  Comment on the results.  Are you comfortable that the model is a good fit?

5.  If you are concerned about the fit, experiment with alternative model specifications to improve the fit.  Justify your choices and comment on the results.

6.  Ensure you check any underlying assumptions of your model type (eg proportional odds for ordinal logistic regression, equidispersion for Poisson regression) and take appropriate action if any assumptions are violated.

7.  Experiment with setting informed priors for one or two coefficients.  Refit the model with these priors and compare the results to the model with flat priors.

8.  Experiment with generating posterior predictions for new data points.  Comment on the results.

9.  Using the approach described in @sec-bayes-reg-updating, update your model with the second dataset.

10.  Examine the updated posteriors and perform an updated posterior predictive check to assess the fit of the updated model to the new data.  Comment on the results.



